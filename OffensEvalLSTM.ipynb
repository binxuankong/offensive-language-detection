{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OffensEval.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "nvVJTjey00VL",
        "colab_type": "code",
        "outputId": "bc7c59f5-5b40-4947-aa43-8634a684ab81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31m  HTTP error 403 while getting http://download.pytorch.org/whl/cu100/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\u001b[0m\n",
            "\u001b[31m  Could not install requirement torch==0.4.1 from http://download.pytorch.org/whl/cu100/torch-0.4.1-cp36-cp36m-linux_x86_64.whl because of error 403 Client Error: Forbidden for url: http://download.pytorch.org/whl/cu100/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\u001b[0m\n",
            "\u001b[31mCould not install requirement torch==0.4.1 from http://download.pytorch.org/whl/cu100/torch-0.4.1-cp36-cp36m-linux_x86_64.whl because of HTTP error 403 Client Error: Forbidden for url: http://download.pytorch.org/whl/cu100/torch-0.4.1-cp36-cp36m-linux_x86_64.whl for URL http://download.pytorch.org/whl/cu100/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gyAgEkU61Dys",
        "colab_type": "code",
        "outputId": "6ed0bce2-501d-48ae-cbf0-9deb36417c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-ipNoOvO5SBH",
        "colab_type": "code",
        "outputId": "219ce522-e039-42cb-a59d-4e2a5359a670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import copy\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import codecs\n",
        "import random\n",
        "import csv\n",
        "\n",
        "from tqdm import tqdm \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "1fvnz6vZ47do",
        "colab_type": "code",
        "outputId": "43510cb4-14a7-4ff2-d6df-bd5a7e89eb73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "data_directory = \"/content/drive/My Drive/OffensEval/data/startkit/training-v1/offenseval-training-v1.tsv\"\n",
        "print(\"Reading offenseval-training-v1.tsv...\")\n",
        "training_data = pd.read_csv(data_directory, sep='\\t', header=0)\n",
        "tweets = training_data[[\"tweet\"]]\n",
        "task_a_labels = training_data[[\"subtask_a\"]]\n",
        "task_b_labels = training_data.query(\"subtask_a == 'OFF'\")[[\"subtask_b\"]]\n",
        "task_c_labels = training_data.query(\"subtask_b == 'TIN'\")[[\"subtask_c\"]]\n",
        "clean_tweets = copy.deepcopy(tweets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading offenseval-training-v1.tsv...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z1siy1YH6kEu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_data(tweet):\n",
        "  noises = ['@USER', 'URL', '\\'s', '\\'ve', 'n\\'t', '\\'re', '\\'ll', '\\'d']\n",
        "  \n",
        "  for noise in noises:\n",
        "    tweet = tweet.replace(noise, '')\n",
        "  \n",
        "  return re.sub(r'[^a-zA-Z]', ' ', tweet)\n",
        "\n",
        "\n",
        "def tokenize(tweet):\n",
        "  return word_tokenize(tweet.lower())\n",
        "\n",
        "\n",
        "def remove_stop_words(tweets):\n",
        "  clean_tweets = []\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  \n",
        "  for tweet in tweets:\n",
        "    if tweet not in stop_words:\n",
        "      if tweet.replace(' ', '') != '':\n",
        "        if len(tweet) > 1:\n",
        "          clean_tweets.append(tweet)\n",
        "  \n",
        "  return clean_tweets\n",
        "\n",
        "\n",
        "def lemmatize_and_stem(tweets):\n",
        "  clean_tweets = []\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  stemmer = PorterStemmer()\n",
        "  \n",
        "  for tweet in tweets:\n",
        "    tweet = lemmatizer.lemmatize(tweet)\n",
        "    tweet = stemmer.stem(tweet)\n",
        "    if len(tweet) > 1:\n",
        "      clean_tweets.append(tweet)\n",
        "  \n",
        "  return clean_tweets\n",
        "\n",
        "\n",
        "def word_to_index(tweets):\n",
        "  vocabulary = []\n",
        "  \n",
        "  for tweet in tweets:\n",
        "    for token in tweet:\n",
        "      if token not in vocabulary:\n",
        "        vocabulary.append(token)\n",
        "  \n",
        "  word2index = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
        "  \n",
        "  return word2index\n",
        "\n",
        "\n",
        "def label_to_index(labels):\n",
        "  dictionary = []\n",
        "  \n",
        "  for label in labels:\n",
        "    if label not in dictionary:\n",
        "      dictionary.append(label)\n",
        "  \n",
        "  label2index = {l: idx for (idx, l) in enumerate(dictionary)}\n",
        "  \n",
        "  return label2index\n",
        "\n",
        "\n",
        "def tweet_to_tensor(tweets, word2index, max_len):\n",
        "  vectorized_tweets = [[word2index[token] for token in tweet if token in word2index] for tweet in tweets]\n",
        "  tweet_tensor = Variable(torch.zeros((len(vectorized_tweets), max_len))).long()\n",
        "  tweet_lengths = [len(tweet) for tweet in vectorized_tweets]\n",
        "  \n",
        "  for index, (tweet, tweetlen) in enumerate(zip(vectorized_tweets, tweet_lengths)):\n",
        "    tweet_tensor[index, :tweetlen] = torch.LongTensor(tweet)\n",
        "  \n",
        "  return tweet_tensor\n",
        "\n",
        "\n",
        "def get_tensors_by_label(tensors, labels, keyword, max_len):\n",
        "  if tensors.shape[0] != len(labels):\n",
        "    print(\"Unmatching sizes\")\n",
        "    return\n",
        "  \n",
        "  length = labels.count(keyword)\n",
        "  tweet_tensor = Variable(torch.zeros((length, max_len))).long()\n",
        "  index = 0\n",
        "  \n",
        "  for tensor, label in zip(tensors, labels):\n",
        "    if label == keyword:\n",
        "      tweet_tensor[index] = tensor\n",
        "      index += 1\n",
        "  \n",
        "  return tweet_tensor\n",
        "\n",
        "\n",
        "def label_to_tensor(labels, label2index):\n",
        "  vectorized_labels = [label2index[label] for label in labels if label in label2index]\n",
        "  label_tensor = torch.LongTensor(vectorized_labels)\n",
        "  \n",
        "  return label_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s2ZFfKN6Hjod",
        "colab_type": "code",
        "outputId": "882ed7c0-c390-4aec-b5d0-6f00aab4367e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "tqdm.pandas(desc=\"Cleaning Data...\")\n",
        "clean_tweets['tweet'] = tweets['tweet'].progress_apply(clean_data)\n",
        "tqdm.pandas(desc=\"Tokenizing Data...\")\n",
        "clean_tweets['tokens'] = clean_tweets['tweet'].progress_apply(tokenize)\n",
        "tqdm.pandas(desc=\"Removing Stop Words...\")\n",
        "clean_tweets['tokens'] = clean_tweets['tokens'].progress_apply(remove_stop_words)\n",
        "tqdm.pandas(desc=\"Lemmatizing And Stemming...\")\n",
        "clean_tweets['tokens'] = clean_tweets['tokens'].progress_apply(lemmatize_and_stem)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaning Data...: 100%|██████████| 13240/13240 [00:00<00:00, 84048.34it/s]\n",
            "Tokenizing Data...: 100%|██████████| 13240/13240 [00:02<00:00, 5345.34it/s]\n",
            "Removing Stop Words...: 100%|██████████| 13240/13240 [00:02<00:00, 5364.75it/s]\n",
            "Lemmatizing And Stemming...: 100%|██████████| 13240/13240 [00:06<00:00, 1931.46it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "yv48DouA93Mo",
        "colab_type": "code",
        "outputId": "9b36df01-e41a-48f7-f5e3-1bcf7a535852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "tokenized_tweets = clean_tweets['tokens'].tolist()\n",
        "\n",
        "tweet_lengths = [len(tweet) for tweet in tokenized_tweets]\n",
        "max_length = np.max(np.array(tweet_lengths))\n",
        "\n",
        "word2index = word_to_index(tokenized_tweets)\n",
        "\n",
        "labels_a = task_a_labels['subtask_a'].values.tolist()\n",
        "labels_b = task_b_labels['subtask_b'].values.tolist()\n",
        "labels_c = task_c_labels['subtask_c'].values.tolist()\n",
        "\n",
        "a2index = label_to_index(labels_a)\n",
        "b2index = label_to_index(labels_b)\n",
        "c2index = label_to_index(labels_c)\n",
        "\n",
        "tweets_a_tensor = tweet_to_tensor(tokenized_tweets, word2index, max_length)\n",
        "tweets_b_tensor = get_tensors_by_label(tweets_a_tensor, labels_a, \"OFF\", max_length)\n",
        "tweets_c_tensor = get_tensors_by_label(tweets_b_tensor, labels_b, \"TIN\", max_length)\n",
        "\n",
        "labels_a_tensor = label_to_tensor(labels_a, a2index)\n",
        "labels_b_tensor = label_to_tensor(labels_b, b2index)\n",
        "labels_c_tensor = label_to_tensor(labels_c, c2index)\n",
        "\n",
        "print(\"Task A tensor size:\")\n",
        "print(tweets_a_tensor.shape)\n",
        "print(\"Task B tensor size:\")\n",
        "print(tweets_b_tensor.shape)\n",
        "print(\"Task C tensor size:\")\n",
        "print(tweets_c_tensor.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Task A tensor size:\n",
            "torch.Size([13240, 39])\n",
            "Task B tensor size:\n",
            "torch.Size([4400, 39])\n",
            "Task C tensor size:\n",
            "torch.Size([3876, 39])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EylhXTij7Qxn",
        "colab_type": "code",
        "outputId": "ed323556-03fa-43d3-ef34-bdb51a7d129f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "USE_GPU = True\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "  print('cuda')\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  print('cpu')\n",
        "  device = torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d9HL7jB0-Xqo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_valid_split(dataset, labels, validation_split):\n",
        "  dataset_size = dataset.shape[0]\n",
        "  \n",
        "  indices = np.arange(dataset_size)\n",
        "  np.random.shuffle(indices)\n",
        "  dataset = dataset[indices]\n",
        "  labels = labels[indices]\n",
        "  \n",
        "  split = int(np.floor(validation_split * dataset_size))\n",
        "  train_data = dataset[split:]\n",
        "  valid_data = dataset[:split]\n",
        "  train_labels = labels[split:]\n",
        "  valid_labels = labels[:split]\n",
        "  \n",
        "  return train_data, valid_data, train_labels, valid_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VItP1hbudlD0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "  \n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, dropout):\n",
        "    super(LSTMClassifier, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "    self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "    h0 = Variable(torch.zeros(1, batch_size, self.hidden_dim))\n",
        "    c0 = Variable(torch.zeros(1, batch_size, self.hidden_dim))\n",
        "    return (h0, c0)\n",
        "  \n",
        "  def forward(self, x, batch_size):\n",
        "    self.hidden = self.init_hidden(batch_size)\n",
        "    embedded = self.embedding(x)\n",
        "    embedded = embedded.view(x.shape[1], batch_size, -1)\n",
        "    lstm_out, self.hidden = self.lstm(embedded, self.hidden)\n",
        "    dropout = self.dropout(lstm_out)\n",
        "    preds = self.hidden2label(dropout[-1])\n",
        "    return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K4JWXU2f6yvD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_accuracy(output, target):\n",
        "  num_correct = 0.0\n",
        "  for i, o in enumerate(output):\n",
        "    probs = torch.softmax(o, 0)\n",
        "    _, pred_label = torch.max(probs, 0)\n",
        "    if pred_label == target[i]:\n",
        "      num_correct += 1\n",
        "  acc = num_correct / len(target)\n",
        "  return acc\n",
        "\n",
        "\n",
        "def get_f1_score(output, target):\n",
        "  preds = []\n",
        "  for o in output:\n",
        "    probs = torch.softmax(o, 0)\n",
        "    _, pred_label = torch.max(probs, 0)\n",
        "    preds.append(pred_label)\n",
        "  score = f1_score(preds, target, average='macro')\n",
        "  return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OdsNa_Jc3hO9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_epoch(model, data, labels, optimizer, loss_fn):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  preds = model(data, len(data))\n",
        "  loss = loss_fn(preds, labels)\n",
        "  acc = get_accuracy(preds, labels)\n",
        "    \n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  train_loss = loss.item()\n",
        "  \n",
        "  return acc, train_loss\n",
        "\n",
        "\n",
        "def evaluate(model, data, labels, loss_fn):\n",
        "  model.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    preds = model(data, len(data))\n",
        "    loss = loss_fn(preds, labels)\n",
        "    acc = get_accuracy(preds, labels)\n",
        "    valid_loss = loss.item()\n",
        "  \n",
        "  return acc, valid_loss\n",
        "\n",
        "\n",
        "def train(model, data, labels, optimizer, epochs=1, validation_split=0.2, print_every=5):\n",
        "  class_count = np.array([len(np.where(labels == t)[0]) for t in np.unique(labels)])\n",
        "  weight = torch.FloatTensor(class_count/len(labels))\n",
        "  loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
        "  \n",
        "  if USE_GPU:\n",
        "    model = model.to(device=device)\n",
        "    data = data.to(device=device)\n",
        "    labels = labels.to(device=device)\n",
        "  \n",
        "  for e in range(1, epochs+1):\n",
        "    train_data, valid_data, train_labels, valid_labels = train_valid_split(data, labels, validation_split)\n",
        "    \n",
        "    train_acc, train_loss = train_epoch(model, train_data, train_labels, optimizer, loss_fn)\n",
        "    valid_acc, valid_loss = evaluate(model, valid_data, valid_labels, loss_fn)\n",
        "    \n",
        "    if e % print_every == 0:\n",
        "      print(f'| Epoch: {e:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
        "    \n",
        "  return valid_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Trr4zsc9w2mq",
        "colab_type": "code",
        "outputId": "6772816b-4eec-4515-af49-685d9b0b48be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install GPy GPyOpt\n",
        "\n",
        "import GPy\n",
        "import GPyOpt\n",
        "from GPyOpt.methods import BayesianOptimization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: GPy in /usr/local/lib/python3.6/dist-packages (1.9.6)\n",
            "Requirement already satisfied: GPyOpt in /usr/local/lib/python3.6/dist-packages (1.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from GPy) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from GPy) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.6/dist-packages (from GPy) (1.1.0)\n",
            "Requirement already satisfied: paramz>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from GPy) (0.9.4)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.6/dist-packages (from paramz>=0.9.0->GPy) (4.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GjVTGf4aSCMw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use GPyOpt for hyper-parameter search\n",
        "domain = [{'name': 'embedding_dim', 'type': 'discrete', 'domain': (100, 120, 140, 160, 180, 200)},\n",
        "          {'name': 'hidden_dim', 'type': 'discrete', 'domain': (32, 64, 96, 128, 160, 192)},\n",
        "          {'name': 'drop_out', 'type': 'continuous', 'domain': (0.1, 0.5)},\n",
        "          {'name': 'lr', 'type': 'continuous', 'domain': (0.0001, 0.001)},\n",
        "          {'name': 'weight_decay', 'type': 'continuous', 'domain': (0, 0.5)}\n",
        "         ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tUy4fD3jwuqX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(word2index)\n",
        "LABEL_SIZE = 2\n",
        "\n",
        "def taskA_tuning(params):\n",
        "  param = params[0]\n",
        "  # LSTMClassifier(vocab_size, embedding_dim, hidden_dim, label_size, dropout)\n",
        "  model = LSTMClassifier(INPUT_DIM, int(param[0]), int(param[1]), 2, param[2])\n",
        "  optimizer = optim.Adam(model.parameters(), lr=param[3], weight_decay=param[4])\n",
        "  acc = train(model, tweets_a_tensor, labels_a_tensor, optimizer, epochs=10, print_every=100)\n",
        "  return acc\n",
        "\n",
        "taskA_opt = BayesianOptimization(f=taskA_tuning,\n",
        "                                 domain=domain,\n",
        "                                 model_type='GP',\n",
        "                                 acquisition_type='EI',\n",
        "                                 acquisition_jitter=0.05,\n",
        "                                 maximize=True)\n",
        "\n",
        "taskA_opt.run_optimization(max_iter=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C599-qBBa147",
        "colab_type": "code",
        "outputId": "9ab424ef-fbb4-4669-acda-0d761492a267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"The hyperparameters used for the model for Sub-Task A are\")\n",
        "print(f'embedding_dim: {int(taskA_opt.X[-1][0])}')\n",
        "print(f'hidden_dim: {int(taskA_opt.X[-1][1])}')\n",
        "print(f'drop_out: {taskA_opt.X[-1][2]:.2f}')\n",
        "print(f'learning_rate: {taskA_opt.X[-1][3]:.4f}')\n",
        "print(f'weight_decay: {taskA_opt.X[-1][4]:.4f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The hyperparameters used for the model for Sub-Task A are\n",
            "embedding_dim: 100\n",
            "hidden_dim: 128\n",
            "drop_out: 0.11\n",
            "learning_rate: 0.0006\n",
            "weight_decay: 0.1309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U68vARmFtLqU",
        "colab_type": "code",
        "outputId": "6bc38e71-5a46-4316-e48e-6cecb5840e49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "# LSTMClassifier(vocab_size, embedding_dim, hidden_dim, label_size, dropout)\n",
        "test_model_A = LSTMClassifier(INPUT_DIM, int(taskA_opt.X[-1][0]), int(taskA_opt.X[-1][1]), LABEL_SIZE, taskA_opt.X[-1][2])\n",
        "optimizer_A = optim.Adam(test_model_A.parameters(), lr=taskA_opt.X[-1][3], weight_decay=taskA_opt.X[-1][4])\n",
        "acc_A = train(test_model_A, tweets_a_tensor, labels_a_tensor, optimizer_A, epochs=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 05 | Train Loss: 0.591 | Train Acc: 66.71% | Val. Loss: 0.577 | Val. Acc: 67.22% |\n",
            "| Epoch: 10 | Train Loss: 0.538 | Train Acc: 67.25% | Val. Loss: 0.536 | Val. Acc: 66.46% |\n",
            "| Epoch: 15 | Train Loss: 0.522 | Train Acc: 67.44% | Val. Loss: 0.525 | Val. Acc: 66.64% |\n",
            "| Epoch: 20 | Train Loss: 0.509 | Train Acc: 67.79% | Val. Loss: 0.533 | Val. Acc: 65.82% |\n",
            "| Epoch: 25 | Train Loss: 0.511 | Train Acc: 68.03% | Val. Loss: 0.528 | Val. Acc: 66.21% |\n",
            "| Epoch: 30 | Train Loss: 0.513 | Train Acc: 67.98% | Val. Loss: 0.520 | Val. Acc: 67.54% |\n",
            "| Epoch: 35 | Train Loss: 0.511 | Train Acc: 68.37% | Val. Loss: 0.522 | Val. Acc: 66.86% |\n",
            "| Epoch: 40 | Train Loss: 0.515 | Train Acc: 68.05% | Val. Loss: 0.510 | Val. Acc: 68.66% |\n",
            "| Epoch: 45 | Train Loss: 0.511 | Train Acc: 68.80% | Val. Loss: 0.527 | Val. Acc: 66.75% |\n",
            "| Epoch: 50 | Train Loss: 0.515 | Train Acc: 68.23% | Val. Loss: 0.512 | Val. Acc: 69.41% |\n",
            "| Epoch: 55 | Train Loss: 0.512 | Train Acc: 68.90% | Val. Loss: 0.513 | Val. Acc: 68.47% |\n",
            "| Epoch: 60 | Train Loss: 0.513 | Train Acc: 69.04% | Val. Loss: 0.525 | Val. Acc: 67.94% |\n",
            "| Epoch: 65 | Train Loss: 0.511 | Train Acc: 68.88% | Val. Loss: 0.508 | Val. Acc: 69.73% |\n",
            "| Epoch: 70 | Train Loss: 0.517 | Train Acc: 68.96% | Val. Loss: 0.504 | Val. Acc: 71.23% |\n",
            "| Epoch: 75 | Train Loss: 0.510 | Train Acc: 69.72% | Val. Loss: 0.524 | Val. Acc: 67.96% |\n",
            "| Epoch: 80 | Train Loss: 0.512 | Train Acc: 69.05% | Val. Loss: 0.512 | Val. Acc: 70.54% |\n",
            "| Epoch: 85 | Train Loss: 0.513 | Train Acc: 69.52% | Val. Loss: 0.517 | Val. Acc: 70.86% |\n",
            "| Epoch: 90 | Train Loss: 0.514 | Train Acc: 70.00% | Val. Loss: 0.516 | Val. Acc: 69.40% |\n",
            "| Epoch: 95 | Train Loss: 0.508 | Train Acc: 70.13% | Val. Loss: 0.523 | Val. Acc: 69.14% |\n",
            "| Epoch: 100 | Train Loss: 0.511 | Train Acc: 70.90% | Val. Loss: 0.522 | Val. Acc: 69.37% |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2cAQn8NLRrps",
        "colab_type": "code",
        "outputId": "936440f5-2979-4199-a17f-6326cb5743ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "_, valid_data, _, valid_labels = train_valid_split(tweets_a_tensor, labels_a_tensor, 0.2)\n",
        "\n",
        "test_model_A.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = test_model_A(valid_data, len(valid_data))\n",
        "    acc_A = get_accuracy(output, valid_labels)\n",
        "    score_A = get_f1_score(output, valid_labels)\n",
        "\n",
        "print('Validation performance for Sub-Task A')\n",
        "print(f'Validation Accuracy: {acc_A:.4f}')\n",
        "print(f'Validation F1 Score: {score_A:.4f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation performance for Sub-Task A\n",
            "Validation Accuracy: 0.6962\n",
            "Validation F1 Score: 0.4198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UfdKZF-bCkZG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(word2index)\n",
        "LABEL_SIZE = 2\n",
        "\n",
        "def taskB_tuning(params):\n",
        "  param = params[0]\n",
        "  # LSTMClassifier(vocab_size, embedding_dim, hidden_dim, label_size, dropout)\n",
        "  model = LSTMClassifier(INPUT_DIM, int(param[0]), int(param[1]), LABEL_SIZE, param[2])\n",
        "  optimizer = optim.SGD(model.parameters(), lr=param[3], weight_decay=param[4])\n",
        "  acc = train(model, tweets_b_tensor, labels_b_tensor, optimizer, epochs=10, print_every=100)\n",
        "  return acc\n",
        "\n",
        "taskB_opt = BayesianOptimization(f=taskB_tuning,\n",
        "                                 domain=domain,\n",
        "                                 model_type='GP',\n",
        "                                 acquisition_type='EI',\n",
        "                                 acquisition_jitter=0.05,\n",
        "                                 maximize=True)\n",
        "\n",
        "taskB_opt.run_optimization(max_iter=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qoXwYXmH76eR",
        "colab_type": "code",
        "outputId": "d3bdec2b-3acf-428d-ae1c-858e133229a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"The hyperparameters used for the model for Sub-Task B are\")\n",
        "print(f'embedding_dim: {int(taskB_opt.X[-1][0])}')\n",
        "print(f'hidden_dim: {int(taskB_opt.X[-1][1])}')\n",
        "print(f'drop_out: {taskB_opt.X[-1][2]:.2f}')\n",
        "print(f'learning_rate: {taskB_opt.X[-1][3]:.4f}')\n",
        "print(f'weight_decay: {taskB_opt.X[-1][4]:.4f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The hyperparameters used for the model for Sub-Task B are\n",
            "embedding_dim: 140\n",
            "hidden_dim: 192\n",
            "drop_out: 0.43\n",
            "learning_rate: 0.0002\n",
            "weight_decay: 0.0837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hN2EJQ7s8Chp",
        "colab_type": "code",
        "outputId": "5b5cea1e-0c57-4ec7-caa3-bff0455b9e7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "# LSTMClassifier(vocab_size, embedding_dim, hidden_dim, label_size, output_dim, dropout)\n",
        "test_model_B = LSTMClassifier(INPUT_DIM, int(taskB_opt.X[-1][0]), int(taskB_opt.X[-1][1]), LABEL_SIZE, taskB_opt.X[-1][2])\n",
        "optimizer_B = optim.Adam(test_model_B.parameters(), lr=taskB_opt.X[-1][3], weight_decay=taskB_opt.X[-1][4])\n",
        "acc_B = train(test_model_B, tweets_b_tensor, labels_b_tensor, optimizer_B, epochs=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 05 | Train Loss: 0.553 | Train Acc: 84.99% | Val. Loss: 0.532 | Val. Acc: 85.40% |\n",
            "| Epoch: 10 | Train Loss: 0.444 | Train Acc: 87.33% | Val. Loss: 0.412 | Val. Acc: 89.11% |\n",
            "| Epoch: 15 | Train Loss: 0.373 | Train Acc: 88.38% | Val. Loss: 0.344 | Val. Acc: 87.87% |\n",
            "| Epoch: 20 | Train Loss: 0.301 | Train Acc: 88.51% | Val. Loss: 0.284 | Val. Acc: 89.32% |\n",
            "| Epoch: 25 | Train Loss: 0.265 | Train Acc: 88.69% | Val. Loss: 0.250 | Val. Acc: 90.19% |\n",
            "| Epoch: 30 | Train Loss: 0.242 | Train Acc: 89.35% | Val. Loss: 0.243 | Val. Acc: 88.53% |\n",
            "| Epoch: 35 | Train Loss: 0.218 | Train Acc: 89.35% | Val. Loss: 0.206 | Val. Acc: 89.18% |\n",
            "| Epoch: 40 | Train Loss: 0.198 | Train Acc: 89.69% | Val. Loss: 0.194 | Val. Acc: 87.88% |\n",
            "| Epoch: 45 | Train Loss: 0.192 | Train Acc: 89.37% | Val. Loss: 0.182 | Val. Acc: 90.38% |\n",
            "| Epoch: 50 | Train Loss: 0.182 | Train Acc: 89.42% | Val. Loss: 0.191 | Val. Acc: 90.59% |\n",
            "| Epoch: 55 | Train Loss: 0.172 | Train Acc: 89.97% | Val. Loss: 0.173 | Val. Acc: 91.02% |\n",
            "| Epoch: 60 | Train Loss: 0.165 | Train Acc: 90.14% | Val. Loss: 0.155 | Val. Acc: 89.69% |\n",
            "| Epoch: 65 | Train Loss: 0.168 | Train Acc: 90.02% | Val. Loss: 0.151 | Val. Acc: 91.31% |\n",
            "| Epoch: 70 | Train Loss: 0.157 | Train Acc: 90.65% | Val. Loss: 0.172 | Val. Acc: 89.87% |\n",
            "| Epoch: 75 | Train Loss: 0.157 | Train Acc: 90.66% | Val. Loss: 0.137 | Val. Acc: 91.78% |\n",
            "| Epoch: 80 | Train Loss: 0.156 | Train Acc: 90.75% | Val. Loss: 0.155 | Val. Acc: 91.30% |\n",
            "| Epoch: 85 | Train Loss: 0.154 | Train Acc: 90.39% | Val. Loss: 0.134 | Val. Acc: 92.64% |\n",
            "| Epoch: 90 | Train Loss: 0.150 | Train Acc: 91.87% | Val. Loss: 0.156 | Val. Acc: 90.51% |\n",
            "| Epoch: 95 | Train Loss: 0.145 | Train Acc: 91.67% | Val. Loss: 0.145 | Val. Acc: 91.30% |\n",
            "| Epoch: 100 | Train Loss: 0.140 | Train Acc: 91.80% | Val. Loss: 0.129 | Val. Acc: 91.37% |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6TJ4lVcNT-r9",
        "colab_type": "code",
        "outputId": "b2084091-dfc6-47ce-fc3d-7b362fc392a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "_, valid_data, _, valid_labels = train_valid_split(tweets_b_tensor, labels_b_tensor, 0.2)\n",
        "\n",
        "test_model_B.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = test_model_B(valid_data, len(valid_data))\n",
        "    acc_B = get_accuracy(output, valid_labels)\n",
        "    score_B = get_f1_score(output, valid_labels)\n",
        "\n",
        "print('Validation performance for Sub-Task B')\n",
        "print(f'Validation Accuracy: {acc_B:.4f}')\n",
        "print(f'Validation F1 Score: {score_B:.4f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation performance for Sub-Task B\n",
            "Validation Accuracy: 0.9043\n",
            "Validation F1 Score: 0.4771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m7iYqJdjSZfb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(word2index)\n",
        "LABEL_SIZE = 3\n",
        "\n",
        "def taskC_tuning(params):\n",
        "  param = params[0]\n",
        "  # LSTMClassifier(vocab_size, embedding_dim, hidden_dim, label_size, dropout)\n",
        "  model = LSTMClassifier(INPUT_DIM, int(param[0]), int(param[1]), LABEL_SIZE, param[2])\n",
        "  optimizer = optim.SGD(model.parameters(), lr=param[3], weight_decay=param[4])\n",
        "  acc = train(model, tweets_c_tensor, labels_c_tensor, optimizer, epochs=10, print_every=100)\n",
        "  return acc\n",
        "\n",
        "taskC_opt = BayesianOptimization(f=taskC_tuning,\n",
        "                                 domain=domain,\n",
        "                                 model_type='GP',\n",
        "                                 acquisition_type='EI',\n",
        "                                 acquisition_jitter=0.05,\n",
        "                                 maximize=True)\n",
        "\n",
        "taskC_opt.run_optimization(max_iter=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R9GVRcUiAkRK",
        "colab_type": "code",
        "outputId": "93e7b085-8458-461c-ccb4-a6dc03da247b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"The hyperparameters used for the model for Sub-Task C are\")\n",
        "print(f'embedding_dim: {int(taskC_opt.X[-1][0])}')\n",
        "print(f'hidden_dim: {int(taskC_opt.X[-1][1])}')\n",
        "print(f'drop_out: {taskC_opt.X[-1][2]:.2f}')\n",
        "print(f'learning_rate: {taskC_opt.X[-1][3]:.4f}')\n",
        "print(f'weight_decay: {taskC_opt.X[-1][4]:.4f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The hyperparameters used for the model for Sub-Task C are\n",
            "embedding_dim: 180\n",
            "hidden_dim: 128\n",
            "drop_out: 0.36\n",
            "learning_rate: 0.0002\n",
            "weight_decay: 0.0170\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QvaaX3oNArZF",
        "colab_type": "code",
        "outputId": "be945b46-2ebb-4fa3-86eb-3443cdfb5580",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "# LSTMClassifier(vocab_size, embedding_dim, hidden_dim, label_size, dropout)\n",
        "test_model_C = LSTMClassifier(INPUT_DIM, int(taskC_opt.X[-1][0]), int(taskC_opt.X[-1][1]), LABEL_SIZE, taskC_opt.X[-1][2])\n",
        "optimizer_C = optim.SGD(test_model_C.parameters(), lr=taskC_opt.X[-1][3], weight_decay=taskC_opt.X[-1][4])\n",
        "acc_C = train(test_model_C, tweets_c_tensor, labels_c_tensor, optimizer_C, epochs=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 05 | Train Loss: 1.096 | Train Acc: 36.85% | Val. Loss: 1.092 | Val. Acc: 30.00% |\n",
            "| Epoch: 10 | Train Loss: 1.098 | Train Acc: 34.61% | Val. Loss: 1.088 | Val. Acc: 30.79% |\n",
            "| Epoch: 15 | Train Loss: 1.089 | Train Acc: 37.29% | Val. Loss: 1.082 | Val. Acc: 32.58% |\n",
            "| Epoch: 20 | Train Loss: 1.082 | Train Acc: 39.06% | Val. Loss: 1.078 | Val. Acc: 33.17% |\n",
            "| Epoch: 25 | Train Loss: 1.079 | Train Acc: 39.58% | Val. Loss: 1.076 | Val. Acc: 33.90% |\n",
            "| Epoch: 30 | Train Loss: 1.076 | Train Acc: 39.14% | Val. Loss: 1.067 | Val. Acc: 36.20% |\n",
            "| Epoch: 35 | Train Loss: 1.071 | Train Acc: 42.79% | Val. Loss: 1.064 | Val. Acc: 45.26% |\n",
            "| Epoch: 40 | Train Loss: 1.068 | Train Acc: 41.59% | Val. Loss: 1.065 | Val. Acc: 43.15% |\n",
            "| Epoch: 45 | Train Loss: 1.062 | Train Acc: 43.82% | Val. Loss: 1.057 | Val. Acc: 48.52% |\n",
            "| Epoch: 50 | Train Loss: 1.055 | Train Acc: 44.57% | Val. Loss: 1.053 | Val. Acc: 50.91% |\n",
            "| Epoch: 55 | Train Loss: 1.050 | Train Acc: 46.26% | Val. Loss: 1.047 | Val. Acc: 52.92% |\n",
            "| Epoch: 60 | Train Loss: 1.050 | Train Acc: 47.16% | Val. Loss: 1.042 | Val. Acc: 54.39% |\n",
            "| Epoch: 65 | Train Loss: 1.044 | Train Acc: 47.35% | Val. Loss: 1.039 | Val. Acc: 55.55% |\n",
            "| Epoch: 70 | Train Loss: 1.040 | Train Acc: 47.73% | Val. Loss: 1.034 | Val. Acc: 57.85% |\n",
            "| Epoch: 75 | Train Loss: 1.037 | Train Acc: 49.28% | Val. Loss: 1.030 | Val. Acc: 56.45% |\n",
            "| Epoch: 80 | Train Loss: 1.031 | Train Acc: 50.88% | Val. Loss: 1.020 | Val. Acc: 62.06% |\n",
            "| Epoch: 85 | Train Loss: 1.026 | Train Acc: 50.33% | Val. Loss: 1.024 | Val. Acc: 56.01% |\n",
            "| Epoch: 90 | Train Loss: 1.024 | Train Acc: 51.71% | Val. Loss: 1.013 | Val. Acc: 62.55% |\n",
            "| Epoch: 95 | Train Loss: 1.022 | Train Acc: 51.73% | Val. Loss: 1.017 | Val. Acc: 58.14% |\n",
            "| Epoch: 100 | Train Loss: 1.018 | Train Acc: 53.35% | Val. Loss: 1.013 | Val. Acc: 60.05% |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eqjxxFFYUKvY",
        "colab_type": "code",
        "outputId": "ee34c1a7-0bf8-421e-af66-7ec97226e316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "_, valid_data, _, valid_labels = train_valid_split(tweets_c_tensor, labels_c_tensor, 0.2)\n",
        "\n",
        "test_model_C.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = test_model_C(valid_data, len(valid_data))\n",
        "    acc_C = get_accuracy(output, valid_labels)\n",
        "    score_C = get_f1_score(output, valid_labels)\n",
        "\n",
        "print('Validation performance for Sub-Task C')\n",
        "print(f'Validation Accuracy: {acc_C:.4f}')\n",
        "print(f'Validation F1 Score: {score_C:.4f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation performance for Sub-Task C\n",
            "Validation Accuracy: 0.6045\n",
            "Validation F1 Score: 0.3183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z9n7KlwGIFjR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def indices_to_labels(indices, label2index):\n",
        "  labels = []\n",
        "  for index in indices:\n",
        "    for key, num in label2index.items():\n",
        "      if index == num:\n",
        "        labels.append(key)\n",
        "  return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ty6MIl-yGM67",
        "colab_type": "code",
        "outputId": "a584af88-9f03-42d3-e197-8dcb17e0954a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "testA_dir = \"/content/drive/My Drive/OffensEval/data/taskA/testset-taska.tsv\"\n",
        "testB_dir = \"/content/drive/My Drive/OffensEval/data/taskB/testset-taskb.tsv\"\n",
        "testC_dir = \"/content/drive/My Drive/OffensEval/data/taskC/test_set_taskc.tsv\"\n",
        "\n",
        "testA_data = pd.read_csv(testA_dir, sep='\\t', header=0)\n",
        "testA_ids = testA_data[\"id\"].tolist()\n",
        "testA_tweets = testA_data[[\"tweet\"]]\n",
        "clean_testA = copy.deepcopy(testA_tweets)\n",
        "\n",
        "testB_data = pd.read_csv(testB_dir, sep='\\t', header=0)\n",
        "testB_ids = testB_data[\"id\"].tolist()\n",
        "testB_tweets = testB_data[[\"tweet\"]]\n",
        "clean_testB = copy.deepcopy(testB_tweets)\n",
        "\n",
        "testC_data = pd.read_csv(testC_dir, sep='\\t', header=0)\n",
        "testC_ids = testC_data[\"id\"].tolist()\n",
        "testC_tweets = testC_data[[\"tweet\"]]\n",
        "clean_testC = copy.deepcopy(testC_tweets)\n",
        "\n",
        "tqdm.pandas(desc=\"Cleaning Data for Task A...\")\n",
        "clean_testA['tweet'] = testA_tweets['tweet'].progress_apply(clean_data)\n",
        "tqdm.pandas(desc=\"Tokenizing Data for Task A...\")\n",
        "clean_testA['tokens'] = clean_testA['tweet'].progress_apply(tokenize)\n",
        "tqdm.pandas(desc=\"Removing Stop Words for Task A...\")\n",
        "clean_testA['tokens'] = clean_testA['tokens'].progress_apply(remove_stop_words)\n",
        "tqdm.pandas(desc=\"Lemmatizing And Stemming for Task A...\")\n",
        "clean_testA['tokens'] = clean_testA['tokens'].progress_apply(lemmatize_and_stem)\n",
        "\n",
        "tqdm.pandas(desc=\"Cleaning Data for Task B...\")\n",
        "clean_testB['tweet'] = testB_tweets['tweet'].progress_apply(clean_data)\n",
        "tqdm.pandas(desc=\"Tokenizing Data for Task B...\")\n",
        "clean_testB['tokens'] = clean_testB['tweet'].progress_apply(tokenize)\n",
        "tqdm.pandas(desc=\"Removing Stop Words for Task B...\")\n",
        "clean_testB['tokens'] = clean_testB['tokens'].progress_apply(remove_stop_words)\n",
        "tqdm.pandas(desc=\"Lemmatizing And Stemming for Task B...\")\n",
        "clean_testB['tokens'] = clean_testB['tokens'].progress_apply(lemmatize_and_stem)\n",
        "\n",
        "tqdm.pandas(desc=\"Cleaning Data for Task C...\")\n",
        "clean_testC['tweet'] = testC_tweets['tweet'].progress_apply(clean_data)\n",
        "tqdm.pandas(desc=\"Tokenizing Data for Task C...\")\n",
        "clean_testC['tokens'] = clean_testC['tweet'].progress_apply(tokenize)\n",
        "tqdm.pandas(desc=\"Removing Stop Words for Task C...\")\n",
        "clean_testC['tokens'] = clean_testC['tokens'].progress_apply(remove_stop_words)\n",
        "tqdm.pandas(desc=\"Lemmatizing And Stemming for Task C...\")\n",
        "clean_testC['tokens'] = clean_testC['tokens'].progress_apply(lemmatize_and_stem)\n",
        "\n",
        "tokenized_testA = clean_testA['tokens'].tolist()\n",
        "tokenized_testB = clean_testB['tokens'].tolist()\n",
        "tokenized_testC = clean_testC['tokens'].tolist()\n",
        "\n",
        "testA_tensor = tweet_to_tensor(tokenized_testA, word2index, max_length)\n",
        "testB_tensor = tweet_to_tensor(tokenized_testB, word2index, max_length)\n",
        "testC_tensor = tweet_to_tensor(tokenized_testC, word2index, max_length)\n",
        "\n",
        "print(\"Test A tensor size:\")\n",
        "print(testA_tensor.shape)\n",
        "print(\"Test B tensor size:\")\n",
        "print(testB_tensor.shape)\n",
        "print(\"Test C tensor size:\")\n",
        "print(testC_tensor.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaning Data for Task A...: 100%|██████████| 860/860 [00:00<00:00, 61372.40it/s]\n",
            "Tokenizing Data for Task A...: 100%|██████████| 860/860 [00:00<00:00, 5042.06it/s]\n",
            "Removing Stop Words for Task A...: 100%|██████████| 860/860 [00:00<00:00, 5233.68it/s]\n",
            "Lemmatizing And Stemming for Task A...: 100%|██████████| 860/860 [00:00<00:00, 2237.74it/s]\n",
            "Cleaning Data for Task B...: 100%|██████████| 240/240 [00:00<00:00, 63824.05it/s]\n",
            "Tokenizing Data for Task B...: 100%|██████████| 240/240 [00:00<00:00, 5029.42it/s]\n",
            "Removing Stop Words for Task B...: 100%|██████████| 240/240 [00:00<00:00, 5288.66it/s]\n",
            "Lemmatizing And Stemming for Task B...: 100%|██████████| 240/240 [00:00<00:00, 2289.27it/s]\n",
            "Cleaning Data for Task C...: 100%|██████████| 213/213 [00:00<00:00, 55662.73it/s]\n",
            "Tokenizing Data for Task C...: 100%|██████████| 213/213 [00:00<00:00, 4931.62it/s]\n",
            "Removing Stop Words for Task C...: 100%|██████████| 213/213 [00:00<00:00, 5313.57it/s]\n",
            "Lemmatizing And Stemming for Task C...: 100%|██████████| 213/213 [00:00<00:00, 2193.33it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test A tensor size:\n",
            "torch.Size([860, 39])\n",
            "Test B tensor size:\n",
            "torch.Size([240, 39])\n",
            "Test C tensor size:\n",
            "torch.Size([213, 39])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "059JpuVoRBvY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_predictions(output, label2index):\n",
        "  labels = []\n",
        "  \n",
        "  for o in output:\n",
        "    probs = torch.softmax(o, 0)\n",
        "    _, pred = torch.max(probs, 0)\n",
        "    for key, idx in label2index.items():\n",
        "      if pred == idx:\n",
        "        labels.append(key)\n",
        "  \n",
        "  return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wr28I4PSGfl8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_model_A.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  output = test_model_A(testA_tensor, len(testA_tensor))\n",
        "\n",
        "preds_testA = get_predictions(output, a2index)\n",
        "\n",
        "taskA_df = pd.DataFrame(testA_ids, columns=['id'])\n",
        "taskA_df['predict'] = preds_testA\n",
        "taskA_df.to_csv('testA.csv', header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qJXZIh25KIhk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_model_B.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  output = test_model_B(testB_tensor, len(testB_tensor))\n",
        "\n",
        "preds_testB = get_predictions(output, b2index)\n",
        "\n",
        "taskB_df = pd.DataFrame(testB_ids, columns=['id'])\n",
        "taskB_df['predict'] = preds_testB\n",
        "taskB_df.to_csv('testB.csv', header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CMGvOxMoU3w8",
        "colab_type": "code",
        "outputId": "e94153e8-88ea-4092-d905-daefd359a8af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "cell_type": "code",
      "source": [
        "test_model_C.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  output = test_model_C(testC_tensor, len(testC_tensor))\n",
        "\n",
        "preds_testC = get_predictions(output, c2index)\n",
        "\n",
        "taskC_df = pd.DataFrame(testC_ids, columns=['id'])\n",
        "taskC_df['predict'] = preds_testC\n",
        "taskC_df.to_csv('testC.csv', header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d1e120077bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_model_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model_C\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestC_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestC_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_model_C' is not defined"
          ]
        }
      ]
    }
  ]
}