{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OffensEval.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "nvVJTjey00VL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gyAgEkU61Dys",
        "colab_type": "code",
        "outputId": "21f142be-d2af-48ee-978b-b8f912d179db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-ipNoOvO5SBH",
        "colab_type": "code",
        "outputId": "5d424e71-5043-49e3-ebdb-a834010c698b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import copy\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import codecs\n",
        "import random\n",
        "import csv\n",
        "\n",
        "from tqdm import tqdm \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from torch.autograd import Variable\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "1fvnz6vZ47do",
        "colab_type": "code",
        "outputId": "d4c09976-4f78-453b-dd12-bd2c979008d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "data_directory = \"/content/drive/My Drive/OffensEval/data/startkit/training-v1/offenseval-training-v1.tsv\"\n",
        "print(\"Reading offenseval-training-v1.tsv...\")\n",
        "training_data = pd.read_csv(data_directory, sep='\\t', header=0)\n",
        "tweets = training_data[[\"tweet\"]]\n",
        "task_a_labels = training_data[[\"subtask_a\"]]\n",
        "task_b_labels = training_data.query(\"subtask_a == 'OFF'\")[[\"subtask_b\"]]\n",
        "task_c_labels = training_data.query(\"subtask_b == 'TIN'\")[[\"subtask_c\"]]\n",
        "clean_tweets = copy.deepcopy(tweets)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading offenseval-training-v1.tsv...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z1siy1YH6kEu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_data(tweet):\n",
        "  noises = ['@USER', 'URL', '\\'s', '\\'ve', 'n\\'t', '\\'re', '\\'ll', '\\'d']\n",
        "  for noise in noises:\n",
        "    tweet = tweet.replace(noise, '')\n",
        "  return re.sub(r'[^a-zA-Z]', ' ', tweet)\n",
        "\n",
        "def tokenize(tweet):\n",
        "  return word_tokenize(tweet.lower())\n",
        "\n",
        "def remove_stop_words(tweets):\n",
        "  clean_tweets = []\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  for tweet in tweets:\n",
        "    if tweet not in stop_words:\n",
        "      if tweet.replace(' ', '') != '':\n",
        "        if len(tweet) > 1:\n",
        "          clean_tweets.append(tweet)\n",
        "  return clean_tweets\n",
        "\n",
        "def lemmatize_and_stem(tweets):\n",
        "  clean_tweets = []\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  stemmer = PorterStemmer()\n",
        "  for tweet in tweets:\n",
        "    tweet = lemmatizer.lemmatize(tweet)\n",
        "    tweet = stemmer.stem(tweet)\n",
        "    if len(tweet) > 1:\n",
        "      clean_tweets.append(tweet)\n",
        "  return clean_tweets\n",
        "\n",
        "def word_to_index(tweets):\n",
        "  vocabulary = []\n",
        "  for tweet in tweets:\n",
        "    for token in tweet:\n",
        "      if token not in vocabulary:\n",
        "        vocabulary.append(token)\n",
        "  word2index = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
        "  return word2index\n",
        "\n",
        "def label_to_index(labels):\n",
        "  dictionary = []\n",
        "  for label in labels:\n",
        "    if label not in dictionary:\n",
        "      dictionary.append(label)\n",
        "  label2index = {l: idx for (idx, l) in enumerate(dictionary)}\n",
        "  return label2index\n",
        "\n",
        "def tweet_to_tensor(tweets, word2index, max_len):\n",
        "  vectorized_tweets = [[word2index[token] for token in tweet if token in word2index] for tweet in tweets]\n",
        "  tweet_tensor = Variable(torch.zeros((len(vectorized_tweets), max_len))).long()\n",
        "  tweet_lengths = [len(tweet) for tweet in vectorized_tweets]\n",
        "  for index, (tweet, tweetlen) in enumerate(zip(vectorized_tweets, tweet_lengths)):\n",
        "    tweet_tensor[index, :tweetlen] = torch.LongTensor(tweet)\n",
        "  return tweet_tensor\n",
        "\n",
        "def get_tensors_by_label(tensors, labels, keyword, max_len):\n",
        "  if tensors.shape[0] != len(labels):\n",
        "    print(\"Unmatching sizes\")\n",
        "    return\n",
        "  length = labels.count(keyword)\n",
        "  tweet_tensor = Variable(torch.zeros((length, max_len))).long()\n",
        "  index = 0\n",
        "  for tensor, label in zip(tensors, labels):\n",
        "    if label == keyword:\n",
        "      tweet_tensor[index] = tensor\n",
        "      index += 1\n",
        "  return tweet_tensor\n",
        "\n",
        "def label_to_tensor(labels, label2index):\n",
        "  vectorized_labels = [label2index[label] for label in labels if label in label2index]\n",
        "  label_tensor = torch.FloatTensor(vectorized_labels)\n",
        "  return label_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s2ZFfKN6Hjod",
        "colab_type": "code",
        "outputId": "3b336f50-0937-4fee-9da6-9047a7673d87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5644
        }
      },
      "cell_type": "code",
      "source": [
        "tqdm.pandas(desc=\"Cleaning Data...\")\n",
        "clean_tweets['tweet'] = tweets['tweet'].progress_apply(clean_data)\n",
        "tqdm.pandas(desc=\"Tokenizing Data...\")\n",
        "clean_tweets['tokens'] = clean_tweets['tweet'].progress_apply(tokenize)\n",
        "tqdm.pandas(desc=\"Removing Stop Words...\")\n",
        "clean_tweets['tokens'] = clean_tweets['tokens'].progress_apply(remove_stop_words)\n",
        "tqdm.pandas(desc=\"Lemmatizing And Stemming...\")\n",
        "clean_tweets['tokens'] = clean_tweets['tokens'].progress_apply(lemmatize_and_stem)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Cleaning Data...:   0%|          | 0/13240 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Cleaning Data...:  57%|█████▋    | 7567/13240 [00:00<00:00, 75663.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Cleaning Data...: 100%|██████████| 13240/13240 [00:00<00:00, 75692.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:   0%|          | 0/13240 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:   4%|▍         | 560/13240 [00:00<00:02, 5595.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:   8%|▊         | 1080/13240 [00:00<00:02, 5470.19it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  12%|█▏        | 1644/13240 [00:00<00:02, 5519.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  17%|█▋        | 2219/13240 [00:00<00:01, 5585.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  21%|██        | 2782/13240 [00:00<00:01, 5598.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  25%|██▌       | 3328/13240 [00:00<00:01, 5554.27it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  29%|██▉       | 3886/13240 [00:00<00:01, 5560.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  34%|███▎      | 4442/13240 [00:00<00:01, 5559.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  38%|███▊      | 5003/13240 [00:00<00:01, 5573.34it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  42%|████▏     | 5570/13240 [00:01<00:01, 5601.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  46%|████▋     | 6130/13240 [00:01<00:01, 5599.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  51%|█████     | 6705/13240 [00:01<00:01, 5641.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  55%|█████▌    | 7286/13240 [00:01<00:01, 5689.79it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  59%|█████▉    | 7850/13240 [00:01<00:00, 5647.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  64%|██████▎   | 8423/13240 [00:01<00:00, 5669.45it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  68%|██████▊   | 8994/13240 [00:01<00:00, 5678.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  72%|███████▏  | 9584/13240 [00:01<00:00, 5742.62it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  77%|███████▋  | 10158/13240 [00:01<00:00, 5606.37it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  81%|████████  | 10719/13240 [00:01<00:00, 5579.20it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  85%|████████▌ | 11277/13240 [00:02<00:00, 5249.42it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  89%|████████▉ | 11806/13240 [00:02<00:00, 5220.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  93%|█████████▎| 12333/13240 [00:02<00:00, 5233.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...:  97%|█████████▋| 12870/13240 [00:02<00:00, 5274.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data...: 100%|██████████| 13240/13240 [00:02<00:00, 5489.36it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:   0%|          | 0/13240 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:   4%|▍         | 534/13240 [00:00<00:02, 5334.15it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:   8%|▊         | 1107/13240 [00:00<00:02, 5443.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  12%|█▏        | 1633/13240 [00:00<00:02, 5384.27it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  17%|█▋        | 2207/13240 [00:00<00:02, 5486.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  21%|██        | 2766/13240 [00:00<00:01, 5515.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  25%|██▌       | 3334/13240 [00:00<00:01, 5562.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  30%|██▉       | 3918/13240 [00:00<00:01, 5641.20it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  34%|███▍      | 4493/13240 [00:00<00:01, 5672.61it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  38%|███▊      | 5078/13240 [00:00<00:01, 5723.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  43%|████▎     | 5657/13240 [00:01<00:01, 5741.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  47%|████▋     | 6217/13240 [00:01<00:01, 5585.27it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  51%|█████     | 6775/13240 [00:01<00:01, 5582.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  56%|█████▌    | 7362/13240 [00:01<00:01, 5664.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  60%|█████▉    | 7926/13240 [00:01<00:00, 5656.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  64%|██████▍   | 8505/13240 [00:01<00:00, 5694.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  69%|██████▊   | 9080/13240 [00:01<00:00, 5709.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  73%|███████▎  | 9664/13240 [00:01<00:00, 5747.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  77%|███████▋  | 10238/13240 [00:01<00:00, 5706.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  82%|████████▏ | 10822/13240 [00:01<00:00, 5743.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  86%|████████▌ | 11398/13240 [00:02<00:00, 5746.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  90%|█████████ | 11973/13240 [00:02<00:00, 5633.45it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  95%|█████████▍| 12539/13240 [00:02<00:00, 5639.39it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...:  99%|█████████▉| 13112/13240 [00:02<00:00, 5666.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words...: 100%|██████████| 13240/13240 [00:02<00:00, 5636.25it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:   0%|          | 0/13240 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:   2%|▏         | 267/13240 [00:00<00:04, 2651.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:   4%|▍         | 503/13240 [00:00<00:04, 2552.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:   6%|▌         | 749/13240 [00:00<00:04, 2511.85it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:   7%|▋         | 980/13240 [00:00<00:05, 2447.39it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:   9%|▉         | 1226/13240 [00:00<00:04, 2449.31it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  11%|█         | 1445/13240 [00:00<00:04, 2363.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  13%|█▎        | 1697/13240 [00:00<00:04, 2405.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  15%|█▍        | 1930/13240 [00:00<00:04, 2381.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  16%|█▋        | 2173/13240 [00:00<00:04, 2393.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  18%|█▊        | 2403/13240 [00:01<00:04, 2345.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  20%|█▉        | 2647/13240 [00:01<00:04, 2372.54it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  22%|██▏       | 2880/13240 [00:01<00:04, 2321.61it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  24%|██▎       | 3119/13240 [00:01<00:04, 2338.51it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  25%|██▌       | 3351/13240 [00:01<00:04, 2332.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  27%|██▋       | 3613/13240 [00:01<00:03, 2410.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  29%|██▉       | 3857/13240 [00:01<00:03, 2414.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  31%|███       | 4099/13240 [00:01<00:03, 2306.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  33%|███▎      | 4331/13240 [00:01<00:03, 2287.40it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  34%|███▍      | 4561/13240 [00:01<00:03, 2281.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  36%|███▋      | 4804/13240 [00:02<00:03, 2324.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  38%|███▊      | 5060/13240 [00:02<00:03, 2387.31it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  40%|████      | 5301/13240 [00:02<00:03, 2393.62it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  42%|████▏     | 5541/13240 [00:02<00:03, 2365.53it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  44%|████▎     | 5779/13240 [00:02<00:03, 2301.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  45%|████▌     | 6010/13240 [00:02<00:03, 2270.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  47%|████▋     | 6239/13240 [00:02<00:03, 2275.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  49%|████▉     | 6478/13240 [00:02<00:02, 2306.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  51%|█████     | 6715/13240 [00:02<00:02, 2323.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  53%|█████▎    | 6952/13240 [00:02<00:02, 2331.63it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  54%|█████▍    | 7207/13240 [00:03<00:02, 2392.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  56%|█████▌    | 7447/13240 [00:03<00:02, 2356.23it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  58%|█████▊    | 7684/13240 [00:03<00:02, 2309.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  60%|█████▉    | 7916/13240 [00:03<00:02, 2297.18it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  62%|██████▏   | 8155/13240 [00:03<00:02, 2322.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  63%|██████▎   | 8392/13240 [00:03<00:02, 2330.26it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  65%|██████▌   | 8640/13240 [00:03<00:01, 2330.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  67%|██████▋   | 8874/13240 [00:03<00:01, 2321.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  69%|██████▉   | 9107/13240 [00:03<00:01, 2282.63it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  71%|███████   | 9357/13240 [00:03<00:01, 2342.18it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  72%|███████▎  | 9599/13240 [00:04<00:01, 2363.16it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  74%|███████▍  | 9840/13240 [00:04<00:01, 2372.54it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  76%|███████▌  | 10078/13240 [00:04<00:01, 2353.37it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  78%|███████▊  | 10314/13240 [00:04<00:01, 2333.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  80%|███████▉  | 10548/13240 [00:04<00:01, 2328.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  82%|████████▏ | 10792/13240 [00:04<00:01, 2360.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  83%|████████▎ | 11038/13240 [00:04<00:00, 2387.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  85%|████████▌ | 11278/13240 [00:04<00:00, 2379.16it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  87%|████████▋ | 11517/13240 [00:04<00:00, 2319.27it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  89%|████████▉ | 11751/13240 [00:05<00:00, 2324.27it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  91%|█████████ | 11984/13240 [00:05<00:00, 2252.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  92%|█████████▏| 12218/13240 [00:05<00:00, 2272.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  94%|█████████▍| 12450/13240 [00:05<00:00, 2286.37it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  96%|█████████▌| 12680/13240 [00:05<00:00, 2273.18it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  98%|█████████▊| 12913/13240 [00:05<00:00, 2285.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming:  99%|█████████▉| 13147/13240 [00:05<00:00, 2299.34it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming: 100%|██████████| 13240/13240 [00:05<00:00, 2336.29it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "yv48DouA93Mo",
        "colab_type": "code",
        "outputId": "bfccfba4-f764-4e05-899b-cdb599d40c2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "tokenized_tweets = clean_tweets['tokens'].tolist()\n",
        "\n",
        "tweet_lengths = [len(tweet) for tweet in tokenized_tweets]\n",
        "max_length = np.max(np.array(tweet_lengths))\n",
        "\n",
        "word2index = word_to_index(tokenized_tweets)\n",
        "\n",
        "labels_a = task_a_labels['subtask_a'].values.tolist()\n",
        "labels_b = task_b_labels['subtask_b'].values.tolist()\n",
        "labels_c = task_c_labels['subtask_c'].values.tolist()\n",
        "\n",
        "a2index = label_to_index(labels_a)\n",
        "b2index = label_to_index(labels_b)\n",
        "c2index = label_to_index(labels_c)\n",
        "\n",
        "tweets_a_tensor = tweet_to_tensor(tokenized_tweets, word2index, max_length)\n",
        "tweets_b_tensor = get_tensors_by_label(tweets_a_tensor, labels_a, \"OFF\", max_length)\n",
        "tweets_c_tensor = get_tensors_by_label(tweets_b_tensor, labels_b, \"TIN\", max_length)\n",
        "\n",
        "labels_a_tensor = label_to_tensor(labels_a, a2index)\n",
        "labels_b_tensor = label_to_tensor(labels_b, b2index)\n",
        "labels_c_tensor = label_to_tensor(labels_c, c2index)\n",
        "\n",
        "print(\"Task A tensor size:\")\n",
        "print(tweets_a_tensor.shape)\n",
        "print(\"Task B tensor size:\")\n",
        "print(tweets_b_tensor.shape)\n",
        "print(\"Task C tensor size:\")\n",
        "print(tweets_c_tensor.shape)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Task A tensor size:\n",
            "torch.Size([13240, 39])\n",
            "Task B tensor size:\n",
            "torch.Size([4400, 39])\n",
            "Task C tensor size:\n",
            "torch.Size([3876, 39])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d9HL7jB0-Xqo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_valid_split(dataset, labels, validation_split):\n",
        "  dataset_size = dataset.shape[0]\n",
        "  split = int(np.floor(validation_split * dataset_size))\n",
        "  train_data = dataset[split:]\n",
        "  valid_data = dataset[:split]\n",
        "  train_labels = labels[split:]\n",
        "  valid_labels = labels[:split]\n",
        "  return train_data, valid_data, train_labels, valid_labels\n",
        "\n",
        "def check_accuracy(output, target, num_class=2):\n",
        "  if num_class == 2:\n",
        "    output = torch.round(torch.sigmoid(output))\n",
        "  else:\n",
        "    output = F.log_softmax(output)\n",
        "    output = output.data.max(1)[1].numpy()\n",
        "  correct = (output == target).float()\n",
        "  acc = correct.sum()/len(correct)\n",
        "  return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XR9d8gpUzq4A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    \n",
        "  def __init__(self, vocab_size, embedding_dim, out_channels, window_size, output_dim, dropout):     \n",
        "    super(CNN, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    #in_channels -- 1 text channel\n",
        "    #out_channels -- the number of output channels\n",
        "    #kernel_size is (window size x embedding dim)\n",
        "    self.conv = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size,embedding_dim))\n",
        "    #the dropout layer\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    #the output layer\n",
        "    self.fc = nn.Linear(out_channels, output_dim)\n",
        "        \n",
        "  def forward(self, x):\n",
        "    #(batch size, max sent length)\n",
        "    embedded = self.embedding(x)\n",
        "    #(batch size, max sent length, embedding dim)\n",
        "    #images have 3 RGB channels \n",
        "    #for the text we add 1 channel\n",
        "    embedded = embedded.unsqueeze(1)\n",
        "    #(batch size, 1, max sent length, embedding dim)\n",
        "    feature_maps = self.conv(embedded)\n",
        "    #(batch size, n filters, max input length - window size +1)\n",
        "    feature_maps = feature_maps.squeeze(3)\n",
        "    feature_maps = F.relu(feature_maps)\n",
        "    #the max pooling layer\n",
        "    pooled = F.max_pool1d(feature_maps, feature_maps.shape[2])\n",
        "    pooled = pooled.squeeze(2)\n",
        "    dropped = self.dropout(pooled)\n",
        "    preds = self.fc(dropped)\n",
        "    return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VItP1hbudlD0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "  \n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, output_dim, dropout):\n",
        "    super(LSTMClassifier, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "    self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
        "    self.dropout_layer = nn.Dropout(dropout)\n",
        "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "    return (Variable(torch.zeros(1, batch_size, self.hidden_dim)), Variable(torch.zeros(1, batch_size, self.hidden_dim)))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    self.hidden = self.init_hidden(x.shape[0])\n",
        "    embedded = self.embedding(x)\n",
        "    outputs, (ht, ct) = self.lstm(embedded, self.hidden)\n",
        "    output = self.dropout_layer(ht[-1])\n",
        "    preds = self.fc(output)\n",
        "    return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7El7yaqz26LG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_part(model, dataset, labels, optimizer, epochs=1, num_class=2, validation_split=0.2):\n",
        "  feature_train, feature_valid, target_train, target_valid = train_valid_split(dataset, labels, validation_split)\n",
        "  if num_class == 2:\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "  else:\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    target_train = target_train.long()\n",
        "    target_valid = target_valid.long()\n",
        "  \n",
        "  for e in range(1, epochs+1):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    predictions = model(feature_train).squeeze(1)\n",
        "    loss = loss_fn(predictions, target_train)\n",
        "    acc = check_accuracy(predictions, target_train, num_class)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    epoch_loss = loss.item()\n",
        "    epoch_acc = acc\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      predictions_valid = model(feature_valid).squeeze(1)\n",
        "      loss = loss_fn(predictions_valid, target_valid)\n",
        "      acc = check_accuracy(predictions_valid, target_valid, num_class)\n",
        "      valid_loss = loss.item()\n",
        "      valid_acc = acc\n",
        "    \n",
        "    print(f'| Epoch: {e:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {epoch_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
        "    \n",
        "  return valid_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Trr4zsc9w2mq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "34d4b30c-9fe2-4706-a6b2-d93fd8b9ea92"
      },
      "cell_type": "code",
      "source": [
        "!pip install GPy GPyOpt\n",
        "\n",
        "import GPy\n",
        "import GPyOpt\n",
        "from GPyOpt.methods import BayesianOptimization"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting GPy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/7d/e55ffc3b16b68e8b50ccecacec56715bcf49d5c2f204f5ba60374d419611/GPy-1.9.6.tar.gz (873kB)\n",
            "\u001b[K    100% |████████████████████████████████| 880kB 20.0MB/s \n",
            "\u001b[?25hCollecting GPyOpt\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/40/ca8f080d74d9f4e29069faa944fcfb083e8693b6daaba0f1e4bc65c88650/GPyOpt-1.2.5.tar.gz (55kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 20.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from GPy) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.6/dist-packages (from GPy) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from GPy) (1.11.0)\n",
            "Collecting paramz>=0.9.0 (from GPy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/78/b0f0164a32518bfd3b98cb2e149b7a4d5504d13fb503b31a6c59b958ed18/paramz-0.9.4.tar.gz (70kB)\n",
            "\u001b[K    100% |████████████████████████████████| 71kB 22.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.6/dist-packages (from paramz>=0.9.0->GPy) (4.3.2)\n",
            "Building wheels for collected packages: GPy, GPyOpt, paramz\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/97/82/1d/32a361e1ff2b4d9129a60343831dd99cdc74440e2db1c55264\n",
            "  Building wheel for GPyOpt (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/33/1d/87/dc02440831ba986b1547dd11a7dcd44e893b0527083066d869\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a9/fc/74/3bbd263c43ed98d67343df24cebf0a0ee34afee40d769fda9c\n",
            "Successfully built GPy GPyOpt paramz\n",
            "Installing collected packages: paramz, GPy, GPyOpt\n",
            "Successfully installed GPy-1.9.6 GPyOpt-1.2.5 paramz-0.9.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GjVTGf4aSCMw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use GPyOpt for hyper-parameter search\n",
        "domain = [{'name': 'embedding_dim', 'type': 'discrete', 'domain': (100, 120, 140, 160, 180, 200)},\n",
        "          {'name': 'n_out_channels', 'type': 'discrete', 'domain': (100, 120, 140, 160, 180, 200)},\n",
        "          {'name': 'hidden_dim', 'type': 'discrete', 'domain': (32, 64, 96, 128)},\n",
        "          {'name': 'drop_out', 'type': 'continuous', 'domain': (0.2, 0.5)},\n",
        "          {'name': 'lr', 'type': 'continuous', 'domain': (0.0001, 0.01)},\n",
        "          {'name': 'momentum', 'type': 'continuous', 'domain': (0.5, 0.9)}\n",
        "         ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tUy4fD3jwuqX",
        "colab_type": "code",
        "outputId": "6a11e5d2-0c1e-4cc9-9c86-f82c86c63c9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4267
        }
      },
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(word2index)\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# number of filters\n",
        "N_OUT_CHANNELS = 100\n",
        "# window size\n",
        "WINDOW_SIZE = 1\n",
        "\n",
        "def taskA_tuning(params):\n",
        "  param = params[0]\n",
        "  # CNN(vocab_size, embedding_dim, out_channels, window_size, output_dim, dropout)\n",
        "  model = CNN(INPUT_DIM, int(param[0]), int(param[1]), WINDOW_SIZE, OUTPUT_DIM, param[3])\n",
        "  # LSTMClassifier(vocab_size, embedding_dim, hidden_dim, label_size, output_dim, dropout)\n",
        "  # model = LSTMClassifier(INPUT_DIM, int(param[0]), int(param[2]), 2, OUTPUT_DIM, param[3])\n",
        "  optimizer = optim.SGD(model.parameters(), lr=param[4], momentum=param[5])\n",
        "  acc = train_part(model, tweets_a_tensor, labels_a_tensor, optimizer, epochs=10)\n",
        "  return acc\n",
        "\n",
        "taskA_opt = BayesianOptimization(f=taskA_tuning,\n",
        "                                 domain=domain,\n",
        "                                 model_type='GP',\n",
        "                                 acquisition_type='EI',\n",
        "                                 acquisition_jitter=0.05,\n",
        "                                 maximize=True)\n",
        "\n",
        "taskA_opt.run_optimization(max_iter=20)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 0.667 | Train Acc: 62.48% | Val. Loss: 0.658 | Val. Acc: 64.95% |\n",
            "| Epoch: 02 | Train Loss: 0.662 | Train Acc: 63.55% | Val. Loss: 0.657 | Val. Acc: 64.99% |\n",
            "| Epoch: 03 | Train Loss: 0.663 | Train Acc: 63.41% | Val. Loss: 0.657 | Val. Acc: 65.22% |\n",
            "| Epoch: 04 | Train Loss: 0.662 | Train Acc: 64.13% | Val. Loss: 0.656 | Val. Acc: 65.48% |\n",
            "| Epoch: 05 | Train Loss: 0.661 | Train Acc: 63.90% | Val. Loss: 0.656 | Val. Acc: 65.82% |\n",
            "| Epoch: 06 | Train Loss: 0.665 | Train Acc: 64.11% | Val. Loss: 0.655 | Val. Acc: 65.94% |\n",
            "| Epoch: 07 | Train Loss: 0.661 | Train Acc: 64.67% | Val. Loss: 0.655 | Val. Acc: 66.05% |\n",
            "| Epoch: 08 | Train Loss: 0.659 | Train Acc: 64.93% | Val. Loss: 0.655 | Val. Acc: 66.05% |\n",
            "| Epoch: 09 | Train Loss: 0.659 | Train Acc: 65.22% | Val. Loss: 0.655 | Val. Acc: 66.09% |\n",
            "| Epoch: 10 | Train Loss: 0.660 | Train Acc: 64.79% | Val. Loss: 0.654 | Val. Acc: 66.13% |\n",
            "| Epoch: 01 | Train Loss: 0.743 | Train Acc: 46.81% | Val. Loss: 0.701 | Val. Acc: 47.28% |\n",
            "| Epoch: 02 | Train Loss: 0.726 | Train Acc: 48.42% | Val. Loss: 0.681 | Val. Acc: 57.10% |\n",
            "| Epoch: 03 | Train Loss: 0.705 | Train Acc: 53.02% | Val. Loss: 0.665 | Val. Acc: 62.88% |\n",
            "| Epoch: 04 | Train Loss: 0.689 | Train Acc: 56.52% | Val. Loss: 0.654 | Val. Acc: 65.52% |\n",
            "| Epoch: 05 | Train Loss: 0.680 | Train Acc: 58.94% | Val. Loss: 0.649 | Val. Acc: 65.82% |\n",
            "| Epoch: 06 | Train Loss: 0.678 | Train Acc: 60.17% | Val. Loss: 0.646 | Val. Acc: 66.13% |\n",
            "| Epoch: 07 | Train Loss: 0.671 | Train Acc: 62.50% | Val. Loss: 0.646 | Val. Acc: 66.31% |\n",
            "| Epoch: 08 | Train Loss: 0.671 | Train Acc: 62.72% | Val. Loss: 0.645 | Val. Acc: 66.24% |\n",
            "| Epoch: 09 | Train Loss: 0.670 | Train Acc: 63.51% | Val. Loss: 0.645 | Val. Acc: 66.31% |\n",
            "| Epoch: 10 | Train Loss: 0.671 | Train Acc: 63.72% | Val. Loss: 0.645 | Val. Acc: 66.31% |\n",
            "| Epoch: 01 | Train Loss: 0.683 | Train Acc: 65.64% | Val. Loss: 0.662 | Val. Acc: 66.31% |\n",
            "| Epoch: 02 | Train Loss: 0.677 | Train Acc: 65.23% | Val. Loss: 0.658 | Val. Acc: 66.31% |\n",
            "| Epoch: 03 | Train Loss: 0.677 | Train Acc: 64.65% | Val. Loss: 0.656 | Val. Acc: 66.20% |\n",
            "| Epoch: 04 | Train Loss: 0.673 | Train Acc: 63.95% | Val. Loss: 0.655 | Val. Acc: 66.09% |\n",
            "| Epoch: 05 | Train Loss: 0.669 | Train Acc: 63.78% | Val. Loss: 0.655 | Val. Acc: 66.05% |\n",
            "| Epoch: 06 | Train Loss: 0.673 | Train Acc: 63.35% | Val. Loss: 0.654 | Val. Acc: 66.05% |\n",
            "| Epoch: 07 | Train Loss: 0.672 | Train Acc: 63.21% | Val. Loss: 0.653 | Val. Acc: 66.13% |\n",
            "| Epoch: 08 | Train Loss: 0.669 | Train Acc: 63.59% | Val. Loss: 0.653 | Val. Acc: 66.20% |\n",
            "| Epoch: 09 | Train Loss: 0.668 | Train Acc: 63.63% | Val. Loss: 0.652 | Val. Acc: 66.20% |\n",
            "| Epoch: 10 | Train Loss: 0.666 | Train Acc: 64.29% | Val. Loss: 0.652 | Val. Acc: 66.24% |\n",
            "| Epoch: 01 | Train Loss: 0.862 | Train Acc: 36.61% | Val. Loss: 0.816 | Val. Acc: 34.29% |\n",
            "| Epoch: 02 | Train Loss: 0.843 | Train Acc: 37.30% | Val. Loss: 0.780 | Val. Acc: 35.50% |\n",
            "| Epoch: 03 | Train Loss: 0.807 | Train Acc: 39.80% | Val. Loss: 0.742 | Val. Acc: 38.71% |\n",
            "| Epoch: 04 | Train Loss: 0.765 | Train Acc: 44.16% | Val. Loss: 0.708 | Val. Acc: 45.96% |\n",
            "| Epoch: 05 | Train Loss: 0.733 | Train Acc: 48.21% | Val. Loss: 0.682 | Val. Acc: 56.31% |\n",
            "| Epoch: 06 | Train Loss: 0.704 | Train Acc: 53.47% | Val. Loss: 0.665 | Val. Acc: 63.86% |\n",
            "| Epoch: 07 | Train Loss: 0.691 | Train Acc: 56.78% | Val. Loss: 0.655 | Val. Acc: 65.56% |\n",
            "| Epoch: 08 | Train Loss: 0.679 | Train Acc: 60.43% | Val. Loss: 0.651 | Val. Acc: 66.28% |\n",
            "| Epoch: 09 | Train Loss: 0.670 | Train Acc: 62.82% | Val. Loss: 0.650 | Val. Acc: 66.31% |\n",
            "| Epoch: 10 | Train Loss: 0.672 | Train Acc: 63.99% | Val. Loss: 0.651 | Val. Acc: 66.39% |\n",
            "| Epoch: 01 | Train Loss: 0.791 | Train Acc: 37.68% | Val. Loss: 0.717 | Val. Acc: 42.60% |\n",
            "| Epoch: 02 | Train Loss: 0.730 | Train Acc: 45.19% | Val. Loss: 0.662 | Val. Acc: 63.29% |\n",
            "| Epoch: 03 | Train Loss: 0.673 | Train Acc: 59.68% | Val. Loss: 0.652 | Val. Acc: 66.13% |\n",
            "| Epoch: 04 | Train Loss: 0.663 | Train Acc: 65.55% | Val. Loss: 0.675 | Val. Acc: 66.43% |\n",
            "| Epoch: 05 | Train Loss: 0.685 | Train Acc: 66.75% | Val. Loss: 0.697 | Val. Acc: 66.43% |\n",
            "| Epoch: 06 | Train Loss: 0.701 | Train Acc: 66.84% | Val. Loss: 0.700 | Val. Acc: 66.43% |\n",
            "| Epoch: 07 | Train Loss: 0.707 | Train Acc: 66.83% | Val. Loss: 0.684 | Val. Acc: 66.43% |\n",
            "| Epoch: 08 | Train Loss: 0.689 | Train Acc: 66.81% | Val. Loss: 0.661 | Val. Acc: 66.43% |\n",
            "| Epoch: 09 | Train Loss: 0.668 | Train Acc: 66.62% | Val. Loss: 0.646 | Val. Acc: 66.35% |\n",
            "| Epoch: 10 | Train Loss: 0.653 | Train Acc: 66.01% | Val. Loss: 0.646 | Val. Acc: 66.01% |\n",
            "| Epoch: 01 | Train Loss: 0.907 | Train Acc: 33.53% | Val. Loss: 0.793 | Val. Acc: 34.14% |\n",
            "| Epoch: 02 | Train Loss: 0.801 | Train Acc: 36.40% | Val. Loss: 0.698 | Val. Acc: 49.92% |\n",
            "| Epoch: 03 | Train Loss: 0.704 | Train Acc: 50.97% | Val. Loss: 0.655 | Val. Acc: 65.79% |\n",
            "| Epoch: 04 | Train Loss: 0.661 | Train Acc: 63.32% | Val. Loss: 0.646 | Val. Acc: 66.43% |\n",
            "| Epoch: 05 | Train Loss: 0.651 | Train Acc: 66.30% | Val. Loss: 0.647 | Val. Acc: 66.43% |\n",
            "| Epoch: 06 | Train Loss: 0.653 | Train Acc: 66.43% | Val. Loss: 0.648 | Val. Acc: 66.43% |\n",
            "| Epoch: 07 | Train Loss: 0.653 | Train Acc: 66.78% | Val. Loss: 0.647 | Val. Acc: 66.43% |\n",
            "| Epoch: 08 | Train Loss: 0.654 | Train Acc: 66.59% | Val. Loss: 0.645 | Val. Acc: 66.43% |\n",
            "| Epoch: 09 | Train Loss: 0.653 | Train Acc: 66.56% | Val. Loss: 0.644 | Val. Acc: 66.43% |\n",
            "| Epoch: 10 | Train Loss: 0.650 | Train Acc: 66.32% | Val. Loss: 0.643 | Val. Acc: 66.43% |\n",
            "| Epoch: 01 | Train Loss: 0.662 | Train Acc: 65.83% | Val. Loss: 0.653 | Val. Acc: 66.16% |\n",
            "| Epoch: 02 | Train Loss: 0.658 | Train Acc: 65.91% | Val. Loss: 0.652 | Val. Acc: 66.16% |\n",
            "| Epoch: 03 | Train Loss: 0.661 | Train Acc: 65.98% | Val. Loss: 0.652 | Val. Acc: 66.16% |\n",
            "| Epoch: 04 | Train Loss: 0.659 | Train Acc: 65.89% | Val. Loss: 0.651 | Val. Acc: 66.16% |\n",
            "| Epoch: 05 | Train Loss: 0.659 | Train Acc: 65.70% | Val. Loss: 0.651 | Val. Acc: 66.20% |\n",
            "| Epoch: 06 | Train Loss: 0.659 | Train Acc: 65.54% | Val. Loss: 0.650 | Val. Acc: 66.28% |\n",
            "| Epoch: 07 | Train Loss: 0.657 | Train Acc: 65.94% | Val. Loss: 0.650 | Val. Acc: 66.28% |\n",
            "| Epoch: 08 | Train Loss: 0.656 | Train Acc: 65.90% | Val. Loss: 0.650 | Val. Acc: 66.28% |\n",
            "| Epoch: 09 | Train Loss: 0.659 | Train Acc: 65.83% | Val. Loss: 0.650 | Val. Acc: 66.28% |\n",
            "| Epoch: 10 | Train Loss: 0.657 | Train Acc: 65.78% | Val. Loss: 0.649 | Val. Acc: 66.31% |\n",
            "| Epoch: 01 | Train Loss: 0.797 | Train Acc: 37.13% | Val. Loss: 0.771 | Val. Acc: 34.59% |\n",
            "| Epoch: 02 | Train Loss: 0.786 | Train Acc: 38.27% | Val. Loss: 0.759 | Val. Acc: 35.46% |\n",
            "| Epoch: 03 | Train Loss: 0.778 | Train Acc: 39.08% | Val. Loss: 0.745 | Val. Acc: 36.71% |\n",
            "| Epoch: 04 | Train Loss: 0.760 | Train Acc: 40.82% | Val. Loss: 0.732 | Val. Acc: 38.90% |\n",
            "| Epoch: 05 | Train Loss: 0.746 | Train Acc: 42.43% | Val. Loss: 0.720 | Val. Acc: 42.15% |\n",
            "| Epoch: 06 | Train Loss: 0.734 | Train Acc: 44.91% | Val. Loss: 0.709 | Val. Acc: 45.32% |\n",
            "| Epoch: 07 | Train Loss: 0.720 | Train Acc: 47.18% | Val. Loss: 0.700 | Val. Acc: 49.51% |\n",
            "| Epoch: 08 | Train Loss: 0.715 | Train Acc: 48.11% | Val. Loss: 0.692 | Val. Acc: 51.40% |\n",
            "| Epoch: 09 | Train Loss: 0.705 | Train Acc: 50.72% | Val. Loss: 0.685 | Val. Acc: 54.38% |\n",
            "| Epoch: 10 | Train Loss: 0.698 | Train Acc: 52.73% | Val. Loss: 0.679 | Val. Acc: 57.25% |\n",
            "| Epoch: 01 | Train Loss: 0.758 | Train Acc: 40.16% | Val. Loss: 0.707 | Val. Acc: 46.03% |\n",
            "| Epoch: 02 | Train Loss: 0.722 | Train Acc: 46.15% | Val. Loss: 0.672 | Val. Acc: 59.97% |\n",
            "| Epoch: 03 | Train Loss: 0.685 | Train Acc: 56.01% | Val. Loss: 0.655 | Val. Acc: 63.60% |\n",
            "| Epoch: 04 | Train Loss: 0.667 | Train Acc: 61.98% | Val. Loss: 0.650 | Val. Acc: 65.63% |\n",
            "| Epoch: 05 | Train Loss: 0.660 | Train Acc: 64.43% | Val. Loss: 0.649 | Val. Acc: 65.97% |\n",
            "| Epoch: 06 | Train Loss: 0.659 | Train Acc: 65.16% | Val. Loss: 0.649 | Val. Acc: 66.28% |\n",
            "| Epoch: 07 | Train Loss: 0.658 | Train Acc: 65.72% | Val. Loss: 0.649 | Val. Acc: 66.24% |\n",
            "| Epoch: 08 | Train Loss: 0.660 | Train Acc: 66.02% | Val. Loss: 0.648 | Val. Acc: 66.28% |\n",
            "| Epoch: 09 | Train Loss: 0.657 | Train Acc: 65.66% | Val. Loss: 0.647 | Val. Acc: 66.28% |\n",
            "| Epoch: 10 | Train Loss: 0.658 | Train Acc: 65.86% | Val. Loss: 0.646 | Val. Acc: 66.28% |\n",
            "| Epoch: 01 | Train Loss: 0.721 | Train Acc: 51.68% | Val. Loss: 0.689 | Val. Acc: 54.08% |\n",
            "| Epoch: 02 | Train Loss: 0.725 | Train Acc: 51.75% | Val. Loss: 0.689 | Val. Acc: 54.31% |\n",
            "| Epoch: 03 | Train Loss: 0.720 | Train Acc: 52.16% | Val. Loss: 0.688 | Val. Acc: 54.57% |\n",
            "| Epoch: 04 | Train Loss: 0.725 | Train Acc: 51.56% | Val. Loss: 0.687 | Val. Acc: 54.68% |\n",
            "| Epoch: 05 | Train Loss: 0.720 | Train Acc: 52.15% | Val. Loss: 0.687 | Val. Acc: 54.87% |\n",
            "| Epoch: 06 | Train Loss: 0.725 | Train Acc: 51.73% | Val. Loss: 0.686 | Val. Acc: 55.14% |\n",
            "| Epoch: 07 | Train Loss: 0.725 | Train Acc: 51.82% | Val. Loss: 0.685 | Val. Acc: 55.48% |\n",
            "| Epoch: 08 | Train Loss: 0.721 | Train Acc: 52.05% | Val. Loss: 0.684 | Val. Acc: 55.74% |\n",
            "| Epoch: 09 | Train Loss: 0.717 | Train Acc: 52.55% | Val. Loss: 0.683 | Val. Acc: 56.38% |\n",
            "| Epoch: 10 | Train Loss: 0.714 | Train Acc: 52.65% | Val. Loss: 0.682 | Val. Acc: 56.91% |\n",
            "| Epoch: 01 | Train Loss: 0.682 | Train Acc: 58.28% | Val. Loss: 0.659 | Val. Acc: 63.94% |\n",
            "| Epoch: 02 | Train Loss: 0.678 | Train Acc: 59.86% | Val. Loss: 0.656 | Val. Acc: 64.61% |\n",
            "| Epoch: 03 | Train Loss: 0.681 | Train Acc: 61.28% | Val. Loss: 0.655 | Val. Acc: 65.79% |\n",
            "| Epoch: 04 | Train Loss: 0.674 | Train Acc: 63.41% | Val. Loss: 0.655 | Val. Acc: 66.16% |\n",
            "| Epoch: 05 | Train Loss: 0.676 | Train Acc: 63.94% | Val. Loss: 0.655 | Val. Acc: 66.31% |\n",
            "| Epoch: 06 | Train Loss: 0.672 | Train Acc: 64.52% | Val. Loss: 0.653 | Val. Acc: 66.39% |\n",
            "| Epoch: 07 | Train Loss: 0.671 | Train Acc: 64.35% | Val. Loss: 0.651 | Val. Acc: 66.43% |\n",
            "| Epoch: 08 | Train Loss: 0.668 | Train Acc: 64.28% | Val. Loss: 0.649 | Val. Acc: 66.47% |\n",
            "| Epoch: 09 | Train Loss: 0.664 | Train Acc: 63.96% | Val. Loss: 0.648 | Val. Acc: 66.47% |\n",
            "| Epoch: 10 | Train Loss: 0.664 | Train Acc: 64.02% | Val. Loss: 0.647 | Val. Acc: 66.39% |\n",
            "| Epoch: 01 | Train Loss: 0.674 | Train Acc: 64.20% | Val. Loss: 0.657 | Val. Acc: 66.05% |\n",
            "| Epoch: 02 | Train Loss: 0.673 | Train Acc: 64.51% | Val. Loss: 0.657 | Val. Acc: 66.05% |\n",
            "| Epoch: 03 | Train Loss: 0.673 | Train Acc: 63.94% | Val. Loss: 0.656 | Val. Acc: 66.05% |\n",
            "| Epoch: 04 | Train Loss: 0.667 | Train Acc: 64.66% | Val. Loss: 0.656 | Val. Acc: 66.05% |\n",
            "| Epoch: 05 | Train Loss: 0.673 | Train Acc: 63.83% | Val. Loss: 0.656 | Val. Acc: 66.05% |\n",
            "| Epoch: 06 | Train Loss: 0.673 | Train Acc: 63.96% | Val. Loss: 0.655 | Val. Acc: 66.05% |\n",
            "| Epoch: 07 | Train Loss: 0.670 | Train Acc: 64.67% | Val. Loss: 0.655 | Val. Acc: 66.05% |\n",
            "| Epoch: 08 | Train Loss: 0.672 | Train Acc: 63.72% | Val. Loss: 0.655 | Val. Acc: 66.05% |\n",
            "| Epoch: 09 | Train Loss: 0.670 | Train Acc: 63.92% | Val. Loss: 0.654 | Val. Acc: 66.13% |\n",
            "| Epoch: 10 | Train Loss: 0.668 | Train Acc: 64.00% | Val. Loss: 0.654 | Val. Acc: 66.13% |\n",
            "| Epoch: 01 | Train Loss: 0.789 | Train Acc: 40.16% | Val. Loss: 0.738 | Val. Acc: 37.08% |\n",
            "| Epoch: 02 | Train Loss: 0.756 | Train Acc: 43.68% | Val. Loss: 0.700 | Val. Acc: 49.24% |\n",
            "| Epoch: 03 | Train Loss: 0.711 | Train Acc: 50.88% | Val. Loss: 0.672 | Val. Acc: 61.74% |\n",
            "| Epoch: 04 | Train Loss: 0.686 | Train Acc: 56.88% | Val. Loss: 0.659 | Val. Acc: 65.82% |\n",
            "| Epoch: 05 | Train Loss: 0.673 | Train Acc: 61.18% | Val. Loss: 0.654 | Val. Acc: 66.16% |\n",
            "| Epoch: 06 | Train Loss: 0.668 | Train Acc: 63.64% | Val. Loss: 0.654 | Val. Acc: 66.35% |\n",
            "| Epoch: 07 | Train Loss: 0.667 | Train Acc: 64.87% | Val. Loss: 0.654 | Val. Acc: 66.35% |\n",
            "| Epoch: 08 | Train Loss: 0.665 | Train Acc: 65.25% | Val. Loss: 0.655 | Val. Acc: 66.35% |\n",
            "| Epoch: 09 | Train Loss: 0.666 | Train Acc: 65.66% | Val. Loss: 0.654 | Val. Acc: 66.35% |\n",
            "| Epoch: 10 | Train Loss: 0.668 | Train Acc: 65.39% | Val. Loss: 0.654 | Val. Acc: 66.35% |\n",
            "| Epoch: 01 | Train Loss: 0.905 | Train Acc: 34.93% | Val. Loss: 0.822 | Val. Acc: 34.03% |\n",
            "| Epoch: 02 | Train Loss: 0.840 | Train Acc: 37.00% | Val. Loss: 0.732 | Val. Acc: 40.18% |\n",
            "| Epoch: 03 | Train Loss: 0.747 | Train Acc: 44.56% | Val. Loss: 0.667 | Val. Acc: 63.44% |\n",
            "| Epoch: 04 | Train Loss: 0.685 | Train Acc: 56.82% | Val. Loss: 0.651 | Val. Acc: 66.39% |\n",
            "| Epoch: 05 | Train Loss: 0.669 | Train Acc: 64.68% | Val. Loss: 0.675 | Val. Acc: 66.43% |\n",
            "| Epoch: 06 | Train Loss: 0.685 | Train Acc: 66.72% | Val. Loss: 0.712 | Val. Acc: 66.43% |\n",
            "| Epoch: 07 | Train Loss: 0.715 | Train Acc: 66.83% | Val. Loss: 0.741 | Val. Acc: 66.43% |\n",
            "| Epoch: 08 | Train Loss: 0.744 | Train Acc: 66.86% | Val. Loss: 0.750 | Val. Acc: 66.43% |\n",
            "| Epoch: 09 | Train Loss: 0.753 | Train Acc: 66.85% | Val. Loss: 0.739 | Val. Acc: 66.43% |\n",
            "| Epoch: 10 | Train Loss: 0.744 | Train Acc: 66.84% | Val. Loss: 0.713 | Val. Acc: 66.43% |\n",
            "| Epoch: 01 | Train Loss: 0.833 | Train Acc: 38.78% | Val. Loss: 0.786 | Val. Acc: 34.52% |\n",
            "| Epoch: 02 | Train Loss: 0.817 | Train Acc: 39.82% | Val. Loss: 0.760 | Val. Acc: 36.14% |\n",
            "| Epoch: 03 | Train Loss: 0.787 | Train Acc: 42.61% | Val. Loss: 0.732 | Val. Acc: 40.63% |\n",
            "| Epoch: 04 | Train Loss: 0.760 | Train Acc: 44.80% | Val. Loss: 0.706 | Val. Acc: 47.28% |\n",
            "| Epoch: 05 | Train Loss: 0.734 | Train Acc: 48.93% | Val. Loss: 0.684 | Val. Acc: 54.65% |\n",
            "| Epoch: 06 | Train Loss: 0.711 | Train Acc: 52.70% | Val. Loss: 0.668 | Val. Acc: 60.76% |\n",
            "| Epoch: 07 | Train Loss: 0.697 | Train Acc: 54.88% | Val. Loss: 0.658 | Val. Acc: 64.61% |\n",
            "| Epoch: 08 | Train Loss: 0.684 | Train Acc: 58.14% | Val. Loss: 0.651 | Val. Acc: 65.45% |\n",
            "| Epoch: 09 | Train Loss: 0.675 | Train Acc: 60.03% | Val. Loss: 0.648 | Val. Acc: 66.01% |\n",
            "| Epoch: 10 | Train Loss: 0.670 | Train Acc: 61.39% | Val. Loss: 0.646 | Val. Acc: 66.24% |\n",
            "| Epoch: 01 | Train Loss: 0.931 | Train Acc: 35.67% | Val. Loss: 0.857 | Val. Acc: 33.80% |\n",
            "| Epoch: 02 | Train Loss: 0.888 | Train Acc: 36.18% | Val. Loss: 0.784 | Val. Acc: 35.80% |\n",
            "| Epoch: 03 | Train Loss: 0.807 | Train Acc: 40.65% | Val. Loss: 0.714 | Val. Acc: 43.88% |\n",
            "| Epoch: 04 | Train Loss: 0.742 | Train Acc: 47.71% | Val. Loss: 0.668 | Val. Acc: 62.39% |\n",
            "| Epoch: 05 | Train Loss: 0.690 | Train Acc: 56.52% | Val. Loss: 0.649 | Val. Acc: 66.05% |\n",
            "| Epoch: 06 | Train Loss: 0.666 | Train Acc: 62.89% | Val. Loss: 0.649 | Val. Acc: 66.43% |\n",
            "| Epoch: 07 | Train Loss: 0.668 | Train Acc: 64.82% | Val. Loss: 0.658 | Val. Acc: 66.43% |\n",
            "| Epoch: 08 | Train Loss: 0.680 | Train Acc: 65.77% | Val. Loss: 0.667 | Val. Acc: 66.43% |\n",
            "| Epoch: 09 | Train Loss: 0.688 | Train Acc: 66.42% | Val. Loss: 0.672 | Val. Acc: 66.43% |\n",
            "| Epoch: 10 | Train Loss: 0.688 | Train Acc: 66.51% | Val. Loss: 0.672 | Val. Acc: 66.43% |\n",
            "| Epoch: 01 | Train Loss: 0.738 | Train Acc: 45.87% | Val. Loss: 0.682 | Val. Acc: 57.48% |\n",
            "| Epoch: 02 | Train Loss: 0.699 | Train Acc: 53.97% | Val. Loss: 0.655 | Val. Acc: 65.18% |\n",
            "| Epoch: 03 | Train Loss: 0.674 | Train Acc: 60.81% | Val. Loss: 0.651 | Val. Acc: 66.13% |\n",
            "| Epoch: 04 | Train Loss: 0.667 | Train Acc: 64.34% | Val. Loss: 0.654 | Val. Acc: 66.35% |\n",
            "| Epoch: 05 | Train Loss: 0.668 | Train Acc: 65.86% | Val. Loss: 0.655 | Val. Acc: 66.35% |\n",
            "| Epoch: 06 | Train Loss: 0.667 | Train Acc: 65.86% | Val. Loss: 0.653 | Val. Acc: 66.35% |\n",
            "| Epoch: 07 | Train Loss: 0.665 | Train Acc: 65.63% | Val. Loss: 0.650 | Val. Acc: 66.28% |\n",
            "| Epoch: 08 | Train Loss: 0.664 | Train Acc: 65.56% | Val. Loss: 0.649 | Val. Acc: 66.24% |\n",
            "| Epoch: 09 | Train Loss: 0.663 | Train Acc: 64.82% | Val. Loss: 0.648 | Val. Acc: 66.24% |\n",
            "| Epoch: 10 | Train Loss: 0.662 | Train Acc: 64.73% | Val. Loss: 0.648 | Val. Acc: 66.20% |\n",
            "| Epoch: 01 | Train Loss: 0.823 | Train Acc: 37.49% | Val. Loss: 0.800 | Val. Acc: 34.14% |\n",
            "| Epoch: 02 | Train Loss: 0.818 | Train Acc: 37.47% | Val. Loss: 0.792 | Val. Acc: 34.18% |\n",
            "| Epoch: 03 | Train Loss: 0.812 | Train Acc: 37.66% | Val. Loss: 0.783 | Val. Acc: 34.25% |\n",
            "| Epoch: 04 | Train Loss: 0.799 | Train Acc: 39.00% | Val. Loss: 0.774 | Val. Acc: 35.35% |\n",
            "| Epoch: 05 | Train Loss: 0.791 | Train Acc: 39.67% | Val. Loss: 0.765 | Val. Acc: 35.95% |\n",
            "| Epoch: 06 | Train Loss: 0.784 | Train Acc: 40.04% | Val. Loss: 0.757 | Val. Acc: 36.44% |\n",
            "| Epoch: 07 | Train Loss: 0.775 | Train Acc: 41.00% | Val. Loss: 0.749 | Val. Acc: 37.16% |\n",
            "| Epoch: 08 | Train Loss: 0.766 | Train Acc: 41.44% | Val. Loss: 0.741 | Val. Acc: 37.99% |\n",
            "| Epoch: 09 | Train Loss: 0.761 | Train Acc: 42.94% | Val. Loss: 0.734 | Val. Acc: 38.44% |\n",
            "| Epoch: 10 | Train Loss: 0.751 | Train Acc: 44.02% | Val. Loss: 0.728 | Val. Acc: 39.92% |\n",
            "| Epoch: 01 | Train Loss: 0.692 | Train Acc: 57.72% | Val. Loss: 0.660 | Val. Acc: 65.45% |\n",
            "| Epoch: 02 | Train Loss: 0.687 | Train Acc: 59.41% | Val. Loss: 0.656 | Val. Acc: 65.90% |\n",
            "| Epoch: 03 | Train Loss: 0.687 | Train Acc: 59.39% | Val. Loss: 0.654 | Val. Acc: 66.28% |\n",
            "| Epoch: 04 | Train Loss: 0.682 | Train Acc: 62.05% | Val. Loss: 0.654 | Val. Acc: 66.31% |\n",
            "| Epoch: 05 | Train Loss: 0.681 | Train Acc: 63.18% | Val. Loss: 0.655 | Val. Acc: 66.43% |\n",
            "| Epoch: 06 | Train Loss: 0.683 | Train Acc: 63.47% | Val. Loss: 0.656 | Val. Acc: 66.50% |\n",
            "| Epoch: 07 | Train Loss: 0.683 | Train Acc: 64.63% | Val. Loss: 0.656 | Val. Acc: 66.50% |\n",
            "| Epoch: 08 | Train Loss: 0.681 | Train Acc: 64.71% | Val. Loss: 0.655 | Val. Acc: 66.47% |\n",
            "| Epoch: 09 | Train Loss: 0.678 | Train Acc: 64.07% | Val. Loss: 0.652 | Val. Acc: 66.50% |\n",
            "| Epoch: 10 | Train Loss: 0.677 | Train Acc: 64.36% | Val. Loss: 0.650 | Val. Acc: 66.50% |\n",
            "| Epoch: 01 | Train Loss: 0.845 | Train Acc: 38.69% | Val. Loss: 0.775 | Val. Acc: 35.95% |\n",
            "| Epoch: 02 | Train Loss: 0.808 | Train Acc: 40.64% | Val. Loss: 0.726 | Val. Acc: 42.15% |\n",
            "| Epoch: 03 | Train Loss: 0.756 | Train Acc: 45.51% | Val. Loss: 0.688 | Val. Acc: 54.12% |\n",
            "| Epoch: 04 | Train Loss: 0.717 | Train Acc: 51.10% | Val. Loss: 0.664 | Val. Acc: 61.67% |\n",
            "| Epoch: 05 | Train Loss: 0.691 | Train Acc: 56.52% | Val. Loss: 0.652 | Val. Acc: 64.92% |\n",
            "| Epoch: 06 | Train Loss: 0.680 | Train Acc: 59.32% | Val. Loss: 0.648 | Val. Acc: 65.67% |\n",
            "| Epoch: 07 | Train Loss: 0.671 | Train Acc: 61.51% | Val. Loss: 0.646 | Val. Acc: 66.01% |\n",
            "| Epoch: 08 | Train Loss: 0.675 | Train Acc: 61.98% | Val. Loss: 0.646 | Val. Acc: 66.16% |\n",
            "| Epoch: 09 | Train Loss: 0.673 | Train Acc: 63.37% | Val. Loss: 0.646 | Val. Acc: 66.28% |\n",
            "| Epoch: 10 | Train Loss: 0.678 | Train Acc: 63.26% | Val. Loss: 0.646 | Val. Acc: 66.35% |\n",
            "| Epoch: 01 | Train Loss: 0.784 | Train Acc: 39.89% | Val. Loss: 0.739 | Val. Acc: 37.39% |\n",
            "| Epoch: 02 | Train Loss: 0.753 | Train Acc: 43.78% | Val. Loss: 0.706 | Val. Acc: 47.13% |\n",
            "| Epoch: 03 | Train Loss: 0.722 | Train Acc: 49.08% | Val. Loss: 0.678 | Val. Acc: 58.50% |\n",
            "| Epoch: 04 | Train Loss: 0.695 | Train Acc: 54.90% | Val. Loss: 0.661 | Val. Acc: 64.73% |\n",
            "| Epoch: 05 | Train Loss: 0.675 | Train Acc: 60.03% | Val. Loss: 0.655 | Val. Acc: 66.05% |\n",
            "| Epoch: 06 | Train Loss: 0.670 | Train Acc: 63.74% | Val. Loss: 0.656 | Val. Acc: 66.39% |\n",
            "| Epoch: 07 | Train Loss: 0.669 | Train Acc: 65.31% | Val. Loss: 0.660 | Val. Acc: 66.39% |\n",
            "| Epoch: 08 | Train Loss: 0.671 | Train Acc: 66.00% | Val. Loss: 0.662 | Val. Acc: 66.39% |\n",
            "| Epoch: 09 | Train Loss: 0.676 | Train Acc: 65.82% | Val. Loss: 0.663 | Val. Acc: 66.43% |\n",
            "| Epoch: 10 | Train Loss: 0.675 | Train Acc: 66.24% | Val. Loss: 0.661 | Val. Acc: 66.43% |\n",
            "| Epoch: 01 | Train Loss: 0.678 | Train Acc: 59.16% | Val. Loss: 0.660 | Val. Acc: 64.31% |\n",
            "| Epoch: 02 | Train Loss: 0.676 | Train Acc: 59.53% | Val. Loss: 0.657 | Val. Acc: 65.14% |\n",
            "| Epoch: 03 | Train Loss: 0.673 | Train Acc: 60.50% | Val. Loss: 0.655 | Val. Acc: 65.45% |\n",
            "| Epoch: 04 | Train Loss: 0.671 | Train Acc: 61.76% | Val. Loss: 0.653 | Val. Acc: 65.63% |\n",
            "| Epoch: 05 | Train Loss: 0.670 | Train Acc: 62.69% | Val. Loss: 0.651 | Val. Acc: 65.86% |\n",
            "| Epoch: 06 | Train Loss: 0.664 | Train Acc: 63.91% | Val. Loss: 0.650 | Val. Acc: 65.94% |\n",
            "| Epoch: 07 | Train Loss: 0.665 | Train Acc: 63.97% | Val. Loss: 0.650 | Val. Acc: 66.05% |\n",
            "| Epoch: 08 | Train Loss: 0.664 | Train Acc: 64.53% | Val. Loss: 0.649 | Val. Acc: 66.13% |\n",
            "| Epoch: 09 | Train Loss: 0.664 | Train Acc: 64.85% | Val. Loss: 0.649 | Val. Acc: 66.09% |\n",
            "| Epoch: 10 | Train Loss: 0.664 | Train Acc: 64.71% | Val. Loss: 0.649 | Val. Acc: 66.09% |\n",
            "| Epoch: 01 | Train Loss: 0.672 | Train Acc: 62.35% | Val. Loss: 0.659 | Val. Acc: 65.75% |\n",
            "| Epoch: 02 | Train Loss: 0.667 | Train Acc: 63.07% | Val. Loss: 0.658 | Val. Acc: 65.82% |\n",
            "| Epoch: 03 | Train Loss: 0.663 | Train Acc: 63.40% | Val. Loss: 0.657 | Val. Acc: 66.16% |\n",
            "| Epoch: 04 | Train Loss: 0.668 | Train Acc: 63.94% | Val. Loss: 0.656 | Val. Acc: 66.39% |\n",
            "| Epoch: 05 | Train Loss: 0.667 | Train Acc: 63.95% | Val. Loss: 0.655 | Val. Acc: 66.50% |\n",
            "| Epoch: 06 | Train Loss: 0.663 | Train Acc: 64.58% | Val. Loss: 0.655 | Val. Acc: 66.47% |\n",
            "| Epoch: 07 | Train Loss: 0.664 | Train Acc: 65.11% | Val. Loss: 0.654 | Val. Acc: 66.47% |\n",
            "| Epoch: 08 | Train Loss: 0.660 | Train Acc: 65.84% | Val. Loss: 0.654 | Val. Acc: 66.47% |\n",
            "| Epoch: 09 | Train Loss: 0.664 | Train Acc: 65.85% | Val. Loss: 0.654 | Val. Acc: 66.47% |\n",
            "| Epoch: 10 | Train Loss: 0.663 | Train Acc: 65.98% | Val. Loss: 0.654 | Val. Acc: 66.43% |\n",
            "| Epoch: 01 | Train Loss: 0.684 | Train Acc: 57.08% | Val. Loss: 0.658 | Val. Acc: 65.45% |\n",
            "| Epoch: 02 | Train Loss: 0.673 | Train Acc: 60.55% | Val. Loss: 0.651 | Val. Acc: 65.97% |\n",
            "| Epoch: 03 | Train Loss: 0.664 | Train Acc: 63.18% | Val. Loss: 0.648 | Val. Acc: 66.31% |\n",
            "| Epoch: 04 | Train Loss: 0.658 | Train Acc: 64.99% | Val. Loss: 0.649 | Val. Acc: 66.43% |\n",
            "| Epoch: 05 | Train Loss: 0.662 | Train Acc: 65.46% | Val. Loss: 0.649 | Val. Acc: 66.43% |\n",
            "| Epoch: 06 | Train Loss: 0.662 | Train Acc: 65.94% | Val. Loss: 0.648 | Val. Acc: 66.43% |\n",
            "| Epoch: 07 | Train Loss: 0.661 | Train Acc: 66.05% | Val. Loss: 0.647 | Val. Acc: 66.43% |\n",
            "| Epoch: 08 | Train Loss: 0.658 | Train Acc: 65.88% | Val. Loss: 0.646 | Val. Acc: 66.43% |\n",
            "| Epoch: 09 | Train Loss: 0.657 | Train Acc: 65.27% | Val. Loss: 0.645 | Val. Acc: 66.39% |\n",
            "| Epoch: 10 | Train Loss: 0.655 | Train Acc: 65.62% | Val. Loss: 0.644 | Val. Acc: 66.39% |\n",
            "| Epoch: 01 | Train Loss: 0.671 | Train Acc: 62.30% | Val. Loss: 0.648 | Val. Acc: 66.13% |\n",
            "| Epoch: 02 | Train Loss: 0.670 | Train Acc: 62.26% | Val. Loss: 0.648 | Val. Acc: 66.13% |\n",
            "| Epoch: 03 | Train Loss: 0.674 | Train Acc: 62.98% | Val. Loss: 0.647 | Val. Acc: 66.13% |\n",
            "| Epoch: 04 | Train Loss: 0.670 | Train Acc: 63.06% | Val. Loss: 0.647 | Val. Acc: 66.16% |\n",
            "| Epoch: 05 | Train Loss: 0.673 | Train Acc: 62.82% | Val. Loss: 0.647 | Val. Acc: 66.16% |\n",
            "| Epoch: 06 | Train Loss: 0.665 | Train Acc: 63.89% | Val. Loss: 0.647 | Val. Acc: 66.20% |\n",
            "| Epoch: 07 | Train Loss: 0.666 | Train Acc: 63.84% | Val. Loss: 0.647 | Val. Acc: 66.24% |\n",
            "| Epoch: 08 | Train Loss: 0.666 | Train Acc: 63.63% | Val. Loss: 0.647 | Val. Acc: 66.28% |\n",
            "| Epoch: 09 | Train Loss: 0.670 | Train Acc: 63.60% | Val. Loss: 0.647 | Val. Acc: 66.31% |\n",
            "| Epoch: 10 | Train Loss: 0.671 | Train Acc: 63.90% | Val. Loss: 0.647 | Val. Acc: 66.31% |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C599-qBBa147",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "38af5626-3b29-47c7-cefa-846ecc3ee994"
      },
      "cell_type": "code",
      "source": [
        "print(\"The hyperparameters used for the model in task A are\")\n",
        "print(f'embedding_dim: {int(taskA_opt.X[-1][0])}')\n",
        "print(f'out_channels: {int(taskA_opt.X[-1][1])}')\n",
        "print(f'drop_out: {taskA_opt.X[-1][3]:.2f}')\n",
        "print(f'learning_rate: {taskA_opt.X[-1][4]:.4f}')\n",
        "print(f'weight_decay: {taskA_opt.X[-1][5]:.4f}')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The hyperparameters used for the model in task A are\n",
            "embedding_dim: 100\n",
            "out_channels: 180\n",
            "drop_out: 0.42\n",
            "learning_rate: 0.0015\n",
            "weight_decay: 0.8655\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U68vARmFtLqU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "a055b161-92ee-4e2a-b5d9-9068f63e5bb2"
      },
      "cell_type": "code",
      "source": [
        "# CNN(vocab_size, embedding_dim, out_channels, window_size, output_dim, dropout)\n",
        "test_model_A = CNN(INPUT_DIM, int(taskA_opt.X[-1][0]), int(taskA_opt.X[-1][1]), WINDOW_SIZE, OUTPUT_DIM, taskA_opt.X[-1][3])\n",
        "# LSTMClassifier(vocab_size, embedding_dim, hidden_dim, label_size, output_dim, dropout)\n",
        "# test_model_A = LSTMClassifier(INPUT_DIM, int(param[0]), int(param[2]), 2, OUTPUT_DIM, param[3])\n",
        "optimizer_A = optim.SGD(test_model_A.parameters(), lr=taskA_opt.X[-1][4], momentum=taskA_opt.X[-1][5])\n",
        "train_part(test_model_A, tweets_a_tensor, labels_a_tensor, optimizer_A, epochs=50)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 0.956 | Train Acc: 35.33% | Val. Loss: 0.913 | Val. Acc: 33.72% |\n",
            "| Epoch: 02 | Train Loss: 0.932 | Train Acc: 36.01% | Val. Loss: 0.867 | Val. Acc: 34.18% |\n",
            "| Epoch: 03 | Train Loss: 0.890 | Train Acc: 37.26% | Val. Loss: 0.811 | Val. Acc: 35.23% |\n",
            "| Epoch: 04 | Train Loss: 0.834 | Train Acc: 39.61% | Val. Loss: 0.756 | Val. Acc: 38.07% |\n",
            "| Epoch: 05 | Train Loss: 0.779 | Train Acc: 43.70% | Val. Loss: 0.708 | Val. Acc: 45.35% |\n",
            "| Epoch: 06 | Train Loss: 0.729 | Train Acc: 49.36% | Val. Loss: 0.674 | Val. Acc: 59.89% |\n",
            "| Epoch: 07 | Train Loss: 0.699 | Train Acc: 55.17% | Val. Loss: 0.655 | Val. Acc: 65.86% |\n",
            "| Epoch: 08 | Train Loss: 0.674 | Train Acc: 60.98% | Val. Loss: 0.648 | Val. Acc: 66.43% |\n",
            "| Epoch: 09 | Train Loss: 0.670 | Train Acc: 62.91% | Val. Loss: 0.650 | Val. Acc: 66.43% |\n",
            "| Epoch: 10 | Train Loss: 0.671 | Train Acc: 64.71% | Val. Loss: 0.658 | Val. Acc: 66.43% |\n",
            "| Epoch: 11 | Train Loss: 0.673 | Train Acc: 66.12% | Val. Loss: 0.668 | Val. Acc: 66.43% |\n",
            "| Epoch: 12 | Train Loss: 0.682 | Train Acc: 66.43% | Val. Loss: 0.676 | Val. Acc: 66.43% |\n",
            "| Epoch: 13 | Train Loss: 0.690 | Train Acc: 66.37% | Val. Loss: 0.682 | Val. Acc: 66.43% |\n",
            "| Epoch: 14 | Train Loss: 0.695 | Train Acc: 66.43% | Val. Loss: 0.685 | Val. Acc: 66.43% |\n",
            "| Epoch: 15 | Train Loss: 0.697 | Train Acc: 66.59% | Val. Loss: 0.685 | Val. Acc: 66.43% |\n",
            "| Epoch: 16 | Train Loss: 0.696 | Train Acc: 66.71% | Val. Loss: 0.682 | Val. Acc: 66.43% |\n",
            "| Epoch: 17 | Train Loss: 0.695 | Train Acc: 66.66% | Val. Loss: 0.678 | Val. Acc: 66.43% |\n",
            "| Epoch: 18 | Train Loss: 0.689 | Train Acc: 66.46% | Val. Loss: 0.671 | Val. Acc: 66.43% |\n",
            "| Epoch: 19 | Train Loss: 0.683 | Train Acc: 66.47% | Val. Loss: 0.665 | Val. Acc: 66.43% |\n",
            "| Epoch: 20 | Train Loss: 0.681 | Train Acc: 66.29% | Val. Loss: 0.659 | Val. Acc: 66.43% |\n",
            "| Epoch: 21 | Train Loss: 0.678 | Train Acc: 66.01% | Val. Loss: 0.654 | Val. Acc: 66.43% |\n",
            "| Epoch: 22 | Train Loss: 0.670 | Train Acc: 65.97% | Val. Loss: 0.650 | Val. Acc: 66.43% |\n",
            "| Epoch: 23 | Train Loss: 0.667 | Train Acc: 65.55% | Val. Loss: 0.647 | Val. Acc: 66.43% |\n",
            "| Epoch: 24 | Train Loss: 0.664 | Train Acc: 64.99% | Val. Loss: 0.646 | Val. Acc: 66.43% |\n",
            "| Epoch: 25 | Train Loss: 0.661 | Train Acc: 64.81% | Val. Loss: 0.645 | Val. Acc: 66.43% |\n",
            "| Epoch: 26 | Train Loss: 0.663 | Train Acc: 64.10% | Val. Loss: 0.646 | Val. Acc: 66.43% |\n",
            "| Epoch: 27 | Train Loss: 0.663 | Train Acc: 63.40% | Val. Loss: 0.646 | Val. Acc: 66.43% |\n",
            "| Epoch: 28 | Train Loss: 0.662 | Train Acc: 63.41% | Val. Loss: 0.647 | Val. Acc: 66.39% |\n",
            "| Epoch: 29 | Train Loss: 0.666 | Train Acc: 62.30% | Val. Loss: 0.647 | Val. Acc: 66.35% |\n",
            "| Epoch: 30 | Train Loss: 0.668 | Train Acc: 62.10% | Val. Loss: 0.647 | Val. Acc: 66.35% |\n",
            "| Epoch: 31 | Train Loss: 0.665 | Train Acc: 62.17% | Val. Loss: 0.647 | Val. Acc: 66.35% |\n",
            "| Epoch: 32 | Train Loss: 0.672 | Train Acc: 61.79% | Val. Loss: 0.647 | Val. Acc: 66.39% |\n",
            "| Epoch: 33 | Train Loss: 0.662 | Train Acc: 62.61% | Val. Loss: 0.646 | Val. Acc: 66.43% |\n",
            "| Epoch: 34 | Train Loss: 0.664 | Train Acc: 62.75% | Val. Loss: 0.645 | Val. Acc: 66.47% |\n",
            "| Epoch: 35 | Train Loss: 0.667 | Train Acc: 62.58% | Val. Loss: 0.645 | Val. Acc: 66.43% |\n",
            "| Epoch: 36 | Train Loss: 0.666 | Train Acc: 63.36% | Val. Loss: 0.644 | Val. Acc: 66.43% |\n",
            "| Epoch: 37 | Train Loss: 0.661 | Train Acc: 63.90% | Val. Loss: 0.644 | Val. Acc: 66.43% |\n",
            "| Epoch: 38 | Train Loss: 0.659 | Train Acc: 64.46% | Val. Loss: 0.644 | Val. Acc: 66.43% |\n",
            "| Epoch: 39 | Train Loss: 0.662 | Train Acc: 64.42% | Val. Loss: 0.644 | Val. Acc: 66.43% |\n",
            "| Epoch: 40 | Train Loss: 0.657 | Train Acc: 64.03% | Val. Loss: 0.644 | Val. Acc: 66.43% |\n",
            "| Epoch: 41 | Train Loss: 0.660 | Train Acc: 64.84% | Val. Loss: 0.644 | Val. Acc: 66.43% |\n",
            "| Epoch: 42 | Train Loss: 0.658 | Train Acc: 64.76% | Val. Loss: 0.644 | Val. Acc: 66.43% |\n",
            "| Epoch: 43 | Train Loss: 0.661 | Train Acc: 64.60% | Val. Loss: 0.644 | Val. Acc: 66.43% |\n",
            "| Epoch: 44 | Train Loss: 0.663 | Train Acc: 65.10% | Val. Loss: 0.644 | Val. Acc: 66.43% |\n",
            "| Epoch: 45 | Train Loss: 0.660 | Train Acc: 65.45% | Val. Loss: 0.644 | Val. Acc: 66.43% |\n",
            "| Epoch: 46 | Train Loss: 0.659 | Train Acc: 65.34% | Val. Loss: 0.644 | Val. Acc: 66.43% |\n",
            "| Epoch: 47 | Train Loss: 0.662 | Train Acc: 64.67% | Val. Loss: 0.643 | Val. Acc: 66.43% |\n",
            "| Epoch: 48 | Train Loss: 0.657 | Train Acc: 65.42% | Val. Loss: 0.643 | Val. Acc: 66.43% |\n",
            "| Epoch: 49 | Train Loss: 0.660 | Train Acc: 64.57% | Val. Loss: 0.643 | Val. Acc: 66.43% |\n",
            "| Epoch: 50 | Train Loss: 0.658 | Train Acc: 65.33% | Val. Loss: 0.643 | Val. Acc: 66.43% |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6643)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "metadata": {
        "id": "UfdKZF-bCkZG",
        "colab_type": "code",
        "outputId": "58ed15c2-f720-442c-eb4c-d0ce1866aee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4267
        }
      },
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(word2index)\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# number of filters\n",
        "N_OUT_CHANNELS = 100\n",
        "# window size\n",
        "WINDOW_SIZE = 1\n",
        "\n",
        "def taskB_tuning(params):\n",
        "  param = params[0]\n",
        "  # CNN(vocab_size, embedding_dim, out_channels, window_size, output_dim, dropout)\n",
        "  model = CNN(INPUT_DIM, int(param[0]), int(param[1]), WINDOW_SIZE, OUTPUT_DIM, param[3])\n",
        "  # LSTMClassifier(vocab_size, embedding_dim, hidden_dim, label_size, output_dim, dropout)\n",
        "  # model = LSTMClassifier(INPUT_DIM, int(param[0]), int(param[2]), 2, OUTPUT_DIM, param[3])\n",
        "  optimizer = optim.SGD(model.parameters(), lr=param[4], momentum=param[5])\n",
        "  acc = train_part(model, tweets_b_tensor, labels_b_tensor, optimizer, epochs=10)\n",
        "  return acc\n",
        "\n",
        "taskB_opt = BayesianOptimization(f=taskB_tuning,\n",
        "                                 domain=domain,\n",
        "                                 model_type='GP',\n",
        "                                 acquisition_type='EI',\n",
        "                                 acquisition_jitter=0.05,\n",
        "                                 maximize=True)\n",
        "\n",
        "taskB_opt.run_optimization(max_iter=20)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 0.743 | Train Acc: 47.30% | Val. Loss: 0.603 | Val. Acc: 83.52% |\n",
            "| Epoch: 02 | Train Loss: 0.626 | Train Acc: 67.10% | Val. Loss: 0.481 | Val. Acc: 89.32% |\n",
            "| Epoch: 03 | Train Loss: 0.504 | Train Acc: 82.78% | Val. Loss: 0.406 | Val. Acc: 89.43% |\n",
            "| Epoch: 04 | Train Loss: 0.432 | Train Acc: 87.44% | Val. Loss: 0.368 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.396 | Train Acc: 87.70% | Val. Loss: 0.351 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.382 | Train Acc: 87.76% | Val. Loss: 0.343 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.377 | Train Acc: 87.76% | Val. Loss: 0.340 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.373 | Train Acc: 87.76% | Val. Loss: 0.339 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.373 | Train Acc: 87.76% | Val. Loss: 0.338 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.371 | Train Acc: 87.76% | Val. Loss: 0.338 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.740 | Train Acc: 47.95% | Val. Loss: 0.564 | Val. Acc: 87.16% |\n",
            "| Epoch: 02 | Train Loss: 0.595 | Train Acc: 70.40% | Val. Loss: 0.416 | Val. Acc: 89.43% |\n",
            "| Epoch: 03 | Train Loss: 0.451 | Train Acc: 86.11% | Val. Loss: 0.348 | Val. Acc: 89.43% |\n",
            "| Epoch: 04 | Train Loss: 0.386 | Train Acc: 87.81% | Val. Loss: 0.338 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.380 | Train Acc: 87.76% | Val. Loss: 0.349 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.398 | Train Acc: 87.76% | Val. Loss: 0.362 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.409 | Train Acc: 87.76% | Val. Loss: 0.372 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.425 | Train Acc: 87.76% | Val. Loss: 0.376 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.429 | Train Acc: 87.76% | Val. Loss: 0.374 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.424 | Train Acc: 87.76% | Val. Loss: 0.368 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.845 | Train Acc: 31.96% | Val. Loss: 0.739 | Val. Acc: 33.07% |\n",
            "| Epoch: 02 | Train Loss: 0.761 | Train Acc: 43.84% | Val. Loss: 0.625 | Val. Acc: 76.25% |\n",
            "| Epoch: 03 | Train Loss: 0.651 | Train Acc: 62.02% | Val. Loss: 0.519 | Val. Acc: 89.32% |\n",
            "| Epoch: 04 | Train Loss: 0.541 | Train Acc: 78.10% | Val. Loss: 0.439 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.471 | Train Acc: 85.45% | Val. Loss: 0.389 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.417 | Train Acc: 87.41% | Val. Loss: 0.360 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.397 | Train Acc: 87.56% | Val. Loss: 0.345 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.380 | Train Acc: 87.76% | Val. Loss: 0.338 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.375 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.376 | Train Acc: 87.73% | Val. Loss: 0.335 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.545 | Train Acc: 78.12% | Val. Loss: 0.466 | Val. Acc: 89.43% |\n",
            "| Epoch: 02 | Train Loss: 0.498 | Train Acc: 82.41% | Val. Loss: 0.415 | Val. Acc: 89.43% |\n",
            "| Epoch: 03 | Train Loss: 0.454 | Train Acc: 85.06% | Val. Loss: 0.379 | Val. Acc: 89.43% |\n",
            "| Epoch: 04 | Train Loss: 0.416 | Train Acc: 86.93% | Val. Loss: 0.357 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.393 | Train Acc: 87.61% | Val. Loss: 0.346 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.381 | Train Acc: 87.73% | Val. Loss: 0.340 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.377 | Train Acc: 87.76% | Val. Loss: 0.338 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.381 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.374 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.375 | Train Acc: 87.73% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.650 | Train Acc: 61.16% | Val. Loss: 0.550 | Val. Acc: 87.61% |\n",
            "| Epoch: 02 | Train Loss: 0.597 | Train Acc: 69.60% | Val. Loss: 0.466 | Val. Acc: 89.43% |\n",
            "| Epoch: 03 | Train Loss: 0.506 | Train Acc: 80.62% | Val. Loss: 0.395 | Val. Acc: 89.43% |\n",
            "| Epoch: 04 | Train Loss: 0.433 | Train Acc: 86.42% | Val. Loss: 0.353 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.391 | Train Acc: 87.78% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.376 | Train Acc: 87.70% | Val. Loss: 0.339 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.381 | Train Acc: 87.76% | Val. Loss: 0.349 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.402 | Train Acc: 87.78% | Val. Loss: 0.362 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.408 | Train Acc: 87.76% | Val. Loss: 0.373 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.427 | Train Acc: 87.76% | Val. Loss: 0.383 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.508 | Train Acc: 81.02% | Val. Loss: 0.446 | Val. Acc: 89.20% |\n",
            "| Epoch: 02 | Train Loss: 0.479 | Train Acc: 83.84% | Val. Loss: 0.408 | Val. Acc: 89.43% |\n",
            "| Epoch: 03 | Train Loss: 0.442 | Train Acc: 86.53% | Val. Loss: 0.379 | Val. Acc: 89.43% |\n",
            "| Epoch: 04 | Train Loss: 0.414 | Train Acc: 86.85% | Val. Loss: 0.360 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.399 | Train Acc: 87.47% | Val. Loss: 0.348 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.387 | Train Acc: 87.76% | Val. Loss: 0.342 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.383 | Train Acc: 87.61% | Val. Loss: 0.339 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.372 | Train Acc: 87.73% | Val. Loss: 0.338 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.377 | Train Acc: 87.67% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.374 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.476 | Train Acc: 84.86% | Val. Loss: 0.424 | Val. Acc: 89.32% |\n",
            "| Epoch: 02 | Train Loss: 0.448 | Train Acc: 86.59% | Val. Loss: 0.386 | Val. Acc: 89.43% |\n",
            "| Epoch: 03 | Train Loss: 0.414 | Train Acc: 87.39% | Val. Loss: 0.360 | Val. Acc: 89.43% |\n",
            "| Epoch: 04 | Train Loss: 0.390 | Train Acc: 87.70% | Val. Loss: 0.345 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.380 | Train Acc: 87.76% | Val. Loss: 0.339 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.374 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.374 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.375 | Train Acc: 87.76% | Val. Loss: 0.338 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.372 | Train Acc: 87.78% | Val. Loss: 0.338 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.376 | Train Acc: 87.76% | Val. Loss: 0.338 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.851 | Train Acc: 24.43% | Val. Loss: 0.776 | Val. Acc: 25.91% |\n",
            "| Epoch: 02 | Train Loss: 0.791 | Train Acc: 33.27% | Val. Loss: 0.685 | Val. Acc: 54.77% |\n",
            "| Epoch: 03 | Train Loss: 0.697 | Train Acc: 53.78% | Val. Loss: 0.592 | Val. Acc: 85.45% |\n",
            "| Epoch: 04 | Train Loss: 0.613 | Train Acc: 73.12% | Val. Loss: 0.513 | Val. Acc: 89.32% |\n",
            "| Epoch: 05 | Train Loss: 0.531 | Train Acc: 84.91% | Val. Loss: 0.454 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.475 | Train Acc: 87.13% | Val. Loss: 0.412 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.435 | Train Acc: 87.64% | Val. Loss: 0.385 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.410 | Train Acc: 87.76% | Val. Loss: 0.368 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.395 | Train Acc: 87.76% | Val. Loss: 0.357 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.383 | Train Acc: 87.76% | Val. Loss: 0.350 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.977 | Train Acc: 17.95% | Val. Loss: 0.728 | Val. Acc: 39.09% |\n",
            "| Epoch: 02 | Train Loss: 0.748 | Train Acc: 43.24% | Val. Loss: 0.488 | Val. Acc: 89.43% |\n",
            "| Epoch: 03 | Train Loss: 0.510 | Train Acc: 84.91% | Val. Loss: 0.369 | Val. Acc: 89.43% |\n",
            "| Epoch: 04 | Train Loss: 0.398 | Train Acc: 87.73% | Val. Loss: 0.334 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.368 | Train Acc: 87.76% | Val. Loss: 0.331 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.367 | Train Acc: 87.76% | Val. Loss: 0.335 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.378 | Train Acc: 87.76% | Val. Loss: 0.339 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.382 | Train Acc: 87.76% | Val. Loss: 0.340 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.384 | Train Acc: 87.76% | Val. Loss: 0.339 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.381 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.542 | Train Acc: 83.84% | Val. Loss: 0.479 | Val. Acc: 89.32% |\n",
            "| Epoch: 02 | Train Loss: 0.499 | Train Acc: 86.22% | Val. Loss: 0.424 | Val. Acc: 89.43% |\n",
            "| Epoch: 03 | Train Loss: 0.445 | Train Acc: 87.56% | Val. Loss: 0.384 | Val. Acc: 89.43% |\n",
            "| Epoch: 04 | Train Loss: 0.409 | Train Acc: 87.73% | Val. Loss: 0.359 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.387 | Train Acc: 87.76% | Val. Loss: 0.347 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.375 | Train Acc: 87.76% | Val. Loss: 0.341 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.368 | Train Acc: 87.76% | Val. Loss: 0.338 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.369 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.369 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.368 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.727 | Train Acc: 48.04% | Val. Loss: 0.590 | Val. Acc: 86.48% |\n",
            "| Epoch: 02 | Train Loss: 0.608 | Train Acc: 70.45% | Val. Loss: 0.464 | Val. Acc: 89.43% |\n",
            "| Epoch: 03 | Train Loss: 0.493 | Train Acc: 84.23% | Val. Loss: 0.386 | Val. Acc: 89.43% |\n",
            "| Epoch: 04 | Train Loss: 0.416 | Train Acc: 87.47% | Val. Loss: 0.351 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.389 | Train Acc: 87.76% | Val. Loss: 0.338 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.380 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.375 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.376 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.376 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.380 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.644 | Train Acc: 63.95% | Val. Loss: 0.550 | Val. Acc: 88.52% |\n",
            "| Epoch: 02 | Train Loss: 0.571 | Train Acc: 76.85% | Val. Loss: 0.466 | Val. Acc: 89.43% |\n",
            "| Epoch: 03 | Train Loss: 0.489 | Train Acc: 86.25% | Val. Loss: 0.402 | Val. Acc: 89.43% |\n",
            "| Epoch: 04 | Train Loss: 0.428 | Train Acc: 87.50% | Val. Loss: 0.364 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.395 | Train Acc: 87.73% | Val. Loss: 0.345 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.382 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.376 | Train Acc: 87.76% | Val. Loss: 0.335 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.378 | Train Acc: 87.76% | Val. Loss: 0.334 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.373 | Train Acc: 87.76% | Val. Loss: 0.334 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.378 | Train Acc: 87.76% | Val. Loss: 0.335 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.764 | Train Acc: 40.34% | Val. Loss: 0.633 | Val. Acc: 75.00% |\n",
            "| Epoch: 02 | Train Loss: 0.654 | Train Acc: 62.44% | Val. Loss: 0.493 | Val. Acc: 89.09% |\n",
            "| Epoch: 03 | Train Loss: 0.519 | Train Acc: 84.01% | Val. Loss: 0.396 | Val. Acc: 89.43% |\n",
            "| Epoch: 04 | Train Loss: 0.421 | Train Acc: 87.61% | Val. Loss: 0.349 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.380 | Train Acc: 87.76% | Val. Loss: 0.335 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.370 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.378 | Train Acc: 87.76% | Val. Loss: 0.343 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.387 | Train Acc: 87.76% | Val. Loss: 0.350 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.393 | Train Acc: 87.76% | Val. Loss: 0.354 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.399 | Train Acc: 87.76% | Val. Loss: 0.355 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.591 | Train Acc: 70.80% | Val. Loss: 0.503 | Val. Acc: 88.52% |\n",
            "| Epoch: 02 | Train Loss: 0.526 | Train Acc: 81.22% | Val. Loss: 0.438 | Val. Acc: 89.32% |\n",
            "| Epoch: 03 | Train Loss: 0.466 | Train Acc: 86.02% | Val. Loss: 0.391 | Val. Acc: 89.43% |\n",
            "| Epoch: 04 | Train Loss: 0.427 | Train Acc: 87.13% | Val. Loss: 0.365 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.394 | Train Acc: 87.70% | Val. Loss: 0.351 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.388 | Train Acc: 87.76% | Val. Loss: 0.345 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.380 | Train Acc: 87.76% | Val. Loss: 0.342 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.375 | Train Acc: 87.76% | Val. Loss: 0.340 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.377 | Train Acc: 87.76% | Val. Loss: 0.340 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.377 | Train Acc: 87.76% | Val. Loss: 0.339 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.687 | Train Acc: 54.91% | Val. Loss: 0.671 | Val. Acc: 60.34% |\n",
            "| Epoch: 02 | Train Loss: 0.678 | Train Acc: 57.22% | Val. Loss: 0.654 | Val. Acc: 67.84% |\n",
            "| Epoch: 03 | Train Loss: 0.660 | Train Acc: 60.94% | Val. Loss: 0.633 | Val. Acc: 76.14% |\n",
            "| Epoch: 04 | Train Loss: 0.646 | Train Acc: 65.09% | Val. Loss: 0.609 | Val. Acc: 83.75% |\n",
            "| Epoch: 05 | Train Loss: 0.617 | Train Acc: 70.62% | Val. Loss: 0.583 | Val. Acc: 86.59% |\n",
            "| Epoch: 06 | Train Loss: 0.592 | Train Acc: 75.94% | Val. Loss: 0.558 | Val. Acc: 88.30% |\n",
            "| Epoch: 07 | Train Loss: 0.569 | Train Acc: 79.52% | Val. Loss: 0.534 | Val. Acc: 88.98% |\n",
            "| Epoch: 08 | Train Loss: 0.550 | Train Acc: 82.39% | Val. Loss: 0.511 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.526 | Train Acc: 85.26% | Val. Loss: 0.490 | Val. Acc: 89.55% |\n",
            "| Epoch: 10 | Train Loss: 0.501 | Train Acc: 85.85% | Val. Loss: 0.471 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 1.023 | Train Acc: 21.56% | Val. Loss: 0.810 | Val. Acc: 17.84% |\n",
            "| Epoch: 02 | Train Loss: 0.837 | Train Acc: 35.60% | Val. Loss: 0.591 | Val. Acc: 84.55% |\n",
            "| Epoch: 03 | Train Loss: 0.625 | Train Acc: 65.94% | Val. Loss: 0.442 | Val. Acc: 89.43% |\n",
            "| Epoch: 04 | Train Loss: 0.477 | Train Acc: 84.06% | Val. Loss: 0.370 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.413 | Train Acc: 87.33% | Val. Loss: 0.344 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.381 | Train Acc: 87.76% | Val. Loss: 0.338 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.380 | Train Acc: 87.73% | Val. Loss: 0.338 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.384 | Train Acc: 87.76% | Val. Loss: 0.340 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.388 | Train Acc: 87.76% | Val. Loss: 0.341 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.389 | Train Acc: 87.76% | Val. Loss: 0.342 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.960 | Train Acc: 27.30% | Val. Loss: 0.854 | Val. Acc: 14.66% |\n",
            "| Epoch: 02 | Train Loss: 0.896 | Train Acc: 30.68% | Val. Loss: 0.746 | Val. Acc: 32.16% |\n",
            "| Epoch: 03 | Train Loss: 0.778 | Train Acc: 43.41% | Val. Loss: 0.643 | Val. Acc: 68.30% |\n",
            "| Epoch: 04 | Train Loss: 0.676 | Train Acc: 56.16% | Val. Loss: 0.558 | Val. Acc: 82.73% |\n",
            "| Epoch: 05 | Train Loss: 0.591 | Train Acc: 69.69% | Val. Loss: 0.495 | Val. Acc: 87.84% |\n",
            "| Epoch: 06 | Train Loss: 0.534 | Train Acc: 75.88% | Val. Loss: 0.450 | Val. Acc: 88.86% |\n",
            "| Epoch: 07 | Train Loss: 0.489 | Train Acc: 82.07% | Val. Loss: 0.419 | Val. Acc: 89.32% |\n",
            "| Epoch: 08 | Train Loss: 0.453 | Train Acc: 84.80% | Val. Loss: 0.397 | Val. Acc: 89.32% |\n",
            "| Epoch: 09 | Train Loss: 0.439 | Train Acc: 85.65% | Val. Loss: 0.382 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.422 | Train Acc: 86.73% | Val. Loss: 0.372 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 1.181 | Train Acc: 13.41% | Val. Loss: 1.138 | Val. Acc: 10.57% |\n",
            "| Epoch: 02 | Train Loss: 1.139 | Train Acc: 13.64% | Val. Loss: 1.067 | Val. Acc: 10.68% |\n",
            "| Epoch: 03 | Train Loss: 1.070 | Train Acc: 14.83% | Val. Loss: 0.984 | Val. Acc: 10.80% |\n",
            "| Epoch: 04 | Train Loss: 0.988 | Train Acc: 18.52% | Val. Loss: 0.897 | Val. Acc: 12.39% |\n",
            "| Epoch: 05 | Train Loss: 0.910 | Train Acc: 22.36% | Val. Loss: 0.814 | Val. Acc: 16.70% |\n",
            "| Epoch: 06 | Train Loss: 0.820 | Train Acc: 31.82% | Val. Loss: 0.738 | Val. Acc: 31.82% |\n",
            "| Epoch: 07 | Train Loss: 0.748 | Train Acc: 44.23% | Val. Loss: 0.672 | Val. Acc: 60.11% |\n",
            "| Epoch: 08 | Train Loss: 0.686 | Train Acc: 55.99% | Val. Loss: 0.615 | Val. Acc: 83.41% |\n",
            "| Epoch: 09 | Train Loss: 0.632 | Train Acc: 66.42% | Val. Loss: 0.568 | Val. Acc: 88.41% |\n",
            "| Epoch: 10 | Train Loss: 0.584 | Train Acc: 75.43% | Val. Loss: 0.529 | Val. Acc: 89.55% |\n",
            "| Epoch: 01 | Train Loss: 0.795 | Train Acc: 42.36% | Val. Loss: 0.565 | Val. Acc: 85.11% |\n",
            "| Epoch: 02 | Train Loss: 0.597 | Train Acc: 68.55% | Val. Loss: 0.392 | Val. Acc: 89.43% |\n",
            "| Epoch: 03 | Train Loss: 0.433 | Train Acc: 86.51% | Val. Loss: 0.335 | Val. Acc: 89.43% |\n",
            "| Epoch: 04 | Train Loss: 0.379 | Train Acc: 87.76% | Val. Loss: 0.351 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.399 | Train Acc: 87.76% | Val. Loss: 0.388 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.445 | Train Acc: 87.76% | Val. Loss: 0.425 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.479 | Train Acc: 87.76% | Val. Loss: 0.453 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.512 | Train Acc: 87.76% | Val. Loss: 0.469 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.533 | Train Acc: 87.76% | Val. Loss: 0.474 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.535 | Train Acc: 87.76% | Val. Loss: 0.469 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.776 | Train Acc: 41.56% | Val. Loss: 0.724 | Val. Acc: 40.45% |\n",
            "| Epoch: 02 | Train Loss: 0.744 | Train Acc: 46.62% | Val. Loss: 0.674 | Val. Acc: 58.98% |\n",
            "| Epoch: 03 | Train Loss: 0.689 | Train Acc: 56.22% | Val. Loss: 0.619 | Val. Acc: 79.20% |\n",
            "| Epoch: 04 | Train Loss: 0.641 | Train Acc: 65.37% | Val. Loss: 0.566 | Val. Acc: 87.05% |\n",
            "| Epoch: 05 | Train Loss: 0.589 | Train Acc: 73.75% | Val. Loss: 0.518 | Val. Acc: 89.09% |\n",
            "| Epoch: 06 | Train Loss: 0.546 | Train Acc: 80.37% | Val. Loss: 0.479 | Val. Acc: 89.32% |\n",
            "| Epoch: 07 | Train Loss: 0.506 | Train Acc: 84.12% | Val. Loss: 0.447 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.476 | Train Acc: 86.39% | Val. Loss: 0.422 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.451 | Train Acc: 86.88% | Val. Loss: 0.403 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.431 | Train Acc: 87.44% | Val. Loss: 0.389 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.799 | Train Acc: 40.54% | Val. Loss: 0.732 | Val. Acc: 37.95% |\n",
            "| Epoch: 02 | Train Loss: 0.754 | Train Acc: 46.05% | Val. Loss: 0.660 | Val. Acc: 64.55% |\n",
            "| Epoch: 03 | Train Loss: 0.691 | Train Acc: 55.60% | Val. Loss: 0.580 | Val. Acc: 83.75% |\n",
            "| Epoch: 04 | Train Loss: 0.608 | Train Acc: 67.84% | Val. Loss: 0.507 | Val. Acc: 88.75% |\n",
            "| Epoch: 05 | Train Loss: 0.543 | Train Acc: 76.99% | Val. Loss: 0.448 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.484 | Train Acc: 82.67% | Val. Loss: 0.406 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.443 | Train Acc: 86.14% | Val. Loss: 0.377 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.415 | Train Acc: 87.22% | Val. Loss: 0.359 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.396 | Train Acc: 87.59% | Val. Loss: 0.349 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.386 | Train Acc: 87.73% | Val. Loss: 0.343 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.835 | Train Acc: 31.99% | Val. Loss: 0.674 | Val. Acc: 61.02% |\n",
            "| Epoch: 02 | Train Loss: 0.696 | Train Acc: 53.81% | Val. Loss: 0.515 | Val. Acc: 89.20% |\n",
            "| Epoch: 03 | Train Loss: 0.542 | Train Acc: 80.11% | Val. Loss: 0.408 | Val. Acc: 89.43% |\n",
            "| Epoch: 04 | Train Loss: 0.441 | Train Acc: 87.39% | Val. Loss: 0.357 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.392 | Train Acc: 87.73% | Val. Loss: 0.339 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.374 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.373 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.375 | Train Acc: 87.76% | Val. Loss: 0.339 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.385 | Train Acc: 87.76% | Val. Loss: 0.340 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.381 | Train Acc: 87.76% | Val. Loss: 0.340 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 1.153 | Train Acc: 14.12% | Val. Loss: 0.747 | Val. Acc: 32.05% |\n",
            "| Epoch: 02 | Train Loss: 0.764 | Train Acc: 42.10% | Val. Loss: 0.420 | Val. Acc: 89.43% |\n",
            "| Epoch: 03 | Train Loss: 0.446 | Train Acc: 87.02% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 04 | Train Loss: 0.371 | Train Acc: 87.76% | Val. Loss: 0.352 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.397 | Train Acc: 87.76% | Val. Loss: 0.379 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.431 | Train Acc: 87.76% | Val. Loss: 0.397 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.452 | Train Acc: 87.76% | Val. Loss: 0.403 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.454 | Train Acc: 87.76% | Val. Loss: 0.399 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.450 | Train Acc: 87.76% | Val. Loss: 0.388 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.441 | Train Acc: 87.76% | Val. Loss: 0.374 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.649 | Train Acc: 62.13% | Val. Loss: 0.586 | Val. Acc: 85.68% |\n",
            "| Epoch: 02 | Train Loss: 0.611 | Train Acc: 69.72% | Val. Loss: 0.518 | Val. Acc: 88.98% |\n",
            "| Epoch: 03 | Train Loss: 0.537 | Train Acc: 79.06% | Val. Loss: 0.458 | Val. Acc: 89.32% |\n",
            "| Epoch: 04 | Train Loss: 0.483 | Train Acc: 84.91% | Val. Loss: 0.413 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.440 | Train Acc: 86.65% | Val. Loss: 0.383 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.412 | Train Acc: 87.30% | Val. Loss: 0.364 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.397 | Train Acc: 87.70% | Val. Loss: 0.353 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.392 | Train Acc: 87.73% | Val. Loss: 0.347 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.381 | Train Acc: 87.73% | Val. Loss: 0.343 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.374 | Train Acc: 87.76% | Val. Loss: 0.341 | Val. Acc: 89.43% |\n",
            "| Epoch: 01 | Train Loss: 0.709 | Train Acc: 52.67% | Val. Loss: 0.542 | Val. Acc: 88.75% |\n",
            "| Epoch: 02 | Train Loss: 0.571 | Train Acc: 75.65% | Val. Loss: 0.411 | Val. Acc: 89.43% |\n",
            "| Epoch: 03 | Train Loss: 0.441 | Train Acc: 87.05% | Val. Loss: 0.351 | Val. Acc: 89.43% |\n",
            "| Epoch: 04 | Train Loss: 0.390 | Train Acc: 87.73% | Val. Loss: 0.334 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.372 | Train Acc: 87.76% | Val. Loss: 0.332 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.375 | Train Acc: 87.76% | Val. Loss: 0.335 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.377 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.378 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.383 | Train Acc: 87.76% | Val. Loss: 0.335 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.378 | Train Acc: 87.76% | Val. Loss: 0.334 | Val. Acc: 89.43% |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qoXwYXmH76eR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "fd14731b-04fa-43f6-cf91-184cb98839df"
      },
      "cell_type": "code",
      "source": [
        "print(\"The hyperparameters used for the model in task B are\")\n",
        "print(f'embedding_dim: {int(taskB_opt.X[-1][0])}')\n",
        "print(f'out_channels: {int(taskB_opt.X[-1][1])}')\n",
        "print(f'drop_out: {taskB_opt.X[-1][3]:.2f}')\n",
        "print(f'learning_rate: {taskB_opt.X[-1][4]:.4f}')\n",
        "print(f'weight_decay: {taskB_opt.X[-1][5]:.4f}')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The hyperparameters used for the model in task B are\n",
            "embedding_dim: 160\n",
            "out_channels: 160\n",
            "drop_out: 0.37\n",
            "learning_rate: 0.0091\n",
            "weight_decay: 0.6488\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hN2EJQ7s8Chp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "ec70c698-4787-4a60-c98e-9033902121b9"
      },
      "cell_type": "code",
      "source": [
        "# CNN(vocab_size, embedding_dim, out_channels, window_size, output_dim, dropout)\n",
        "test_model_B = CNN(INPUT_DIM, int(taskB_opt.X[-1][0]), int(taskB_opt.X[-1][1]), WINDOW_SIZE, OUTPUT_DIM, taskB_opt.X[-1][3])\n",
        "# LSTMClassifier(vocab_size, embedding_dim, hidden_dim, label_size, output_dim, dropout)\n",
        "# test_model_B = LSTMClassifier(INPUT_DIM, int(param[0]), int(param[2]), 2, OUTPUT_DIM, param[3])\n",
        "optimizer_B = optim.SGD(test_model_B.parameters(), lr=taskB_opt.X[-1][4], momentum=taskB_opt.X[-1][5])\n",
        "train_part(test_model_B, tweets_b_tensor, labels_b_tensor, optimizer_B, epochs=50)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 0.571 | Train Acc: 75.71% | Val. Loss: 0.474 | Val. Acc: 89.20% |\n",
            "| Epoch: 02 | Train Loss: 0.503 | Train Acc: 83.84% | Val. Loss: 0.399 | Val. Acc: 89.43% |\n",
            "| Epoch: 03 | Train Loss: 0.430 | Train Acc: 87.19% | Val. Loss: 0.358 | Val. Acc: 89.43% |\n",
            "| Epoch: 04 | Train Loss: 0.391 | Train Acc: 87.73% | Val. Loss: 0.341 | Val. Acc: 89.43% |\n",
            "| Epoch: 05 | Train Loss: 0.381 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 06 | Train Loss: 0.374 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 07 | Train Loss: 0.374 | Train Acc: 87.76% | Val. Loss: 0.338 | Val. Acc: 89.43% |\n",
            "| Epoch: 08 | Train Loss: 0.374 | Train Acc: 87.76% | Val. Loss: 0.338 | Val. Acc: 89.43% |\n",
            "| Epoch: 09 | Train Loss: 0.376 | Train Acc: 87.76% | Val. Loss: 0.338 | Val. Acc: 89.43% |\n",
            "| Epoch: 10 | Train Loss: 0.378 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 11 | Train Loss: 0.377 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 12 | Train Loss: 0.372 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 13 | Train Loss: 0.374 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 14 | Train Loss: 0.374 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 15 | Train Loss: 0.375 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 16 | Train Loss: 0.372 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 17 | Train Loss: 0.374 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 18 | Train Loss: 0.375 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 19 | Train Loss: 0.370 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 20 | Train Loss: 0.376 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 21 | Train Loss: 0.373 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 22 | Train Loss: 0.375 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 23 | Train Loss: 0.372 | Train Acc: 87.76% | Val. Loss: 0.337 | Val. Acc: 89.43% |\n",
            "| Epoch: 24 | Train Loss: 0.370 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 25 | Train Loss: 0.372 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 26 | Train Loss: 0.372 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 27 | Train Loss: 0.372 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 28 | Train Loss: 0.372 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 29 | Train Loss: 0.372 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 30 | Train Loss: 0.371 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 31 | Train Loss: 0.372 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 32 | Train Loss: 0.371 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 33 | Train Loss: 0.370 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 34 | Train Loss: 0.373 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 35 | Train Loss: 0.372 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 36 | Train Loss: 0.371 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 37 | Train Loss: 0.373 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 38 | Train Loss: 0.368 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 39 | Train Loss: 0.370 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 40 | Train Loss: 0.368 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 41 | Train Loss: 0.369 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 42 | Train Loss: 0.368 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 43 | Train Loss: 0.373 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 44 | Train Loss: 0.372 | Train Acc: 87.76% | Val. Loss: 0.336 | Val. Acc: 89.43% |\n",
            "| Epoch: 45 | Train Loss: 0.367 | Train Acc: 87.76% | Val. Loss: 0.335 | Val. Acc: 89.43% |\n",
            "| Epoch: 46 | Train Loss: 0.365 | Train Acc: 87.76% | Val. Loss: 0.335 | Val. Acc: 89.43% |\n",
            "| Epoch: 47 | Train Loss: 0.370 | Train Acc: 87.76% | Val. Loss: 0.335 | Val. Acc: 89.43% |\n",
            "| Epoch: 48 | Train Loss: 0.367 | Train Acc: 87.76% | Val. Loss: 0.335 | Val. Acc: 89.43% |\n",
            "| Epoch: 49 | Train Loss: 0.367 | Train Acc: 87.76% | Val. Loss: 0.335 | Val. Acc: 89.43% |\n",
            "| Epoch: 50 | Train Loss: 0.370 | Train Acc: 87.76% | Val. Loss: 0.335 | Val. Acc: 89.43% |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.8943)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "metadata": {
        "id": "m7iYqJdjSZfb",
        "colab_type": "code",
        "outputId": "6bc944b0-2a4c-4972-dcda-666bb0926330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4267
        }
      },
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(word2index)\n",
        "OUTPUT_DIM = 3\n",
        "\n",
        "# number of filters\n",
        "N_OUT_CHANNELS = 100\n",
        "# window size\n",
        "WINDOW_SIZE = 1\n",
        "\n",
        "def taskC_tuning(params):\n",
        "  param = params[0]\n",
        "  # CNN(vocab_size, embedding_dim, out_channels, window_size, output_dim, dropout)\n",
        "  model = CNN(INPUT_DIM, int(param[0]), int(param[1]), WINDOW_SIZE, OUTPUT_DIM, param[3])\n",
        "  # LSTMClassifier(vocab_size, embedding_dim, hidden_dim, label_size, output_dim, dropout)\n",
        "  # model = LSTMClassifier(INPUT_DIM, int(param[0]), int(param[2]), 2, OUTPUT_DIM, param[3])\n",
        "  optimizer = optim.SGD(model.parameters(), lr=param[4], momentum=param[5])\n",
        "  acc = train_part(model, tweets_c_tensor, labels_c_tensor, optimizer, epochs=10, num_class=3)\n",
        "  return acc\n",
        "\n",
        "taskC_opt = BayesianOptimization(f=taskC_tuning,\n",
        "                                 domain=domain,\n",
        "                                 model_type='GP',\n",
        "                                 acquisition_type='EI',\n",
        "                                 acquisition_jitter=0.05,\n",
        "                                 maximize=True)\n",
        "\n",
        "taskC_opt.run_optimization(max_iter=20)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 1.166 | Train Acc: 62.04% | Val. Loss: 1.173 | Val. Acc: 60.00% |\n",
            "| Epoch: 02 | Train Loss: 1.140 | Train Acc: 61.79% | Val. Loss: 1.127 | Val. Acc: 60.00% |\n",
            "| Epoch: 03 | Train Loss: 1.099 | Train Acc: 62.14% | Val. Loss: 1.072 | Val. Acc: 60.00% |\n",
            "| Epoch: 04 | Train Loss: 1.050 | Train Acc: 62.11% | Val. Loss: 1.018 | Val. Acc: 60.00% |\n",
            "| Epoch: 05 | Train Loss: 1.009 | Train Acc: 62.14% | Val. Loss: 0.973 | Val. Acc: 60.00% |\n",
            "| Epoch: 06 | Train Loss: 0.976 | Train Acc: 61.82% | Val. Loss: 0.940 | Val. Acc: 59.87% |\n",
            "| Epoch: 07 | Train Loss: 0.958 | Train Acc: 60.75% | Val. Loss: 0.922 | Val. Acc: 59.23% |\n",
            "| Epoch: 08 | Train Loss: 0.939 | Train Acc: 59.56% | Val. Loss: 0.915 | Val. Acc: 59.35% |\n",
            "| Epoch: 09 | Train Loss: 0.937 | Train Acc: 57.34% | Val. Loss: 0.912 | Val. Acc: 59.23% |\n",
            "| Epoch: 10 | Train Loss: 0.937 | Train Acc: 56.92% | Val. Loss: 0.911 | Val. Acc: 59.35% |\n",
            "| Epoch: 01 | Train Loss: 1.043 | Train Acc: 51.95% | Val. Loss: 0.996 | Val. Acc: 59.35% |\n",
            "| Epoch: 02 | Train Loss: 1.031 | Train Acc: 52.05% | Val. Loss: 0.977 | Val. Acc: 59.61% |\n",
            "| Epoch: 03 | Train Loss: 1.009 | Train Acc: 54.82% | Val. Loss: 0.959 | Val. Acc: 59.87% |\n",
            "| Epoch: 04 | Train Loss: 0.988 | Train Acc: 56.98% | Val. Loss: 0.945 | Val. Acc: 60.00% |\n",
            "| Epoch: 05 | Train Loss: 0.974 | Train Acc: 58.37% | Val. Loss: 0.936 | Val. Acc: 60.00% |\n",
            "| Epoch: 06 | Train Loss: 0.963 | Train Acc: 59.08% | Val. Loss: 0.930 | Val. Acc: 60.00% |\n",
            "| Epoch: 07 | Train Loss: 0.960 | Train Acc: 59.05% | Val. Loss: 0.927 | Val. Acc: 60.00% |\n",
            "| Epoch: 08 | Train Loss: 0.960 | Train Acc: 59.66% | Val. Loss: 0.925 | Val. Acc: 60.00% |\n",
            "| Epoch: 09 | Train Loss: 0.957 | Train Acc: 60.24% | Val. Loss: 0.923 | Val. Acc: 60.00% |\n",
            "| Epoch: 10 | Train Loss: 0.941 | Train Acc: 59.69% | Val. Loss: 0.920 | Val. Acc: 60.00% |\n",
            "| Epoch: 01 | Train Loss: 1.294 | Train Acc: 29.28% | Val. Loss: 0.991 | Val. Acc: 44.90% |\n",
            "| Epoch: 02 | Train Loss: 1.057 | Train Acc: 42.66% | Val. Loss: 0.947 | Val. Acc: 59.74% |\n",
            "| Epoch: 03 | Train Loss: 0.997 | Train Acc: 58.59% | Val. Loss: 0.989 | Val. Acc: 60.00% |\n",
            "| Epoch: 04 | Train Loss: 1.015 | Train Acc: 61.59% | Val. Loss: 0.980 | Val. Acc: 60.00% |\n",
            "| Epoch: 05 | Train Loss: 1.001 | Train Acc: 61.24% | Val. Loss: 0.942 | Val. Acc: 60.00% |\n",
            "| Epoch: 06 | Train Loss: 0.974 | Train Acc: 60.11% | Val. Loss: 0.920 | Val. Acc: 59.61% |\n",
            "| Epoch: 07 | Train Loss: 0.966 | Train Acc: 56.76% | Val. Loss: 0.916 | Val. Acc: 58.84% |\n",
            "| Epoch: 08 | Train Loss: 0.980 | Train Acc: 53.53% | Val. Loss: 0.914 | Val. Acc: 59.35% |\n",
            "| Epoch: 09 | Train Loss: 0.972 | Train Acc: 54.85% | Val. Loss: 0.912 | Val. Acc: 59.87% |\n",
            "| Epoch: 10 | Train Loss: 0.944 | Train Acc: 57.85% | Val. Loss: 0.913 | Val. Acc: 59.87% |\n",
            "| Epoch: 01 | Train Loss: 1.596 | Train Acc: 18.57% | Val. Loss: 1.159 | Val. Acc: 28.39% |\n",
            "| Epoch: 02 | Train Loss: 1.213 | Train Acc: 28.76% | Val. Loss: 0.947 | Val. Acc: 59.74% |\n",
            "| Epoch: 03 | Train Loss: 0.964 | Train Acc: 57.79% | Val. Loss: 1.032 | Val. Acc: 60.00% |\n",
            "| Epoch: 04 | Train Loss: 1.013 | Train Acc: 62.72% | Val. Loss: 1.092 | Val. Acc: 60.00% |\n",
            "| Epoch: 05 | Train Loss: 1.068 | Train Acc: 62.59% | Val. Loss: 1.036 | Val. Acc: 60.00% |\n",
            "| Epoch: 06 | Train Loss: 1.030 | Train Acc: 62.43% | Val. Loss: 0.957 | Val. Acc: 60.00% |\n",
            "| Epoch: 07 | Train Loss: 0.973 | Train Acc: 61.30% | Val. Loss: 0.938 | Val. Acc: 58.97% |\n",
            "| Epoch: 08 | Train Loss: 0.974 | Train Acc: 54.89% | Val. Loss: 0.938 | Val. Acc: 56.77% |\n",
            "| Epoch: 09 | Train Loss: 0.975 | Train Acc: 51.53% | Val. Loss: 0.917 | Val. Acc: 58.84% |\n",
            "| Epoch: 10 | Train Loss: 0.950 | Train Acc: 55.34% | Val. Loss: 0.907 | Val. Acc: 60.00% |\n",
            "| Epoch: 01 | Train Loss: 1.367 | Train Acc: 28.25% | Val. Loss: 1.259 | Val. Acc: 29.94% |\n",
            "| Epoch: 02 | Train Loss: 1.350 | Train Acc: 28.12% | Val. Loss: 1.217 | Val. Acc: 29.94% |\n",
            "| Epoch: 03 | Train Loss: 1.294 | Train Acc: 29.02% | Val. Loss: 1.165 | Val. Acc: 30.06% |\n",
            "| Epoch: 04 | Train Loss: 1.239 | Train Acc: 29.89% | Val. Loss: 1.108 | Val. Acc: 30.84% |\n",
            "| Epoch: 05 | Train Loss: 1.172 | Train Acc: 33.25% | Val. Loss: 1.053 | Val. Acc: 32.39% |\n",
            "| Epoch: 06 | Train Loss: 1.117 | Train Acc: 36.54% | Val. Loss: 1.007 | Val. Acc: 40.90% |\n",
            "| Epoch: 07 | Train Loss: 1.056 | Train Acc: 43.18% | Val. Loss: 0.972 | Val. Acc: 48.26% |\n",
            "| Epoch: 08 | Train Loss: 1.021 | Train Acc: 47.86% | Val. Loss: 0.950 | Val. Acc: 57.55% |\n",
            "| Epoch: 09 | Train Loss: 0.994 | Train Acc: 53.21% | Val. Loss: 0.941 | Val. Acc: 58.97% |\n",
            "| Epoch: 10 | Train Loss: 0.974 | Train Acc: 55.56% | Val. Loss: 0.941 | Val. Acc: 59.74% |\n",
            "| Epoch: 01 | Train Loss: 1.204 | Train Acc: 30.02% | Val. Loss: 0.979 | Val. Acc: 46.58% |\n",
            "| Epoch: 02 | Train Loss: 1.006 | Train Acc: 45.47% | Val. Loss: 0.931 | Val. Acc: 59.23% |\n",
            "| Epoch: 03 | Train Loss: 0.941 | Train Acc: 60.27% | Val. Loss: 0.952 | Val. Acc: 60.00% |\n",
            "| Epoch: 04 | Train Loss: 0.955 | Train Acc: 62.33% | Val. Loss: 0.949 | Val. Acc: 60.00% |\n",
            "| Epoch: 05 | Train Loss: 0.951 | Train Acc: 62.40% | Val. Loss: 0.928 | Val. Acc: 60.00% |\n",
            "| Epoch: 06 | Train Loss: 0.931 | Train Acc: 62.24% | Val. Loss: 0.915 | Val. Acc: 59.87% |\n",
            "| Epoch: 07 | Train Loss: 0.926 | Train Acc: 60.69% | Val. Loss: 0.910 | Val. Acc: 59.61% |\n",
            "| Epoch: 08 | Train Loss: 0.931 | Train Acc: 60.85% | Val. Loss: 0.909 | Val. Acc: 60.00% |\n",
            "| Epoch: 09 | Train Loss: 0.920 | Train Acc: 61.04% | Val. Loss: 0.907 | Val. Acc: 60.00% |\n",
            "| Epoch: 10 | Train Loss: 0.915 | Train Acc: 60.92% | Val. Loss: 0.907 | Val. Acc: 60.00% |\n",
            "| Epoch: 01 | Train Loss: 1.343 | Train Acc: 19.61% | Val. Loss: 1.043 | Val. Acc: 49.03% |\n",
            "| Epoch: 02 | Train Loss: 1.078 | Train Acc: 41.76% | Val. Loss: 0.932 | Val. Acc: 60.00% |\n",
            "| Epoch: 03 | Train Loss: 0.953 | Train Acc: 60.66% | Val. Loss: 0.970 | Val. Acc: 60.00% |\n",
            "| Epoch: 04 | Train Loss: 0.971 | Train Acc: 62.50% | Val. Loss: 0.986 | Val. Acc: 60.00% |\n",
            "| Epoch: 05 | Train Loss: 0.984 | Train Acc: 62.56% | Val. Loss: 0.954 | Val. Acc: 60.00% |\n",
            "| Epoch: 06 | Train Loss: 0.961 | Train Acc: 62.08% | Val. Loss: 0.925 | Val. Acc: 60.00% |\n",
            "| Epoch: 07 | Train Loss: 0.950 | Train Acc: 60.59% | Val. Loss: 0.918 | Val. Acc: 59.35% |\n",
            "| Epoch: 08 | Train Loss: 0.943 | Train Acc: 57.40% | Val. Loss: 0.910 | Val. Acc: 59.35% |\n",
            "| Epoch: 09 | Train Loss: 0.933 | Train Acc: 58.37% | Val. Loss: 0.901 | Val. Acc: 59.74% |\n",
            "| Epoch: 10 | Train Loss: 0.917 | Train Acc: 60.72% | Val. Loss: 0.900 | Val. Acc: 60.00% |\n",
            "| Epoch: 01 | Train Loss: 1.034 | Train Acc: 43.95% | Val. Loss: 0.926 | Val. Acc: 58.84% |\n",
            "| Epoch: 02 | Train Loss: 0.964 | Train Acc: 55.63% | Val. Loss: 0.934 | Val. Acc: 60.00% |\n",
            "| Epoch: 03 | Train Loss: 0.938 | Train Acc: 61.66% | Val. Loss: 0.960 | Val. Acc: 60.00% |\n",
            "| Epoch: 04 | Train Loss: 0.956 | Train Acc: 62.37% | Val. Loss: 0.941 | Val. Acc: 60.00% |\n",
            "| Epoch: 05 | Train Loss: 0.954 | Train Acc: 62.24% | Val. Loss: 0.910 | Val. Acc: 60.00% |\n",
            "| Epoch: 06 | Train Loss: 0.918 | Train Acc: 60.50% | Val. Loss: 0.901 | Val. Acc: 60.13% |\n",
            "| Epoch: 07 | Train Loss: 0.932 | Train Acc: 58.17% | Val. Loss: 0.899 | Val. Acc: 60.26% |\n",
            "| Epoch: 08 | Train Loss: 0.937 | Train Acc: 56.76% | Val. Loss: 0.894 | Val. Acc: 60.39% |\n",
            "| Epoch: 09 | Train Loss: 0.919 | Train Acc: 59.21% | Val. Loss: 0.896 | Val. Acc: 60.00% |\n",
            "| Epoch: 10 | Train Loss: 0.918 | Train Acc: 60.75% | Val. Loss: 0.900 | Val. Acc: 60.00% |\n",
            "| Epoch: 01 | Train Loss: 1.013 | Train Acc: 55.95% | Val. Loss: 0.969 | Val. Acc: 59.87% |\n",
            "| Epoch: 02 | Train Loss: 0.979 | Train Acc: 58.76% | Val. Loss: 0.946 | Val. Acc: 59.87% |\n",
            "| Epoch: 03 | Train Loss: 0.948 | Train Acc: 60.69% | Val. Loss: 0.936 | Val. Acc: 59.87% |\n",
            "| Epoch: 04 | Train Loss: 0.952 | Train Acc: 59.01% | Val. Loss: 0.931 | Val. Acc: 59.87% |\n",
            "| Epoch: 05 | Train Loss: 0.946 | Train Acc: 60.01% | Val. Loss: 0.927 | Val. Acc: 59.87% |\n",
            "| Epoch: 06 | Train Loss: 0.942 | Train Acc: 58.85% | Val. Loss: 0.923 | Val. Acc: 59.87% |\n",
            "| Epoch: 07 | Train Loss: 0.934 | Train Acc: 59.37% | Val. Loss: 0.920 | Val. Acc: 59.87% |\n",
            "| Epoch: 08 | Train Loss: 0.939 | Train Acc: 59.63% | Val. Loss: 0.917 | Val. Acc: 59.87% |\n",
            "| Epoch: 09 | Train Loss: 0.936 | Train Acc: 59.88% | Val. Loss: 0.915 | Val. Acc: 59.87% |\n",
            "| Epoch: 10 | Train Loss: 0.932 | Train Acc: 60.46% | Val. Loss: 0.912 | Val. Acc: 60.00% |\n",
            "| Epoch: 01 | Train Loss: 1.196 | Train Acc: 31.31% | Val. Loss: 0.979 | Val. Acc: 57.29% |\n",
            "| Epoch: 02 | Train Loss: 1.022 | Train Acc: 48.56% | Val. Loss: 0.934 | Val. Acc: 60.00% |\n",
            "| Epoch: 03 | Train Loss: 0.948 | Train Acc: 60.69% | Val. Loss: 0.975 | Val. Acc: 60.00% |\n",
            "| Epoch: 04 | Train Loss: 0.990 | Train Acc: 62.01% | Val. Loss: 0.973 | Val. Acc: 60.00% |\n",
            "| Epoch: 05 | Train Loss: 0.982 | Train Acc: 62.33% | Val. Loss: 0.932 | Val. Acc: 60.00% |\n",
            "| Epoch: 06 | Train Loss: 0.955 | Train Acc: 60.79% | Val. Loss: 0.911 | Val. Acc: 59.61% |\n",
            "| Epoch: 07 | Train Loss: 0.941 | Train Acc: 58.79% | Val. Loss: 0.908 | Val. Acc: 60.00% |\n",
            "| Epoch: 08 | Train Loss: 0.946 | Train Acc: 56.37% | Val. Loss: 0.899 | Val. Acc: 60.13% |\n",
            "| Epoch: 09 | Train Loss: 0.938 | Train Acc: 57.76% | Val. Loss: 0.893 | Val. Acc: 60.00% |\n",
            "| Epoch: 10 | Train Loss: 0.920 | Train Acc: 59.30% | Val. Loss: 0.896 | Val. Acc: 60.00% |\n",
            "| Epoch: 01 | Train Loss: 1.007 | Train Acc: 43.95% | Val. Loss: 0.968 | Val. Acc: 44.65% |\n",
            "| Epoch: 02 | Train Loss: 0.996 | Train Acc: 45.79% | Val. Loss: 0.967 | Val. Acc: 44.52% |\n",
            "| Epoch: 03 | Train Loss: 1.002 | Train Acc: 45.40% | Val. Loss: 0.966 | Val. Acc: 44.90% |\n",
            "| Epoch: 04 | Train Loss: 0.998 | Train Acc: 45.63% | Val. Loss: 0.964 | Val. Acc: 45.29% |\n",
            "| Epoch: 05 | Train Loss: 1.011 | Train Acc: 43.18% | Val. Loss: 0.963 | Val. Acc: 46.19% |\n",
            "| Epoch: 06 | Train Loss: 0.995 | Train Acc: 46.18% | Val. Loss: 0.962 | Val. Acc: 45.94% |\n",
            "| Epoch: 07 | Train Loss: 1.001 | Train Acc: 45.24% | Val. Loss: 0.960 | Val. Acc: 45.68% |\n",
            "| Epoch: 08 | Train Loss: 0.998 | Train Acc: 46.66% | Val. Loss: 0.959 | Val. Acc: 46.19% |\n",
            "| Epoch: 09 | Train Loss: 0.994 | Train Acc: 46.44% | Val. Loss: 0.957 | Val. Acc: 45.81% |\n",
            "| Epoch: 10 | Train Loss: 1.001 | Train Acc: 45.31% | Val. Loss: 0.956 | Val. Acc: 46.32% |\n",
            "| Epoch: 01 | Train Loss: 1.294 | Train Acc: 30.83% | Val. Loss: 1.078 | Val. Acc: 59.61% |\n",
            "| Epoch: 02 | Train Loss: 1.076 | Train Acc: 55.01% | Val. Loss: 0.972 | Val. Acc: 60.13% |\n",
            "| Epoch: 03 | Train Loss: 0.975 | Train Acc: 61.34% | Val. Loss: 0.942 | Val. Acc: 60.13% |\n",
            "| Epoch: 04 | Train Loss: 0.944 | Train Acc: 61.11% | Val. Loss: 0.934 | Val. Acc: 59.35% |\n",
            "| Epoch: 05 | Train Loss: 0.946 | Train Acc: 58.88% | Val. Loss: 0.934 | Val. Acc: 58.45% |\n",
            "| Epoch: 06 | Train Loss: 0.961 | Train Acc: 57.14% | Val. Loss: 0.932 | Val. Acc: 58.32% |\n",
            "| Epoch: 07 | Train Loss: 0.967 | Train Acc: 56.47% | Val. Loss: 0.925 | Val. Acc: 59.10% |\n",
            "| Epoch: 08 | Train Loss: 0.947 | Train Acc: 57.85% | Val. Loss: 0.920 | Val. Acc: 59.35% |\n",
            "| Epoch: 09 | Train Loss: 0.935 | Train Acc: 59.17% | Val. Loss: 0.916 | Val. Acc: 59.74% |\n",
            "| Epoch: 10 | Train Loss: 0.928 | Train Acc: 60.88% | Val. Loss: 0.912 | Val. Acc: 59.87% |\n",
            "| Epoch: 01 | Train Loss: 1.199 | Train Acc: 31.80% | Val. Loss: 0.996 | Val. Acc: 51.61% |\n",
            "| Epoch: 02 | Train Loss: 1.033 | Train Acc: 47.50% | Val. Loss: 0.942 | Val. Acc: 60.00% |\n",
            "| Epoch: 03 | Train Loss: 0.955 | Train Acc: 60.05% | Val. Loss: 0.972 | Val. Acc: 60.00% |\n",
            "| Epoch: 04 | Train Loss: 0.964 | Train Acc: 62.17% | Val. Loss: 0.971 | Val. Acc: 60.00% |\n",
            "| Epoch: 05 | Train Loss: 0.978 | Train Acc: 61.88% | Val. Loss: 0.939 | Val. Acc: 60.00% |\n",
            "| Epoch: 06 | Train Loss: 0.948 | Train Acc: 60.85% | Val. Loss: 0.919 | Val. Acc: 59.61% |\n",
            "| Epoch: 07 | Train Loss: 0.943 | Train Acc: 58.88% | Val. Loss: 0.915 | Val. Acc: 58.97% |\n",
            "| Epoch: 08 | Train Loss: 0.943 | Train Acc: 56.76% | Val. Loss: 0.911 | Val. Acc: 59.10% |\n",
            "| Epoch: 09 | Train Loss: 0.934 | Train Acc: 58.05% | Val. Loss: 0.908 | Val. Acc: 60.00% |\n",
            "| Epoch: 10 | Train Loss: 0.925 | Train Acc: 59.40% | Val. Loss: 0.910 | Val. Acc: 60.00% |\n",
            "| Epoch: 01 | Train Loss: 1.206 | Train Acc: 29.60% | Val. Loss: 1.154 | Val. Acc: 30.45% |\n",
            "| Epoch: 02 | Train Loss: 1.206 | Train Acc: 30.09% | Val. Loss: 1.150 | Val. Acc: 30.45% |\n",
            "| Epoch: 03 | Train Loss: 1.199 | Train Acc: 29.28% | Val. Loss: 1.144 | Val. Acc: 30.58% |\n",
            "| Epoch: 04 | Train Loss: 1.191 | Train Acc: 30.28% | Val. Loss: 1.136 | Val. Acc: 30.97% |\n",
            "| Epoch: 05 | Train Loss: 1.177 | Train Acc: 31.12% | Val. Loss: 1.128 | Val. Acc: 31.10% |\n",
            "| Epoch: 06 | Train Loss: 1.170 | Train Acc: 30.73% | Val. Loss: 1.118 | Val. Acc: 31.48% |\n",
            "| Epoch: 07 | Train Loss: 1.156 | Train Acc: 31.89% | Val. Loss: 1.108 | Val. Acc: 31.87% |\n",
            "| Epoch: 08 | Train Loss: 1.143 | Train Acc: 31.67% | Val. Loss: 1.097 | Val. Acc: 32.13% |\n",
            "| Epoch: 09 | Train Loss: 1.137 | Train Acc: 32.31% | Val. Loss: 1.086 | Val. Acc: 32.26% |\n",
            "| Epoch: 10 | Train Loss: 1.117 | Train Acc: 33.54% | Val. Loss: 1.075 | Val. Acc: 33.81% |\n",
            "| Epoch: 01 | Train Loss: 1.332 | Train Acc: 25.38% | Val. Loss: 1.126 | Val. Acc: 30.45% |\n",
            "| Epoch: 02 | Train Loss: 1.210 | Train Acc: 30.35% | Val. Loss: 0.995 | Val. Acc: 53.68% |\n",
            "| Epoch: 03 | Train Loss: 1.062 | Train Acc: 44.60% | Val. Loss: 0.937 | Val. Acc: 59.74% |\n",
            "| Epoch: 04 | Train Loss: 0.985 | Train Acc: 54.69% | Val. Loss: 0.946 | Val. Acc: 60.00% |\n",
            "| Epoch: 05 | Train Loss: 0.978 | Train Acc: 59.59% | Val. Loss: 0.970 | Val. Acc: 60.00% |\n",
            "| Epoch: 06 | Train Loss: 1.005 | Train Acc: 61.27% | Val. Loss: 0.977 | Val. Acc: 60.00% |\n",
            "| Epoch: 07 | Train Loss: 1.003 | Train Acc: 61.43% | Val. Loss: 0.963 | Val. Acc: 60.00% |\n",
            "| Epoch: 08 | Train Loss: 0.987 | Train Acc: 61.17% | Val. Loss: 0.940 | Val. Acc: 60.00% |\n",
            "| Epoch: 09 | Train Loss: 0.986 | Train Acc: 60.27% | Val. Loss: 0.921 | Val. Acc: 60.00% |\n",
            "| Epoch: 10 | Train Loss: 0.963 | Train Acc: 58.95% | Val. Loss: 0.912 | Val. Acc: 59.74% |\n",
            "| Epoch: 01 | Train Loss: 1.008 | Train Acc: 52.11% | Val. Loss: 0.971 | Val. Acc: 57.42% |\n",
            "| Epoch: 02 | Train Loss: 0.997 | Train Acc: 52.95% | Val. Loss: 0.970 | Val. Acc: 57.42% |\n",
            "| Epoch: 03 | Train Loss: 1.002 | Train Acc: 52.47% | Val. Loss: 0.970 | Val. Acc: 57.68% |\n",
            "| Epoch: 04 | Train Loss: 1.012 | Train Acc: 51.85% | Val. Loss: 0.969 | Val. Acc: 57.68% |\n",
            "| Epoch: 05 | Train Loss: 1.004 | Train Acc: 52.92% | Val. Loss: 0.968 | Val. Acc: 57.81% |\n",
            "| Epoch: 06 | Train Loss: 1.008 | Train Acc: 51.89% | Val. Loss: 0.968 | Val. Acc: 57.94% |\n",
            "| Epoch: 07 | Train Loss: 1.009 | Train Acc: 52.02% | Val. Loss: 0.967 | Val. Acc: 58.06% |\n",
            "| Epoch: 08 | Train Loss: 0.997 | Train Acc: 53.14% | Val. Loss: 0.967 | Val. Acc: 58.06% |\n",
            "| Epoch: 09 | Train Loss: 0.997 | Train Acc: 53.11% | Val. Loss: 0.966 | Val. Acc: 58.06% |\n",
            "| Epoch: 10 | Train Loss: 1.003 | Train Acc: 53.21% | Val. Loss: 0.965 | Val. Acc: 57.94% |\n",
            "| Epoch: 01 | Train Loss: 1.001 | Train Acc: 54.60% | Val. Loss: 0.960 | Val. Acc: 57.94% |\n",
            "| Epoch: 02 | Train Loss: 0.981 | Train Acc: 56.59% | Val. Loss: 0.946 | Val. Acc: 59.61% |\n",
            "| Epoch: 03 | Train Loss: 0.954 | Train Acc: 59.69% | Val. Loss: 0.939 | Val. Acc: 59.87% |\n",
            "| Epoch: 04 | Train Loss: 0.949 | Train Acc: 59.21% | Val. Loss: 0.934 | Val. Acc: 60.00% |\n",
            "| Epoch: 05 | Train Loss: 0.958 | Train Acc: 59.17% | Val. Loss: 0.930 | Val. Acc: 60.00% |\n",
            "| Epoch: 06 | Train Loss: 0.952 | Train Acc: 59.88% | Val. Loss: 0.926 | Val. Acc: 59.87% |\n",
            "| Epoch: 07 | Train Loss: 0.947 | Train Acc: 59.56% | Val. Loss: 0.922 | Val. Acc: 59.87% |\n",
            "| Epoch: 08 | Train Loss: 0.943 | Train Acc: 59.79% | Val. Loss: 0.919 | Val. Acc: 59.87% |\n",
            "| Epoch: 09 | Train Loss: 0.943 | Train Acc: 59.50% | Val. Loss: 0.917 | Val. Acc: 59.87% |\n",
            "| Epoch: 10 | Train Loss: 0.942 | Train Acc: 58.40% | Val. Loss: 0.916 | Val. Acc: 59.87% |\n",
            "| Epoch: 01 | Train Loss: 1.299 | Train Acc: 26.51% | Val. Loss: 1.045 | Val. Acc: 46.32% |\n",
            "| Epoch: 02 | Train Loss: 1.132 | Train Acc: 37.83% | Val. Loss: 0.948 | Val. Acc: 59.10% |\n",
            "| Epoch: 03 | Train Loss: 1.009 | Train Acc: 52.27% | Val. Loss: 0.940 | Val. Acc: 60.00% |\n",
            "| Epoch: 04 | Train Loss: 0.991 | Train Acc: 58.85% | Val. Loss: 0.957 | Val. Acc: 60.00% |\n",
            "| Epoch: 05 | Train Loss: 0.989 | Train Acc: 60.34% | Val. Loss: 0.956 | Val. Acc: 60.00% |\n",
            "| Epoch: 06 | Train Loss: 0.987 | Train Acc: 60.43% | Val. Loss: 0.938 | Val. Acc: 60.00% |\n",
            "| Epoch: 07 | Train Loss: 0.981 | Train Acc: 59.37% | Val. Loss: 0.921 | Val. Acc: 60.00% |\n",
            "| Epoch: 08 | Train Loss: 0.972 | Train Acc: 58.69% | Val. Loss: 0.913 | Val. Acc: 60.00% |\n",
            "| Epoch: 09 | Train Loss: 0.962 | Train Acc: 57.30% | Val. Loss: 0.908 | Val. Acc: 60.13% |\n",
            "| Epoch: 10 | Train Loss: 0.965 | Train Acc: 57.01% | Val. Loss: 0.906 | Val. Acc: 60.13% |\n",
            "| Epoch: 01 | Train Loss: 1.272 | Train Acc: 26.12% | Val. Loss: 1.007 | Val. Acc: 53.42% |\n",
            "| Epoch: 02 | Train Loss: 1.065 | Train Acc: 44.11% | Val. Loss: 0.944 | Val. Acc: 60.00% |\n",
            "| Epoch: 03 | Train Loss: 0.975 | Train Acc: 59.17% | Val. Loss: 1.008 | Val. Acc: 60.00% |\n",
            "| Epoch: 04 | Train Loss: 1.016 | Train Acc: 62.21% | Val. Loss: 1.031 | Val. Acc: 60.00% |\n",
            "| Epoch: 05 | Train Loss: 1.040 | Train Acc: 62.17% | Val. Loss: 0.981 | Val. Acc: 60.00% |\n",
            "| Epoch: 06 | Train Loss: 0.996 | Train Acc: 61.75% | Val. Loss: 0.938 | Val. Acc: 60.00% |\n",
            "| Epoch: 07 | Train Loss: 0.975 | Train Acc: 58.40% | Val. Loss: 0.937 | Val. Acc: 56.13% |\n",
            "| Epoch: 08 | Train Loss: 0.986 | Train Acc: 53.98% | Val. Loss: 0.929 | Val. Acc: 56.90% |\n",
            "| Epoch: 09 | Train Loss: 0.984 | Train Acc: 52.63% | Val. Loss: 0.908 | Val. Acc: 59.10% |\n",
            "| Epoch: 10 | Train Loss: 0.952 | Train Acc: 56.88% | Val. Loss: 0.906 | Val. Acc: 60.00% |\n",
            "| Epoch: 01 | Train Loss: 1.145 | Train Acc: 41.73% | Val. Loss: 1.080 | Val. Acc: 53.55% |\n",
            "| Epoch: 02 | Train Loss: 1.088 | Train Acc: 49.53% | Val. Loss: 1.011 | Val. Acc: 59.23% |\n",
            "| Epoch: 03 | Train Loss: 1.011 | Train Acc: 57.50% | Val. Loss: 0.962 | Val. Acc: 59.87% |\n",
            "| Epoch: 04 | Train Loss: 0.966 | Train Acc: 60.82% | Val. Loss: 0.935 | Val. Acc: 60.00% |\n",
            "| Epoch: 05 | Train Loss: 0.943 | Train Acc: 61.69% | Val. Loss: 0.924 | Val. Acc: 60.00% |\n",
            "| Epoch: 06 | Train Loss: 0.929 | Train Acc: 61.95% | Val. Loss: 0.919 | Val. Acc: 60.00% |\n",
            "| Epoch: 07 | Train Loss: 0.938 | Train Acc: 60.85% | Val. Loss: 0.917 | Val. Acc: 60.13% |\n",
            "| Epoch: 08 | Train Loss: 0.941 | Train Acc: 60.43% | Val. Loss: 0.916 | Val. Acc: 60.00% |\n",
            "| Epoch: 09 | Train Loss: 0.938 | Train Acc: 59.11% | Val. Loss: 0.916 | Val. Acc: 59.61% |\n",
            "| Epoch: 10 | Train Loss: 0.940 | Train Acc: 58.21% | Val. Loss: 0.915 | Val. Acc: 59.61% |\n",
            "| Epoch: 01 | Train Loss: 1.155 | Train Acc: 40.95% | Val. Loss: 1.079 | Val. Acc: 47.61% |\n",
            "| Epoch: 02 | Train Loss: 1.156 | Train Acc: 39.79% | Val. Loss: 1.078 | Val. Acc: 47.74% |\n",
            "| Epoch: 03 | Train Loss: 1.172 | Train Acc: 38.25% | Val. Loss: 1.075 | Val. Acc: 48.52% |\n",
            "| Epoch: 04 | Train Loss: 1.161 | Train Acc: 39.76% | Val. Loss: 1.073 | Val. Acc: 49.42% |\n",
            "| Epoch: 05 | Train Loss: 1.152 | Train Acc: 39.66% | Val. Loss: 1.071 | Val. Acc: 50.58% |\n",
            "| Epoch: 06 | Train Loss: 1.140 | Train Acc: 41.70% | Val. Loss: 1.068 | Val. Acc: 50.84% |\n",
            "| Epoch: 07 | Train Loss: 1.147 | Train Acc: 41.41% | Val. Loss: 1.066 | Val. Acc: 50.97% |\n",
            "| Epoch: 08 | Train Loss: 1.151 | Train Acc: 41.41% | Val. Loss: 1.063 | Val. Acc: 51.35% |\n",
            "| Epoch: 09 | Train Loss: 1.132 | Train Acc: 41.34% | Val. Loss: 1.061 | Val. Acc: 51.74% |\n",
            "| Epoch: 10 | Train Loss: 1.146 | Train Acc: 40.25% | Val. Loss: 1.058 | Val. Acc: 52.26% |\n",
            "| Epoch: 01 | Train Loss: 1.094 | Train Acc: 44.82% | Val. Loss: 1.030 | Val. Acc: 54.71% |\n",
            "| Epoch: 02 | Train Loss: 1.089 | Train Acc: 45.02% | Val. Loss: 1.029 | Val. Acc: 54.71% |\n",
            "| Epoch: 03 | Train Loss: 1.094 | Train Acc: 45.44% | Val. Loss: 1.027 | Val. Acc: 54.84% |\n",
            "| Epoch: 04 | Train Loss: 1.093 | Train Acc: 46.28% | Val. Loss: 1.025 | Val. Acc: 54.97% |\n",
            "| Epoch: 05 | Train Loss: 1.100 | Train Acc: 44.89% | Val. Loss: 1.023 | Val. Acc: 54.84% |\n",
            "| Epoch: 06 | Train Loss: 1.101 | Train Acc: 44.50% | Val. Loss: 1.021 | Val. Acc: 55.23% |\n",
            "| Epoch: 07 | Train Loss: 1.078 | Train Acc: 47.79% | Val. Loss: 1.019 | Val. Acc: 55.23% |\n",
            "| Epoch: 08 | Train Loss: 1.093 | Train Acc: 44.82% | Val. Loss: 1.017 | Val. Acc: 55.35% |\n",
            "| Epoch: 09 | Train Loss: 1.090 | Train Acc: 46.31% | Val. Loss: 1.014 | Val. Acc: 55.35% |\n",
            "| Epoch: 10 | Train Loss: 1.083 | Train Acc: 47.34% | Val. Loss: 1.012 | Val. Acc: 55.35% |\n",
            "| Epoch: 01 | Train Loss: 1.222 | Train Acc: 33.76% | Val. Loss: 1.054 | Val. Acc: 58.19% |\n",
            "| Epoch: 02 | Train Loss: 1.085 | Train Acc: 47.76% | Val. Loss: 0.970 | Val. Acc: 59.74% |\n",
            "| Epoch: 03 | Train Loss: 0.989 | Train Acc: 59.17% | Val. Loss: 0.941 | Val. Acc: 59.87% |\n",
            "| Epoch: 04 | Train Loss: 0.964 | Train Acc: 60.05% | Val. Loss: 0.930 | Val. Acc: 59.74% |\n",
            "| Epoch: 05 | Train Loss: 0.955 | Train Acc: 60.46% | Val. Loss: 0.924 | Val. Acc: 59.74% |\n",
            "| Epoch: 06 | Train Loss: 0.944 | Train Acc: 60.21% | Val. Loss: 0.919 | Val. Acc: 59.61% |\n",
            "| Epoch: 07 | Train Loss: 0.941 | Train Acc: 59.50% | Val. Loss: 0.916 | Val. Acc: 59.61% |\n",
            "| Epoch: 08 | Train Loss: 0.939 | Train Acc: 59.46% | Val. Loss: 0.914 | Val. Acc: 59.61% |\n",
            "| Epoch: 09 | Train Loss: 0.958 | Train Acc: 58.30% | Val. Loss: 0.913 | Val. Acc: 59.61% |\n",
            "| Epoch: 10 | Train Loss: 0.941 | Train Acc: 59.72% | Val. Loss: 0.912 | Val. Acc: 59.74% |\n",
            "| Epoch: 01 | Train Loss: 1.598 | Train Acc: 15.93% | Val. Loss: 1.457 | Val. Acc: 10.45% |\n",
            "| Epoch: 02 | Train Loss: 1.546 | Train Acc: 17.22% | Val. Loss: 1.364 | Val. Acc: 10.58% |\n",
            "| Epoch: 03 | Train Loss: 1.454 | Train Acc: 19.64% | Val. Loss: 1.254 | Val. Acc: 13.94% |\n",
            "| Epoch: 04 | Train Loss: 1.346 | Train Acc: 24.73% | Val. Loss: 1.145 | Val. Acc: 24.00% |\n",
            "| Epoch: 05 | Train Loss: 1.215 | Train Acc: 31.89% | Val. Loss: 1.056 | Val. Acc: 46.71% |\n",
            "| Epoch: 06 | Train Loss: 1.124 | Train Acc: 39.28% | Val. Loss: 0.993 | Val. Acc: 56.39% |\n",
            "| Epoch: 07 | Train Loss: 1.051 | Train Acc: 47.92% | Val. Loss: 0.958 | Val. Acc: 59.35% |\n",
            "| Epoch: 08 | Train Loss: 1.005 | Train Acc: 52.53% | Val. Loss: 0.946 | Val. Acc: 59.74% |\n",
            "| Epoch: 09 | Train Loss: 0.998 | Train Acc: 56.11% | Val. Loss: 0.949 | Val. Acc: 59.74% |\n",
            "| Epoch: 10 | Train Loss: 1.000 | Train Acc: 58.08% | Val. Loss: 0.960 | Val. Acc: 59.87% |\n",
            "| Epoch: 01 | Train Loss: 1.021 | Train Acc: 51.02% | Val. Loss: 0.952 | Val. Acc: 58.45% |\n",
            "| Epoch: 02 | Train Loss: 0.990 | Train Acc: 55.85% | Val. Loss: 0.944 | Val. Acc: 59.10% |\n",
            "| Epoch: 03 | Train Loss: 0.969 | Train Acc: 58.95% | Val. Loss: 0.947 | Val. Acc: 60.00% |\n",
            "| Epoch: 04 | Train Loss: 0.977 | Train Acc: 60.66% | Val. Loss: 0.944 | Val. Acc: 60.00% |\n",
            "| Epoch: 05 | Train Loss: 0.971 | Train Acc: 59.95% | Val. Loss: 0.932 | Val. Acc: 60.13% |\n",
            "| Epoch: 06 | Train Loss: 0.973 | Train Acc: 59.24% | Val. Loss: 0.921 | Val. Acc: 59.87% |\n",
            "| Epoch: 07 | Train Loss: 0.961 | Train Acc: 58.34% | Val. Loss: 0.912 | Val. Acc: 59.48% |\n",
            "| Epoch: 08 | Train Loss: 0.949 | Train Acc: 57.21% | Val. Loss: 0.901 | Val. Acc: 60.00% |\n",
            "| Epoch: 09 | Train Loss: 0.928 | Train Acc: 59.30% | Val. Loss: 0.894 | Val. Acc: 60.13% |\n",
            "| Epoch: 10 | Train Loss: 0.929 | Train Acc: 59.50% | Val. Loss: 0.894 | Val. Acc: 60.00% |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R9GVRcUiAkRK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4d93e8c0-f5f3-4aac-c5a2-eff0b6621ade"
      },
      "cell_type": "code",
      "source": [
        "print(\"The hyperparameters used for the model in task C are\")\n",
        "print(f'embedding_dim: {int(taskC_opt.X[-1][0])}')\n",
        "print(f'out_channels: {int(taskC_opt.X[-1][1])}')\n",
        "print(f'drop_out: {taskC_opt.X[-1][3]:.2f}')\n",
        "print(f'learning_rate: {taskC_opt.X[-1][4]:.4f}')\n",
        "print(f'weight_decay: {taskC_opt.X[-1][5]:.4f}')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The hyperparameters used for the model in task C are\n",
            "embedding_dim: 180\n",
            "out_channels: 180\n",
            "drop_out: 0.36\n",
            "learning_rate: 0.0100\n",
            "weight_decay: 0.9000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QvaaX3oNArZF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "c6e810ec-01a9-4779-9822-d1fb6df7a23b"
      },
      "cell_type": "code",
      "source": [
        "# CNN(vocab_size, embedding_dim, out_channels, window_size, output_dim, dropout)\n",
        "test_model_C = CNN(INPUT_DIM, int(taskC_opt.X[-1][0]), int(taskC_opt.X[-1][1]), WINDOW_SIZE, OUTPUT_DIM, taskC_opt.X[-1][3])\n",
        "# LSTMClassifier(vocab_size, embedding_dim, hidden_dim, label_size, output_dim, dropout)\n",
        "# test_model_C = LSTMClassifier(INPUT_DIM, int(param[0]), int(param[2]), 2, OUTPUT_DIM, param[3])\n",
        "optimizer_C = optim.SGD(test_model_C.parameters(), lr=taskC_opt.X[-1][4], momentum=taskC_opt.X[-1][5])\n",
        "train_part(test_model_C, tweets_c_tensor, labels_c_tensor, optimizer_C, epochs=50, num_class=3)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 1.120 | Train Acc: 37.73% | Val. Loss: 0.968 | Val. Acc: 57.81% |\n",
            "| Epoch: 02 | Train Loss: 1.005 | Train Acc: 51.44% | Val. Loss: 0.941 | Val. Acc: 60.00% |\n",
            "| Epoch: 03 | Train Loss: 0.960 | Train Acc: 59.92% | Val. Loss: 0.989 | Val. Acc: 60.00% |\n",
            "| Epoch: 04 | Train Loss: 0.997 | Train Acc: 62.17% | Val. Loss: 1.003 | Val. Acc: 60.00% |\n",
            "| Epoch: 05 | Train Loss: 1.012 | Train Acc: 61.92% | Val. Loss: 0.972 | Val. Acc: 60.00% |\n",
            "| Epoch: 06 | Train Loss: 0.998 | Train Acc: 60.95% | Val. Loss: 0.955 | Val. Acc: 60.00% |\n",
            "| Epoch: 07 | Train Loss: 0.989 | Train Acc: 57.85% | Val. Loss: 0.961 | Val. Acc: 59.35% |\n",
            "| Epoch: 08 | Train Loss: 1.006 | Train Acc: 53.89% | Val. Loss: 0.937 | Val. Acc: 60.26% |\n",
            "| Epoch: 09 | Train Loss: 0.976 | Train Acc: 55.24% | Val. Loss: 0.899 | Val. Acc: 60.26% |\n",
            "| Epoch: 10 | Train Loss: 0.924 | Train Acc: 60.37% | Val. Loss: 0.896 | Val. Acc: 60.00% |\n",
            "| Epoch: 11 | Train Loss: 0.909 | Train Acc: 62.66% | Val. Loss: 0.906 | Val. Acc: 60.00% |\n",
            "| Epoch: 12 | Train Loss: 0.909 | Train Acc: 62.56% | Val. Loss: 0.906 | Val. Acc: 60.00% |\n",
            "| Epoch: 13 | Train Loss: 0.921 | Train Acc: 62.46% | Val. Loss: 0.901 | Val. Acc: 60.00% |\n",
            "| Epoch: 14 | Train Loss: 0.920 | Train Acc: 61.30% | Val. Loss: 0.892 | Val. Acc: 60.39% |\n",
            "| Epoch: 15 | Train Loss: 0.917 | Train Acc: 59.92% | Val. Loss: 0.873 | Val. Acc: 61.03% |\n",
            "| Epoch: 16 | Train Loss: 0.897 | Train Acc: 60.46% | Val. Loss: 0.853 | Val. Acc: 61.55% |\n",
            "| Epoch: 17 | Train Loss: 0.880 | Train Acc: 62.21% | Val. Loss: 0.848 | Val. Acc: 60.65% |\n",
            "| Epoch: 18 | Train Loss: 0.870 | Train Acc: 61.82% | Val. Loss: 0.857 | Val. Acc: 60.52% |\n",
            "| Epoch: 19 | Train Loss: 0.864 | Train Acc: 63.21% | Val. Loss: 0.866 | Val. Acc: 60.39% |\n",
            "| Epoch: 20 | Train Loss: 0.876 | Train Acc: 63.04% | Val. Loss: 0.866 | Val. Acc: 60.77% |\n",
            "| Epoch: 21 | Train Loss: 0.875 | Train Acc: 63.46% | Val. Loss: 0.855 | Val. Acc: 61.29% |\n",
            "| Epoch: 22 | Train Loss: 0.876 | Train Acc: 62.50% | Val. Loss: 0.843 | Val. Acc: 63.23% |\n",
            "| Epoch: 23 | Train Loss: 0.866 | Train Acc: 63.04% | Val. Loss: 0.834 | Val. Acc: 63.74% |\n",
            "| Epoch: 24 | Train Loss: 0.855 | Train Acc: 63.21% | Val. Loss: 0.828 | Val. Acc: 63.74% |\n",
            "| Epoch: 25 | Train Loss: 0.858 | Train Acc: 62.69% | Val. Loss: 0.828 | Val. Acc: 62.97% |\n",
            "| Epoch: 26 | Train Loss: 0.846 | Train Acc: 63.88% | Val. Loss: 0.835 | Val. Acc: 61.68% |\n",
            "| Epoch: 27 | Train Loss: 0.855 | Train Acc: 63.91% | Val. Loss: 0.840 | Val. Acc: 61.68% |\n",
            "| Epoch: 28 | Train Loss: 0.844 | Train Acc: 63.59% | Val. Loss: 0.836 | Val. Acc: 61.68% |\n",
            "| Epoch: 29 | Train Loss: 0.849 | Train Acc: 63.91% | Val. Loss: 0.826 | Val. Acc: 64.00% |\n",
            "| Epoch: 30 | Train Loss: 0.844 | Train Acc: 64.72% | Val. Loss: 0.818 | Val. Acc: 65.55% |\n",
            "| Epoch: 31 | Train Loss: 0.842 | Train Acc: 65.14% | Val. Loss: 0.815 | Val. Acc: 65.42% |\n",
            "| Epoch: 32 | Train Loss: 0.833 | Train Acc: 64.46% | Val. Loss: 0.815 | Val. Acc: 65.55% |\n",
            "| Epoch: 33 | Train Loss: 0.842 | Train Acc: 64.08% | Val. Loss: 0.817 | Val. Acc: 65.68% |\n",
            "| Epoch: 34 | Train Loss: 0.824 | Train Acc: 66.04% | Val. Loss: 0.819 | Val. Acc: 65.68% |\n",
            "| Epoch: 35 | Train Loss: 0.833 | Train Acc: 65.33% | Val. Loss: 0.819 | Val. Acc: 65.68% |\n",
            "| Epoch: 36 | Train Loss: 0.827 | Train Acc: 65.08% | Val. Loss: 0.817 | Val. Acc: 65.68% |\n",
            "| Epoch: 37 | Train Loss: 0.833 | Train Acc: 65.27% | Val. Loss: 0.812 | Val. Acc: 65.55% |\n",
            "| Epoch: 38 | Train Loss: 0.829 | Train Acc: 65.62% | Val. Loss: 0.809 | Val. Acc: 65.55% |\n",
            "| Epoch: 39 | Train Loss: 0.823 | Train Acc: 65.43% | Val. Loss: 0.808 | Val. Acc: 65.68% |\n",
            "| Epoch: 40 | Train Loss: 0.825 | Train Acc: 65.01% | Val. Loss: 0.809 | Val. Acc: 65.68% |\n",
            "| Epoch: 41 | Train Loss: 0.828 | Train Acc: 65.24% | Val. Loss: 0.809 | Val. Acc: 65.55% |\n",
            "| Epoch: 42 | Train Loss: 0.829 | Train Acc: 65.53% | Val. Loss: 0.809 | Val. Acc: 65.55% |\n",
            "| Epoch: 43 | Train Loss: 0.817 | Train Acc: 65.62% | Val. Loss: 0.808 | Val. Acc: 65.55% |\n",
            "| Epoch: 44 | Train Loss: 0.819 | Train Acc: 65.98% | Val. Loss: 0.807 | Val. Acc: 65.55% |\n",
            "| Epoch: 45 | Train Loss: 0.819 | Train Acc: 65.66% | Val. Loss: 0.805 | Val. Acc: 65.55% |\n",
            "| Epoch: 46 | Train Loss: 0.815 | Train Acc: 66.33% | Val. Loss: 0.803 | Val. Acc: 65.68% |\n",
            "| Epoch: 47 | Train Loss: 0.815 | Train Acc: 66.37% | Val. Loss: 0.803 | Val. Acc: 65.68% |\n",
            "| Epoch: 48 | Train Loss: 0.816 | Train Acc: 65.82% | Val. Loss: 0.802 | Val. Acc: 65.68% |\n",
            "| Epoch: 49 | Train Loss: 0.815 | Train Acc: 66.11% | Val. Loss: 0.802 | Val. Acc: 65.68% |\n",
            "| Epoch: 50 | Train Loss: 0.811 | Train Acc: 65.88% | Val. Loss: 0.802 | Val. Acc: 65.55% |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6555)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "metadata": {
        "id": "Z9n7KlwGIFjR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def indices_to_labels(indices, label2index):\n",
        "  labels = []\n",
        "  for index in indices:\n",
        "    for key, num in label2index.items():\n",
        "      if index == num:\n",
        "        labels.append(key)\n",
        "  return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ty6MIl-yGM67",
        "colab_type": "code",
        "outputId": "b06f87c0-d16e-46c1-dd7e-7045848c66b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1751
        }
      },
      "cell_type": "code",
      "source": [
        "testA_dir = \"/content/drive/My Drive/OffensEval/data/taskA/testset-taska.tsv\"\n",
        "testB_dir = \"/content/drive/My Drive/OffensEval/data/taskB/testset-taskb.tsv\"\n",
        "testC_dir = \"/content/drive/My Drive/OffensEval/data/taskC/test_set_taskc.tsv\"\n",
        "\n",
        "testA_data = pd.read_csv(testA_dir, sep='\\t', header=0)\n",
        "testA_ids = testA_data[\"id\"].tolist()\n",
        "testA_tweets = testA_data[[\"tweet\"]]\n",
        "clean_testA = copy.deepcopy(testA_tweets)\n",
        "\n",
        "testB_data = pd.read_csv(testB_dir, sep='\\t', header=0)\n",
        "testB_ids = testB_data[\"id\"].tolist()\n",
        "testB_tweets = testB_data[[\"tweet\"]]\n",
        "clean_testB = copy.deepcopy(testB_tweets)\n",
        "\n",
        "testC_data = pd.read_csv(testC_dir, sep='\\t', header=0)\n",
        "testC_ids = testC_data[\"id\"].tolist()\n",
        "testC_tweets = testC_data[[\"tweet\"]]\n",
        "clean_testC = copy.deepcopy(testC_tweets)\n",
        "\n",
        "tqdm.pandas(desc=\"Cleaning Data for Task A...\")\n",
        "clean_testA['tweet'] = testA_tweets['tweet'].progress_apply(clean_data)\n",
        "tqdm.pandas(desc=\"Tokenizing Data for Task A...\")\n",
        "clean_testA['tokens'] = clean_testA['tweet'].progress_apply(tokenize)\n",
        "tqdm.pandas(desc=\"Removing Stop Words for Task A...\")\n",
        "clean_testA['tokens'] = clean_testA['tokens'].progress_apply(remove_stop_words)\n",
        "tqdm.pandas(desc=\"Lemmatizing And Stemming for Task A...\")\n",
        "clean_testA['tokens'] = clean_testA['tokens'].progress_apply(lemmatize_and_stem)\n",
        "\n",
        "tqdm.pandas(desc=\"Cleaning Data for Task B...\")\n",
        "clean_testB['tweet'] = testB_tweets['tweet'].progress_apply(clean_data)\n",
        "tqdm.pandas(desc=\"Tokenizing Data for Task B...\")\n",
        "clean_testB['tokens'] = clean_testB['tweet'].progress_apply(tokenize)\n",
        "tqdm.pandas(desc=\"Removing Stop Words for Task B...\")\n",
        "clean_testB['tokens'] = clean_testB['tokens'].progress_apply(remove_stop_words)\n",
        "tqdm.pandas(desc=\"Lemmatizing And Stemming for Task B...\")\n",
        "clean_testB['tokens'] = clean_testB['tokens'].progress_apply(lemmatize_and_stem)\n",
        "\n",
        "tqdm.pandas(desc=\"Cleaning Data for Task C...\")\n",
        "clean_testC['tweet'] = testC_tweets['tweet'].progress_apply(clean_data)\n",
        "tqdm.pandas(desc=\"Tokenizing Data for Task C...\")\n",
        "clean_testC['tokens'] = clean_testC['tweet'].progress_apply(tokenize)\n",
        "tqdm.pandas(desc=\"Removing Stop Words for Task C...\")\n",
        "clean_testC['tokens'] = clean_testC['tokens'].progress_apply(remove_stop_words)\n",
        "tqdm.pandas(desc=\"Lemmatizing And Stemming for Task C...\")\n",
        "clean_testC['tokens'] = clean_testC['tokens'].progress_apply(lemmatize_and_stem)\n",
        "\n",
        "tokenized_testA = clean_testA['tokens'].tolist()\n",
        "tokenized_testB = clean_testB['tokens'].tolist()\n",
        "tokenized_testC = clean_testC['tokens'].tolist()\n",
        "\n",
        "testA_tensor = tweet_to_tensor(tokenized_testA, word2index, max_length)\n",
        "testB_tensor = tweet_to_tensor(tokenized_testB, word2index, max_length)\n",
        "testC_tensor = tweet_to_tensor(tokenized_testC, word2index, max_length)\n",
        "\n",
        "print(\"Test A tensor size:\")\n",
        "print(testA_tensor.shape)\n",
        "print(\"Test B tensor size:\")\n",
        "print(testB_tensor.shape)\n",
        "print(\"Test C tensor size:\")\n",
        "print(testC_tensor.shape)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Cleaning Data for Task A...:   0%|          | 0/860 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Cleaning Data for Task A...: 100%|██████████| 860/860 [00:00<00:00, 51806.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data for Task A...:   0%|          | 0/860 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data for Task A...:  57%|█████▋    | 486/860 [00:00<00:00, 4857.25it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data for Task A...: 100%|██████████| 860/860 [00:00<00:00, 4757.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words for Task A...:   0%|          | 0/860 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words for Task A...:  70%|███████   | 603/860 [00:00<00:00, 6022.67it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words for Task A...: 100%|██████████| 860/860 [00:00<00:00, 5554.46it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming for Task A...:   0%|          | 0/860 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming for Task A...:  21%|██▏       | 183/860 [00:00<00:00, 1815.54it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming for Task A...:  40%|████      | 346/860 [00:00<00:00, 1755.47it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming for Task A...:  61%|██████    | 526/860 [00:00<00:00, 1766.40it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming for Task A...:  82%|████████▏ | 708/860 [00:00<00:00, 1778.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming for Task A...: 100%|██████████| 860/860 [00:00<00:00, 1746.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Cleaning Data for Task B...:   0%|          | 0/240 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Cleaning Data for Task B...: 100%|██████████| 240/240 [00:00<00:00, 25500.52it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data for Task B...:   0%|          | 0/240 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data for Task B...: 100%|██████████| 240/240 [00:00<00:00, 3935.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words for Task B...:   0%|          | 0/240 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words for Task B...: 100%|██████████| 240/240 [00:00<00:00, 4705.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming for Task B...:   0%|          | 0/240 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming for Task B...:  78%|███████▊  | 187/240 [00:00<00:00, 1869.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming for Task B...: 100%|██████████| 240/240 [00:00<00:00, 1778.54it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Cleaning Data for Task C...:   0%|          | 0/213 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Cleaning Data for Task C...: 100%|██████████| 213/213 [00:00<00:00, 22373.26it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data for Task C...:   0%|          | 0/213 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Tokenizing Data for Task C...: 100%|██████████| 213/213 [00:00<00:00, 3501.39it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words for Task C...:   0%|          | 0/213 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Removing Stop Words for Task C...: 100%|██████████| 213/213 [00:00<00:00, 4720.30it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming for Task C...:   0%|          | 0/213 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming for Task C...:  85%|████████▍ | 181/213 [00:00<00:00, 1808.51it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Lemmatizing And Stemming for Task C...: 100%|██████████| 213/213 [00:00<00:00, 1695.97it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test A tensor size:\n",
            "torch.Size([860, 39])\n",
            "Test B tensor size:\n",
            "torch.Size([240, 39])\n",
            "Test C tensor size:\n",
            "torch.Size([213, 39])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Wr28I4PSGfl8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_model_A.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  output = test_model_A(testA_tensor).squeeze(1)\n",
        "  output = torch.round(torch.sigmoid(output))\n",
        "\n",
        "preds_testA = indices_to_labels(output, a2index)\n",
        "\n",
        "taskA_df = pd.DataFrame(testA_ids, columns=['id'])\n",
        "taskA_df['predict'] = preds_testA\n",
        "taskA_df.to_csv('testA.csv', header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qJXZIh25KIhk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_model_B.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  output = test_model_B(testB_tensor).squeeze(1)\n",
        "  output = torch.round(torch.sigmoid(output))\n",
        "\n",
        "preds_testB = indices_to_labels(output, b2index)\n",
        "\n",
        "taskB_df = pd.DataFrame(testB_ids, columns=['id'])\n",
        "taskB_df['predict'] = preds_testB\n",
        "taskB_df.to_csv('testB.csv', header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CMGvOxMoU3w8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_model_C.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  output = test_model_C(testC_tensor).squeeze(1)\n",
        "  output = F.log_softmax(output)\n",
        "  output = output.data.max(1)[1].numpy()\n",
        "\n",
        "preds_testC = indices_to_labels(output, c2index)\n",
        "\n",
        "taskC_df = pd.DataFrame(testC_ids, columns=['id'])\n",
        "taskC_df['predict'] = preds_testC\n",
        "taskC_df.to_csv('testC.csv', header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}